{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ba0c476-af43-44d4-8d38-3cac4bcf5f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary: ImagePath, array of FIXMAPS\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import math\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import csv\n",
    "\n",
    "grid_size = (20,20)\n",
    "sigma = 1\n",
    "def compute_ppda(distance, h_res, v_res, screen_w, screen_h):\n",
    "    \"\"\"\n",
    "    Compute the number of pixels per degree of visual angle based on the experimental conditions.\n",
    "\n",
    "    :param distance: int, the distance between the observer and the screen (in mm)\n",
    "    :param h_res: int, the horizontal resolution of the screen\n",
    "    :param v_res: int, the vertical resolution of the screen\n",
    "    :param screen_w: int, the width of the screen (in mm)\n",
    "    :param screen_h: int, the height of the screen (in mm)\n",
    "    :return horizontal_ppda: float, the number of pixel per degree of visual angle\n",
    "    \"\"\"\n",
    "    pxl_density_x = h_res / screen_w\n",
    "    pxl_density_y = v_res / screen_h\n",
    "\n",
    "    d = 2 * distance * math.tan(np.deg2rad(0.5))\n",
    "    horizontal_ppda = d * ((pxl_density_x + pxl_density_y) / 2)\n",
    "\n",
    "    return horizontal_ppda\n",
    "\n",
    "def checkObserverRemembered(observer, image_path, base_dir):\n",
    "    csv_file_path = os.path.join(base_dir, \"..\" ,\"hit_status.csv\")\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "        print(\"Error: CSV file not found.\")\n",
    "        return False\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    filtered_rows = df[(df['Setup Folder'] == observer) & (df['Image Path'] == image_path) & (df['Hit'] == 1)]\n",
    "    if not filtered_rows.empty:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "distance = 610\n",
    "h_res = 1920\n",
    "v_res = 1080\n",
    "screen_w = 527\n",
    "screen_h = 296\n",
    "\n",
    "ppda = compute_ppda(distance, h_res, v_res, screen_w, screen_h)\n",
    "#print(\"ppda\", ppda)\n",
    "\n",
    "def bin_fixations(fixation_map):\n",
    "    global grid_size\n",
    "    height, width = fixation_map.shape\n",
    "    binned_map = np.zeros(grid_size)\n",
    "\n",
    "    bin_height = height // grid_size[0]\n",
    "    bin_width = width // grid_size[1]\n",
    "\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            bin_area = fixation_map[i*bin_height:(i+1)*bin_height, j*bin_width:(j+1)*bin_width]\n",
    "            binned_map[i, j] = np.sum(bin_area)\n",
    "            #ili avg?\n",
    "\n",
    "    return binned_map\n",
    "\n",
    "def normalize_map(binned_map):\n",
    "    return binned_map / np.sum(binned_map)\n",
    "    #return binned_map\n",
    "\n",
    "def smooth_map(binned_map):\n",
    "    global sigma\n",
    "    return gaussian_filter(binned_map, sigma=sigma)\n",
    "\n",
    "def process_fixation_map(fixation_map):\n",
    "    binned_map = bin_fixations(fixation_map)\n",
    "    normalized_map = normalize_map(binned_map)\n",
    "    smoothed_map = smooth_map(normalized_map)\n",
    "    return smoothed_map\n",
    "\n",
    "def get_current_fixation_map(image_path, coordinates):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Image at {image_path} not found.\")\n",
    "        return\n",
    "\n",
    "    coordinates = coordinates[0:120]\n",
    "  \n",
    "    fixation_map = np.zeros((1080, 1920), dtype=np.float32)\n",
    "\n",
    "    # Convert coordinates to pixel coordinates and update the saliency map\n",
    "    for x_norm, y_norm in coordinates:\n",
    "        # Scale normalized coordinates to pixel coordinates for the 1920x1080 screen\n",
    "        if(x_norm >0 and y_norm >0):\n",
    "            x = int((x_norm + 1 + 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "            y = int((y_norm + 0.5 + 0.05) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "        if(x_norm >0 and y_norm <0):\n",
    "            x = int((x_norm + 1 + 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "            y = int((y_norm + 0.5 - 0.1) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "        if(x_norm <0 and y_norm >0):\n",
    "            x = int((x_norm + 1 - 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "            y = int((y_norm + 0.5 + 0.05) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "        if(x_norm <0 and y_norm <0):\n",
    "            x = int((x_norm + 1 - 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "            y = int((y_norm + 0.5 - 0.1) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "        # Update the saliency map if coordinates are within the screen\n",
    "        if 0 <= x < 1920 and 0 <= y < 1080:\n",
    "            fixation_map[y, x] += 1\n",
    "    #sigma = ppda / np.sqrt(2)\n",
    "    #fixation_map = gaussian_filter(fixation_map, sigma = sigma)\n",
    "    fixation_map = cv2.GaussianBlur(fixation_map, (11,11), 0)\n",
    "    # Crop the saliency map to the 700x700 region\n",
    "    fixation_map = fixation_map[190:890, 610:1310]\n",
    "    # flip the Y coordinates\n",
    "    fixation_map = np.flipud(fixation_map)\n",
    "    return fixation_map\n",
    "\n",
    "def normalize_fixation_map(fixation_map):\n",
    "    min_val = np.min(fixation_map)\n",
    "    max_val = np.max(fixation_map)\n",
    "    normalized_fixation_map = (fixation_map - min_val) / (max_val - min_val) * 255\n",
    "    return normalized_fixation_map\n",
    "\n",
    "# 90experiments folder\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(),\"..\", \"90experiments\"))\n",
    "\n",
    "fixation_maps = {}  # Dictionary to store fixation maps for each imagePath\n",
    "\n",
    "\n",
    "for folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    match = re.search(r'\\d{1,2}$', folder)\n",
    "    if match:\n",
    "        observer = int(match.group())\n",
    "    #if(observer != 5):\n",
    "    #    continue\n",
    "    if(observer == 1 or observer == 2 or observer == 49 or observer == 50 or observer == 5):\n",
    "        continue\n",
    "\n",
    "    #if(observer not in [70,71,73,74,76,77,79,80,82,83,85,87,88,89,86,90,18,57,6,45,48,60,63,69,3,9,12,21,15,27,30,33,36,42,24,66,51,54,72,75]):\n",
    "    #    continue\n",
    "        \n",
    "    csv_file_path = os.path.join(folder_path, \"eye_tracker_data.csv\")\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "        continue\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    filtered_data = data[data['ImagePath'].str.startswith('targetImages')]\n",
    "    \n",
    "    uniqueImagePaths = []\n",
    "    delete_rows = []\n",
    "\n",
    "    #get only the eye-tracking data from the first viewing\n",
    "    index = 0\n",
    "    \n",
    "    row = filtered_data.iloc[index]\n",
    "    while len(uniqueImagePaths) < 10:\n",
    "        row = filtered_data.iloc[index]\n",
    "        if(row['ImagePath'] not in uniqueImagePaths):\n",
    "            uniqueImagePaths.append(row['ImagePath'])\n",
    "            lastImagePath = row['ImagePath']\n",
    "            index +=1\n",
    "        elif(row['ImagePath'] in uniqueImagePaths):\n",
    "            index += 1    \n",
    "    row = filtered_data.iloc[index]\n",
    "\n",
    "    while(row['ImagePath'] == lastImagePath):\n",
    "        index +=1\n",
    "        row = filtered_data.iloc[index]\n",
    "\n",
    "    filtered_data.reset_index(drop=True, inplace=True)\n",
    "    filtered_data = filtered_data.iloc[:index].copy()\n",
    "    \n",
    "    grouped = filtered_data.groupby('ImagePath')\n",
    "\n",
    "    # Generate and save fixation maps for each image in the current folder\n",
    "    for image_path, group in grouped:\n",
    "        # Construct full image path by going one directory back from base_dir\n",
    "        full_image_path = os.path.abspath(os.path.join(base_dir, \"..\", image_path))\n",
    "        full_image_path = full_image_path.replace('\\\\', '/')\n",
    "\n",
    "        #check if current observer has remembered this image, if not, continue\n",
    "        if(not checkObserverRemembered(observer, image_path, base_dir)):\n",
    "            continue\n",
    "        \n",
    "        # Extract coordinates\n",
    "        coordinates = group[['PosX', 'PosY']].values\n",
    "\n",
    "        current_fixation_map = get_current_fixation_map (full_image_path, coordinates)\n",
    "        if(np.all(current_fixation_map == 0)):\n",
    "            continue\n",
    "\n",
    "        current_fixation_map_20x20 = process_fixation_map(current_fixation_map)\n",
    "        current_fixation_map_20x20 = normalize_fixation_map(current_fixation_map_20x20)\n",
    "        \n",
    "        #current_fixation_map_20x20 = (current_fixation_map)\n",
    "        #add to dictionary or update it\n",
    "        if image_path not in fixation_maps:\n",
    "            fixation_maps[image_path] = [current_fixation_map_20x20]\n",
    "        else:\n",
    "            fixation_maps[image_path].append(current_fixation_map_20x20)\n",
    "\n",
    "# Flatten the fixation maps and standardize them\n",
    "all_fixation_maps = []\n",
    "labels = []\n",
    "\n",
    "for image_path, maps in fixation_maps.items():\n",
    "    for fixation_map in maps:\n",
    "        all_fixation_maps.append(fixation_map.flatten())\n",
    "        labels.append(image_path)\n",
    "\n",
    "X = np.array(all_fixation_maps)\n",
    "y = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "876b3b00-8bc1-4900-b166-defae7d7d6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.1098 - loss: 123.9531 - top_k_categorical_accuracy: 0.3096 - val_accuracy: 0.3636 - val_loss: 4.3983 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2812 - loss: 85.3809 - top_k_categorical_accuracy: 0.5443 - val_accuracy: 0.4545 - val_loss: 3.6935 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3328 - loss: 75.3219 - top_k_categorical_accuracy: 0.6353 - val_accuracy: 0.4727 - val_loss: 3.5471 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3984 - loss: 68.1610 - top_k_categorical_accuracy: 0.6650 - val_accuracy: 0.4545 - val_loss: 3.4603 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3962 - loss: 66.7537 - top_k_categorical_accuracy: 0.6669 - val_accuracy: 0.4727 - val_loss: 3.4662 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4479 - loss: 61.7778 - top_k_categorical_accuracy: 0.7013 - val_accuracy: 0.4545 - val_loss: 3.4117 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4547 - loss: 61.0391 - top_k_categorical_accuracy: 0.6920 - val_accuracy: 0.4545 - val_loss: 3.4633 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4692 - loss: 57.2704 - top_k_categorical_accuracy: 0.7168 - val_accuracy: 0.4727 - val_loss: 3.5420 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4988 - loss: 54.7585 - top_k_categorical_accuracy: 0.7223 - val_accuracy: 0.4727 - val_loss: 3.4349 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5330 - loss: 52.4993 - top_k_categorical_accuracy: 0.7384 - val_accuracy: 0.4545 - val_loss: 3.5910 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5380 - loss: 50.4358 - top_k_categorical_accuracy: 0.7488 - val_accuracy: 0.4545 - val_loss: 3.6415 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5226 - loss: 52.2276 - top_k_categorical_accuracy: 0.7324 - val_accuracy: 0.4909 - val_loss: 3.6172 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5415 - loss: 49.6786 - top_k_categorical_accuracy: 0.7467 - val_accuracy: 0.4909 - val_loss: 3.5799 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5393 - loss: 50.5129 - top_k_categorical_accuracy: 0.7319 - val_accuracy: 0.4545 - val_loss: 3.6567 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5382 - loss: 50.8736 - top_k_categorical_accuracy: 0.7373 - val_accuracy: 0.4545 - val_loss: 3.7440 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5735 - loss: 47.0606 - top_k_categorical_accuracy: 0.7636 - val_accuracy: 0.4545 - val_loss: 3.8322 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5606 - loss: 47.6789 - top_k_categorical_accuracy: 0.7453 - val_accuracy: 0.4545 - val_loss: 3.9129 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5905 - loss: 46.1488 - top_k_categorical_accuracy: 0.7535 - val_accuracy: 0.4727 - val_loss: 3.9362 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5747 - loss: 47.1437 - top_k_categorical_accuracy: 0.7540 - val_accuracy: 0.4727 - val_loss: 4.0377 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5782 - loss: 46.3576 - top_k_categorical_accuracy: 0.7627 - val_accuracy: 0.4727 - val_loss: 3.9863 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6036 - loss: 43.9326 - top_k_categorical_accuracy: 0.7719 - val_accuracy: 0.4727 - val_loss: 4.1023 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6061 - loss: 44.6638 - top_k_categorical_accuracy: 0.7573 - val_accuracy: 0.4727 - val_loss: 4.0524 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6225 - loss: 42.2105 - top_k_categorical_accuracy: 0.7667 - val_accuracy: 0.4727 - val_loss: 4.2362 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5910 - loss: 45.0911 - top_k_categorical_accuracy: 0.7592 - val_accuracy: 0.4727 - val_loss: 4.1109 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6010 - loss: 44.3509 - top_k_categorical_accuracy: 0.7540 - val_accuracy: 0.4182 - val_loss: 3.9935 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6232 - loss: 42.9330 - top_k_categorical_accuracy: 0.7713 - val_accuracy: 0.4364 - val_loss: 4.0789 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6013 - loss: 43.4527 - top_k_categorical_accuracy: 0.7717 - val_accuracy: 0.4545 - val_loss: 4.1105 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6225 - loss: 42.9052 - top_k_categorical_accuracy: 0.7659 - val_accuracy: 0.4364 - val_loss: 4.0642 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6145 - loss: 41.9126 - top_k_categorical_accuracy: 0.7709 - val_accuracy: 0.4545 - val_loss: 4.1752 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6185 - loss: 42.3905 - top_k_categorical_accuracy: 0.7638 - val_accuracy: 0.4364 - val_loss: 4.1109 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6291 - loss: 41.1813 - top_k_categorical_accuracy: 0.7750 - val_accuracy: 0.4364 - val_loss: 4.0728 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6266 - loss: 41.2559 - top_k_categorical_accuracy: 0.7754 - val_accuracy: 0.4182 - val_loss: 4.1609 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6266 - loss: 41.3974 - top_k_categorical_accuracy: 0.7682 - val_accuracy: 0.4545 - val_loss: 4.3094 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6312 - loss: 40.9361 - top_k_categorical_accuracy: 0.7726 - val_accuracy: 0.4182 - val_loss: 4.3159 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6358 - loss: 41.1697 - top_k_categorical_accuracy: 0.7548 - val_accuracy: 0.4545 - val_loss: 4.3465 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6399 - loss: 40.5118 - top_k_categorical_accuracy: 0.7682 - val_accuracy: 0.4545 - val_loss: 4.3142 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6349 - loss: 41.1517 - top_k_categorical_accuracy: 0.7670 - val_accuracy: 0.4545 - val_loss: 4.2332 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6205 - loss: 41.8241 - top_k_categorical_accuracy: 0.7539 - val_accuracy: 0.4182 - val_loss: 4.3532 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6447 - loss: 39.8607 - top_k_categorical_accuracy: 0.7788 - val_accuracy: 0.4909 - val_loss: 4.2504 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6618 - loss: 38.4451 - top_k_categorical_accuracy: 0.7814 - val_accuracy: 0.4545 - val_loss: 4.2946 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6415 - loss: 39.0776 - top_k_categorical_accuracy: 0.7861 - val_accuracy: 0.4909 - val_loss: 4.2760 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6446 - loss: 40.3384 - top_k_categorical_accuracy: 0.7755 - val_accuracy: 0.4727 - val_loss: 4.2369 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6601 - loss: 38.2611 - top_k_categorical_accuracy: 0.7803 - val_accuracy: 0.4727 - val_loss: 4.4779 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6421 - loss: 39.6438 - top_k_categorical_accuracy: 0.7776 - val_accuracy: 0.4545 - val_loss: 4.4925 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6586 - loss: 38.9044 - top_k_categorical_accuracy: 0.7890 - val_accuracy: 0.4545 - val_loss: 4.4940 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 46/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6686 - loss: 38.1097 - top_k_categorical_accuracy: 0.7779 - val_accuracy: 0.4364 - val_loss: 4.5602 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Accuracy: 0.4545\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.1005 - loss: 123.0802 - top_k_categorical_accuracy: 0.3071 - val_accuracy: 0.3636 - val_loss: 4.4730 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2992 - loss: 84.3899 - top_k_categorical_accuracy: 0.5582 - val_accuracy: 0.4182 - val_loss: 3.4963 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3301 - loss: 77.1927 - top_k_categorical_accuracy: 0.6034 - val_accuracy: 0.4182 - val_loss: 3.3281 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3799 - loss: 69.0569 - top_k_categorical_accuracy: 0.6515 - val_accuracy: 0.4545 - val_loss: 3.2017 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4173 - loss: 64.7268 - top_k_categorical_accuracy: 0.6912 - val_accuracy: 0.4182 - val_loss: 3.1327 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4387 - loss: 62.0206 - top_k_categorical_accuracy: 0.6930 - val_accuracy: 0.3818 - val_loss: 3.3021 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4457 - loss: 60.7661 - top_k_categorical_accuracy: 0.6970 - val_accuracy: 0.4545 - val_loss: 3.0284 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4729 - loss: 57.8116 - top_k_categorical_accuracy: 0.7062 - val_accuracy: 0.4364 - val_loss: 3.1567 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4926 - loss: 55.6326 - top_k_categorical_accuracy: 0.7322 - val_accuracy: 0.4727 - val_loss: 3.1116 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5242 - loss: 53.7419 - top_k_categorical_accuracy: 0.7363 - val_accuracy: 0.4727 - val_loss: 3.2250 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5072 - loss: 54.4009 - top_k_categorical_accuracy: 0.7192 - val_accuracy: 0.4727 - val_loss: 3.1632 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5426 - loss: 49.6361 - top_k_categorical_accuracy: 0.7476 - val_accuracy: 0.4727 - val_loss: 3.2408 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5139 - loss: 51.6281 - top_k_categorical_accuracy: 0.7292 - val_accuracy: 0.4364 - val_loss: 3.2068 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5519 - loss: 48.7871 - top_k_categorical_accuracy: 0.7603 - val_accuracy: 0.4727 - val_loss: 3.2804 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5563 - loss: 48.0209 - top_k_categorical_accuracy: 0.7520 - val_accuracy: 0.4909 - val_loss: 3.1482 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5597 - loss: 49.4236 - top_k_categorical_accuracy: 0.7427 - val_accuracy: 0.4909 - val_loss: 3.2023 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5645 - loss: 47.1597 - top_k_categorical_accuracy: 0.7581 - val_accuracy: 0.4364 - val_loss: 3.3052 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5876 - loss: 44.7678 - top_k_categorical_accuracy: 0.7773 - val_accuracy: 0.4000 - val_loss: 3.4263 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5924 - loss: 45.3063 - top_k_categorical_accuracy: 0.7705 - val_accuracy: 0.4364 - val_loss: 3.3485 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5798 - loss: 46.8749 - top_k_categorical_accuracy: 0.7596 - val_accuracy: 0.4364 - val_loss: 3.4193 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5712 - loss: 45.8084 - top_k_categorical_accuracy: 0.7587 - val_accuracy: 0.4545 - val_loss: 3.4324 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5879 - loss: 46.0556 - top_k_categorical_accuracy: 0.7513 - val_accuracy: 0.4182 - val_loss: 3.5044 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6116 - loss: 43.6796 - top_k_categorical_accuracy: 0.7671 - val_accuracy: 0.4182 - val_loss: 3.5203 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6209 - loss: 43.2219 - top_k_categorical_accuracy: 0.7757 - val_accuracy: 0.4545 - val_loss: 3.6138 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6002 - loss: 44.4364 - top_k_categorical_accuracy: 0.7675 - val_accuracy: 0.4545 - val_loss: 3.5100 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6183 - loss: 42.0878 - top_k_categorical_accuracy: 0.7762 - val_accuracy: 0.4545 - val_loss: 3.4657 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6202 - loss: 42.2710 - top_k_categorical_accuracy: 0.7709 - val_accuracy: 0.4727 - val_loss: 3.5753 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6340 - loss: 42.4190 - top_k_categorical_accuracy: 0.7673 - val_accuracy: 0.4727 - val_loss: 3.6126 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6354 - loss: 41.5658 - top_k_categorical_accuracy: 0.7896 - val_accuracy: 0.4727 - val_loss: 3.5949 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6212 - loss: 43.2276 - top_k_categorical_accuracy: 0.7553 - val_accuracy: 0.4364 - val_loss: 3.7372 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6033 - loss: 43.9041 - top_k_categorical_accuracy: 0.7652 - val_accuracy: 0.4182 - val_loss: 3.7591 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6227 - loss: 40.9725 - top_k_categorical_accuracy: 0.7817 - val_accuracy: 0.4182 - val_loss: 3.7561 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6183 - loss: 42.8723 - top_k_categorical_accuracy: 0.7704 - val_accuracy: 0.4000 - val_loss: 3.7640 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6206 - loss: 41.8997 - top_k_categorical_accuracy: 0.7680 - val_accuracy: 0.4182 - val_loss: 3.8428 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6348 - loss: 41.4400 - top_k_categorical_accuracy: 0.7731 - val_accuracy: 0.4000 - val_loss: 3.8495 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6530 - loss: 38.4314 - top_k_categorical_accuracy: 0.7989 - val_accuracy: 0.4182 - val_loss: 3.8513 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6321 - loss: 40.7489 - top_k_categorical_accuracy: 0.7823 - val_accuracy: 0.4182 - val_loss: 3.7895 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6452 - loss: 40.1161 - top_k_categorical_accuracy: 0.7714 - val_accuracy: 0.4545 - val_loss: 3.9396 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6425 - loss: 40.6201 - top_k_categorical_accuracy: 0.7717 - val_accuracy: 0.4364 - val_loss: 3.8747 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6559 - loss: 39.4555 - top_k_categorical_accuracy: 0.7742 - val_accuracy: 0.4000 - val_loss: 3.9453 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6506 - loss: 39.6345 - top_k_categorical_accuracy: 0.7847 - val_accuracy: 0.4182 - val_loss: 4.0281 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6550 - loss: 39.5077 - top_k_categorical_accuracy: 0.7763 - val_accuracy: 0.4182 - val_loss: 4.0245 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6464 - loss: 40.7023 - top_k_categorical_accuracy: 0.7779 - val_accuracy: 0.4182 - val_loss: 4.0169 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6459 - loss: 39.5664 - top_k_categorical_accuracy: 0.7871 - val_accuracy: 0.4000 - val_loss: 4.0973 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6418 - loss: 39.2389 - top_k_categorical_accuracy: 0.7867 - val_accuracy: 0.4727 - val_loss: 3.9566 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 46/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6611 - loss: 38.2510 - top_k_categorical_accuracy: 0.7858 - val_accuracy: 0.3818 - val_loss: 4.0582 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 47/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6430 - loss: 39.8434 - top_k_categorical_accuracy: 0.7789 - val_accuracy: 0.3636 - val_loss: 3.9900 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.0909 - loss: 125.5959 - top_k_categorical_accuracy: 0.2806 - val_accuracy: 0.3636 - val_loss: 4.8237 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2907 - loss: 86.0807 - top_k_categorical_accuracy: 0.5341 - val_accuracy: 0.3455 - val_loss: 4.2624 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3528 - loss: 74.3251 - top_k_categorical_accuracy: 0.6225 - val_accuracy: 0.3455 - val_loss: 3.6710 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3792 - loss: 69.7326 - top_k_categorical_accuracy: 0.6397 - val_accuracy: 0.3455 - val_loss: 3.6903 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4112 - loss: 65.1641 - top_k_categorical_accuracy: 0.6691 - val_accuracy: 0.3818 - val_loss: 3.6967 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4507 - loss: 61.0490 - top_k_categorical_accuracy: 0.6928 - val_accuracy: 0.4000 - val_loss: 3.5201 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4849 - loss: 56.8568 - top_k_categorical_accuracy: 0.7277 - val_accuracy: 0.3636 - val_loss: 3.5521 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4907 - loss: 56.3137 - top_k_categorical_accuracy: 0.7011 - val_accuracy: 0.3636 - val_loss: 3.5592 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5203 - loss: 54.9394 - top_k_categorical_accuracy: 0.7098 - val_accuracy: 0.3636 - val_loss: 3.5716 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5003 - loss: 55.1303 - top_k_categorical_accuracy: 0.7238 - val_accuracy: 0.3818 - val_loss: 3.4407 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5221 - loss: 51.9496 - top_k_categorical_accuracy: 0.7371 - val_accuracy: 0.3818 - val_loss: 3.5428 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5324 - loss: 52.2277 - top_k_categorical_accuracy: 0.7242 - val_accuracy: 0.3818 - val_loss: 3.6796 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5479 - loss: 49.9706 - top_k_categorical_accuracy: 0.7350 - val_accuracy: 0.3818 - val_loss: 3.7796 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5181 - loss: 52.6961 - top_k_categorical_accuracy: 0.7180 - val_accuracy: 0.3818 - val_loss: 3.8715 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5645 - loss: 47.9639 - top_k_categorical_accuracy: 0.7574 - val_accuracy: 0.4000 - val_loss: 3.7755 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5570 - loss: 48.5544 - top_k_categorical_accuracy: 0.7491 - val_accuracy: 0.3636 - val_loss: 3.7280 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5705 - loss: 47.0716 - top_k_categorical_accuracy: 0.7505 - val_accuracy: 0.3818 - val_loss: 3.8554 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5553 - loss: 48.3821 - top_k_categorical_accuracy: 0.7412 - val_accuracy: 0.4000 - val_loss: 3.8252 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6045 - loss: 44.3229 - top_k_categorical_accuracy: 0.7713 - val_accuracy: 0.4000 - val_loss: 3.7123 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5922 - loss: 45.3451 - top_k_categorical_accuracy: 0.7647 - val_accuracy: 0.4182 - val_loss: 3.8068 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5801 - loss: 45.4889 - top_k_categorical_accuracy: 0.7637 - val_accuracy: 0.3455 - val_loss: 3.9505 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5930 - loss: 45.8910 - top_k_categorical_accuracy: 0.7533 - val_accuracy: 0.3636 - val_loss: 3.9224 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6004 - loss: 44.9086 - top_k_categorical_accuracy: 0.7503 - val_accuracy: 0.4000 - val_loss: 4.0880 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6040 - loss: 43.5074 - top_k_categorical_accuracy: 0.7632 - val_accuracy: 0.3818 - val_loss: 4.1260 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6063 - loss: 44.1238 - top_k_categorical_accuracy: 0.7679 - val_accuracy: 0.3818 - val_loss: 4.1705 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6038 - loss: 43.4791 - top_k_categorical_accuracy: 0.7684 - val_accuracy: 0.4364 - val_loss: 4.1860 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6348 - loss: 41.9024 - top_k_categorical_accuracy: 0.7730 - val_accuracy: 0.4000 - val_loss: 4.2756 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6354 - loss: 41.5653 - top_k_categorical_accuracy: 0.7687 - val_accuracy: 0.3818 - val_loss: 4.2033 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6372 - loss: 40.8142 - top_k_categorical_accuracy: 0.7775 - val_accuracy: 0.4000 - val_loss: 4.4300 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6364 - loss: 41.4256 - top_k_categorical_accuracy: 0.7694 - val_accuracy: 0.3636 - val_loss: 4.3422 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6177 - loss: 41.7277 - top_k_categorical_accuracy: 0.7809 - val_accuracy: 0.3818 - val_loss: 4.4703 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6230 - loss: 42.6819 - top_k_categorical_accuracy: 0.7602 - val_accuracy: 0.3455 - val_loss: 4.5311 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6270 - loss: 40.9266 - top_k_categorical_accuracy: 0.7871 - val_accuracy: 0.3273 - val_loss: 4.3598 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6156 - loss: 43.9402 - top_k_categorical_accuracy: 0.7498 - val_accuracy: 0.3818 - val_loss: 4.2217 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6424 - loss: 39.7306 - top_k_categorical_accuracy: 0.7858 - val_accuracy: 0.3455 - val_loss: 4.4218 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6334 - loss: 41.3345 - top_k_categorical_accuracy: 0.7759 - val_accuracy: 0.3455 - val_loss: 4.5678 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6418 - loss: 40.1603 - top_k_categorical_accuracy: 0.7803 - val_accuracy: 0.3636 - val_loss: 4.4597 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6322 - loss: 41.7274 - top_k_categorical_accuracy: 0.7620 - val_accuracy: 0.3273 - val_loss: 4.4572 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6370 - loss: 39.9919 - top_k_categorical_accuracy: 0.7830 - val_accuracy: 0.4000 - val_loss: 4.3759 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6543 - loss: 39.3178 - top_k_categorical_accuracy: 0.7764 - val_accuracy: 0.4000 - val_loss: 4.5658 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6340 - loss: 40.8188 - top_k_categorical_accuracy: 0.7699 - val_accuracy: 0.3818 - val_loss: 4.5516 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6616 - loss: 38.2821 - top_k_categorical_accuracy: 0.7952 - val_accuracy: 0.3636 - val_loss: 4.7583 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6443 - loss: 39.8650 - top_k_categorical_accuracy: 0.7678 - val_accuracy: 0.4000 - val_loss: 4.7758 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6591 - loss: 38.7115 - top_k_categorical_accuracy: 0.7857 - val_accuracy: 0.3636 - val_loss: 4.9212 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6586 - loss: 38.9367 - top_k_categorical_accuracy: 0.7785 - val_accuracy: 0.4182 - val_loss: 4.7282 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 46/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6451 - loss: 39.5138 - top_k_categorical_accuracy: 0.7775 - val_accuracy: 0.4182 - val_loss: 4.6354 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 47/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6468 - loss: 39.8168 - top_k_categorical_accuracy: 0.7789 - val_accuracy: 0.4364 - val_loss: 4.6924 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 48/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6524 - loss: 40.0317 - top_k_categorical_accuracy: 0.7677 - val_accuracy: 0.3818 - val_loss: 4.7589 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 49/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6609 - loss: 38.5532 - top_k_categorical_accuracy: 0.7837 - val_accuracy: 0.4182 - val_loss: 4.6250 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 50/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6593 - loss: 38.4035 - top_k_categorical_accuracy: 0.7826 - val_accuracy: 0.3273 - val_loss: 4.8923 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0874 - loss: 128.0209 - top_k_categorical_accuracy: 0.3000 - val_accuracy: 0.3273 - val_loss: 4.7405 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2830 - loss: 87.3613 - top_k_categorical_accuracy: 0.5509 - val_accuracy: 0.4000 - val_loss: 3.8625 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3340 - loss: 77.1460 - top_k_categorical_accuracy: 0.6173 - val_accuracy: 0.4182 - val_loss: 3.6419 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3954 - loss: 69.6081 - top_k_categorical_accuracy: 0.6616 - val_accuracy: 0.4727 - val_loss: 3.4919 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4143 - loss: 63.7480 - top_k_categorical_accuracy: 0.6878 - val_accuracy: 0.4182 - val_loss: 3.4413 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4387 - loss: 61.2202 - top_k_categorical_accuracy: 0.6938 - val_accuracy: 0.4182 - val_loss: 3.3925 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4573 - loss: 60.7031 - top_k_categorical_accuracy: 0.6903 - val_accuracy: 0.4364 - val_loss: 3.4402 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4624 - loss: 59.2891 - top_k_categorical_accuracy: 0.7003 - val_accuracy: 0.4182 - val_loss: 3.4802 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4937 - loss: 55.5963 - top_k_categorical_accuracy: 0.7207 - val_accuracy: 0.4364 - val_loss: 3.5193 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4947 - loss: 54.8954 - top_k_categorical_accuracy: 0.7226 - val_accuracy: 0.4364 - val_loss: 3.6452 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4996 - loss: 54.1466 - top_k_categorical_accuracy: 0.7298 - val_accuracy: 0.4364 - val_loss: 3.6326 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5445 - loss: 50.9298 - top_k_categorical_accuracy: 0.7474 - val_accuracy: 0.4364 - val_loss: 3.6436 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5190 - loss: 52.2062 - top_k_categorical_accuracy: 0.7303 - val_accuracy: 0.4182 - val_loss: 3.6121 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5444 - loss: 50.2918 - top_k_categorical_accuracy: 0.7447 - val_accuracy: 0.4182 - val_loss: 3.6151 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5455 - loss: 49.6222 - top_k_categorical_accuracy: 0.7405 - val_accuracy: 0.4364 - val_loss: 3.5435 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5677 - loss: 48.3705 - top_k_categorical_accuracy: 0.7543 - val_accuracy: 0.4182 - val_loss: 3.7614 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5649 - loss: 48.0120 - top_k_categorical_accuracy: 0.7561 - val_accuracy: 0.4182 - val_loss: 3.7613 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5650 - loss: 47.8336 - top_k_categorical_accuracy: 0.7524 - val_accuracy: 0.4000 - val_loss: 3.8755 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5712 - loss: 47.5885 - top_k_categorical_accuracy: 0.7483 - val_accuracy: 0.4000 - val_loss: 3.7631 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5864 - loss: 45.7661 - top_k_categorical_accuracy: 0.7765 - val_accuracy: 0.4000 - val_loss: 3.8929 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5609 - loss: 48.9264 - top_k_categorical_accuracy: 0.7436 - val_accuracy: 0.4000 - val_loss: 3.9409 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5824 - loss: 46.7129 - top_k_categorical_accuracy: 0.7499 - val_accuracy: 0.4000 - val_loss: 4.0026 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5771 - loss: 45.2812 - top_k_categorical_accuracy: 0.7634 - val_accuracy: 0.4182 - val_loss: 4.0131 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5932 - loss: 45.3733 - top_k_categorical_accuracy: 0.7523 - val_accuracy: 0.4364 - val_loss: 4.0847 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5950 - loss: 44.5623 - top_k_categorical_accuracy: 0.7481 - val_accuracy: 0.4000 - val_loss: 4.2066 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6109 - loss: 42.8772 - top_k_categorical_accuracy: 0.7667 - val_accuracy: 0.4364 - val_loss: 4.1752 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6043 - loss: 44.8919 - top_k_categorical_accuracy: 0.7519 - val_accuracy: 0.3818 - val_loss: 4.1770 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6120 - loss: 44.8142 - top_k_categorical_accuracy: 0.7444 - val_accuracy: 0.4364 - val_loss: 4.0979 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6135 - loss: 44.4390 - top_k_categorical_accuracy: 0.7572 - val_accuracy: 0.4364 - val_loss: 4.1440 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6145 - loss: 42.5872 - top_k_categorical_accuracy: 0.7580 - val_accuracy: 0.4545 - val_loss: 4.1167 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6193 - loss: 42.3039 - top_k_categorical_accuracy: 0.7779 - val_accuracy: 0.4000 - val_loss: 4.1887 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6127 - loss: 43.0653 - top_k_categorical_accuracy: 0.7646 - val_accuracy: 0.4182 - val_loss: 4.2686 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6203 - loss: 42.5432 - top_k_categorical_accuracy: 0.7740 - val_accuracy: 0.4545 - val_loss: 4.0788 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6188 - loss: 40.9733 - top_k_categorical_accuracy: 0.7896 - val_accuracy: 0.4000 - val_loss: 4.1491 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6260 - loss: 41.5701 - top_k_categorical_accuracy: 0.7767 - val_accuracy: 0.4000 - val_loss: 4.2333 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6344 - loss: 40.0570 - top_k_categorical_accuracy: 0.7818 - val_accuracy: 0.3636 - val_loss: 4.3075 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6335 - loss: 39.4601 - top_k_categorical_accuracy: 0.7832 - val_accuracy: 0.4545 - val_loss: 4.2755 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6350 - loss: 41.0858 - top_k_categorical_accuracy: 0.7702 - val_accuracy: 0.3818 - val_loss: 4.3392 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6355 - loss: 41.4028 - top_k_categorical_accuracy: 0.7723 - val_accuracy: 0.4182 - val_loss: 4.3805 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6265 - loss: 41.4675 - top_k_categorical_accuracy: 0.7720 - val_accuracy: 0.4000 - val_loss: 4.5144 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6347 - loss: 40.1639 - top_k_categorical_accuracy: 0.7659 - val_accuracy: 0.3818 - val_loss: 4.4968 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6412 - loss: 40.9279 - top_k_categorical_accuracy: 0.7622 - val_accuracy: 0.4182 - val_loss: 4.5704 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6563 - loss: 38.4013 - top_k_categorical_accuracy: 0.7852 - val_accuracy: 0.3818 - val_loss: 4.5120 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6524 - loss: 40.0560 - top_k_categorical_accuracy: 0.7810 - val_accuracy: 0.3636 - val_loss: 4.5908 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6425 - loss: 40.2145 - top_k_categorical_accuracy: 0.7712 - val_accuracy: 0.4000 - val_loss: 4.8014 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 46/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6501 - loss: 39.1512 - top_k_categorical_accuracy: 0.7834 - val_accuracy: 0.4182 - val_loss: 4.8043 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.0841 - loss: 130.4024 - top_k_categorical_accuracy: 0.2587 - val_accuracy: 0.4182 - val_loss: 4.0372 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2733 - loss: 87.7552 - top_k_categorical_accuracy: 0.5459 - val_accuracy: 0.4909 - val_loss: 3.6303 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3474 - loss: 75.9383 - top_k_categorical_accuracy: 0.6198 - val_accuracy: 0.4909 - val_loss: 3.3166 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3737 - loss: 69.6168 - top_k_categorical_accuracy: 0.6492 - val_accuracy: 0.4545 - val_loss: 3.4441 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4111 - loss: 64.4110 - top_k_categorical_accuracy: 0.6786 - val_accuracy: 0.4545 - val_loss: 3.3306 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4380 - loss: 61.7270 - top_k_categorical_accuracy: 0.7000 - val_accuracy: 0.4545 - val_loss: 3.3238 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4510 - loss: 61.3885 - top_k_categorical_accuracy: 0.6825 - val_accuracy: 0.4364 - val_loss: 3.3786 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4686 - loss: 57.5197 - top_k_categorical_accuracy: 0.6997 - val_accuracy: 0.4727 - val_loss: 3.6038 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4815 - loss: 55.7653 - top_k_categorical_accuracy: 0.7119 - val_accuracy: 0.4545 - val_loss: 3.5224 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5156 - loss: 53.4368 - top_k_categorical_accuracy: 0.7275 - val_accuracy: 0.4364 - val_loss: 3.5128 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4929 - loss: 53.9445 - top_k_categorical_accuracy: 0.7155 - val_accuracy: 0.4727 - val_loss: 3.5146 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5284 - loss: 52.7995 - top_k_categorical_accuracy: 0.7170 - val_accuracy: 0.4182 - val_loss: 3.5299 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5370 - loss: 50.0779 - top_k_categorical_accuracy: 0.7506 - val_accuracy: 0.4364 - val_loss: 3.4986 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5662 - loss: 47.7955 - top_k_categorical_accuracy: 0.7493 - val_accuracy: 0.4364 - val_loss: 3.6199 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5492 - loss: 49.6297 - top_k_categorical_accuracy: 0.7502 - val_accuracy: 0.4727 - val_loss: 3.6005 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5477 - loss: 49.6280 - top_k_categorical_accuracy: 0.7314 - val_accuracy: 0.4000 - val_loss: 3.6944 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5852 - loss: 46.6448 - top_k_categorical_accuracy: 0.7448 - val_accuracy: 0.4545 - val_loss: 3.8092 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5816 - loss: 46.5756 - top_k_categorical_accuracy: 0.7509 - val_accuracy: 0.4364 - val_loss: 3.7143 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5861 - loss: 46.8782 - top_k_categorical_accuracy: 0.7478 - val_accuracy: 0.4909 - val_loss: 3.7324 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5949 - loss: 45.7014 - top_k_categorical_accuracy: 0.7550 - val_accuracy: 0.4545 - val_loss: 3.7202 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5892 - loss: 45.3527 - top_k_categorical_accuracy: 0.7600 - val_accuracy: 0.4000 - val_loss: 3.9353 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6075 - loss: 44.3330 - top_k_categorical_accuracy: 0.7636 - val_accuracy: 0.4545 - val_loss: 3.8722 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6056 - loss: 43.6446 - top_k_categorical_accuracy: 0.7615 - val_accuracy: 0.4364 - val_loss: 3.8707 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6037 - loss: 43.5295 - top_k_categorical_accuracy: 0.7626 - val_accuracy: 0.4364 - val_loss: 3.9307 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6129 - loss: 43.7513 - top_k_categorical_accuracy: 0.7635 - val_accuracy: 0.4364 - val_loss: 3.9951 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6081 - loss: 44.6445 - top_k_categorical_accuracy: 0.7540 - val_accuracy: 0.4364 - val_loss: 3.9976 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6196 - loss: 42.9288 - top_k_categorical_accuracy: 0.7627 - val_accuracy: 0.4364 - val_loss: 3.9533 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6186 - loss: 42.4010 - top_k_categorical_accuracy: 0.7682 - val_accuracy: 0.4545 - val_loss: 4.0383 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6233 - loss: 42.0824 - top_k_categorical_accuracy: 0.7720 - val_accuracy: 0.4545 - val_loss: 3.9997 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6416 - loss: 40.8788 - top_k_categorical_accuracy: 0.7798 - val_accuracy: 0.4727 - val_loss: 3.9967 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6445 - loss: 40.1971 - top_k_categorical_accuracy: 0.7789 - val_accuracy: 0.4545 - val_loss: 3.9423 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6374 - loss: 40.5798 - top_k_categorical_accuracy: 0.7855 - val_accuracy: 0.4545 - val_loss: 4.0011 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6275 - loss: 42.5668 - top_k_categorical_accuracy: 0.7509 - val_accuracy: 0.4182 - val_loss: 4.0862 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6448 - loss: 40.4304 - top_k_categorical_accuracy: 0.7815 - val_accuracy: 0.4364 - val_loss: 4.0873 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6302 - loss: 41.4991 - top_k_categorical_accuracy: 0.7573 - val_accuracy: 0.4909 - val_loss: 4.2107 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6539 - loss: 40.0628 - top_k_categorical_accuracy: 0.7788 - val_accuracy: 0.4545 - val_loss: 4.2403 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6261 - loss: 41.5687 - top_k_categorical_accuracy: 0.7779 - val_accuracy: 0.4727 - val_loss: 4.1503 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6390 - loss: 40.9788 - top_k_categorical_accuracy: 0.7738 - val_accuracy: 0.4727 - val_loss: 4.2996 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6579 - loss: 39.1180 - top_k_categorical_accuracy: 0.7848 - val_accuracy: 0.4545 - val_loss: 4.2980 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6287 - loss: 41.8246 - top_k_categorical_accuracy: 0.7672 - val_accuracy: 0.4364 - val_loss: 4.3319 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6304 - loss: 40.3827 - top_k_categorical_accuracy: 0.7811 - val_accuracy: 0.4364 - val_loss: 4.2271 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6492 - loss: 39.6069 - top_k_categorical_accuracy: 0.7728 - val_accuracy: 0.4909 - val_loss: 4.2643 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6479 - loss: 39.7616 - top_k_categorical_accuracy: 0.7728 - val_accuracy: 0.4727 - val_loss: 4.3316 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Accuracy: 0.4909\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.0918 - loss: 125.4345 - top_k_categorical_accuracy: 0.2950 - val_accuracy: 0.4182 - val_loss: 3.7277 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2613 - loss: 87.0699 - top_k_categorical_accuracy: 0.5430 - val_accuracy: 0.4545 - val_loss: 3.5540 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3285 - loss: 76.3019 - top_k_categorical_accuracy: 0.6162 - val_accuracy: 0.4364 - val_loss: 3.3076 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3873 - loss: 68.4902 - top_k_categorical_accuracy: 0.6786 - val_accuracy: 0.4909 - val_loss: 3.2903 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4176 - loss: 65.0124 - top_k_categorical_accuracy: 0.6700 - val_accuracy: 0.4364 - val_loss: 3.2206 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4492 - loss: 61.8646 - top_k_categorical_accuracy: 0.6843 - val_accuracy: 0.4909 - val_loss: 3.2032 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4700 - loss: 58.7056 - top_k_categorical_accuracy: 0.6979 - val_accuracy: 0.5273 - val_loss: 3.1605 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4787 - loss: 56.7718 - top_k_categorical_accuracy: 0.7126 - val_accuracy: 0.5091 - val_loss: 3.1648 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4947 - loss: 54.9366 - top_k_categorical_accuracy: 0.7142 - val_accuracy: 0.5091 - val_loss: 3.2027 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5016 - loss: 54.6939 - top_k_categorical_accuracy: 0.7070 - val_accuracy: 0.4909 - val_loss: 3.0668 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5046 - loss: 54.0699 - top_k_categorical_accuracy: 0.7278 - val_accuracy: 0.5273 - val_loss: 3.1447 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5078 - loss: 54.8993 - top_k_categorical_accuracy: 0.7127 - val_accuracy: 0.5273 - val_loss: 3.1879 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5260 - loss: 52.0194 - top_k_categorical_accuracy: 0.7295 - val_accuracy: 0.5273 - val_loss: 3.1033 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5435 - loss: 50.8647 - top_k_categorical_accuracy: 0.7312 - val_accuracy: 0.5273 - val_loss: 3.2088 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5569 - loss: 49.9254 - top_k_categorical_accuracy: 0.7322 - val_accuracy: 0.4909 - val_loss: 3.2658 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5702 - loss: 47.4165 - top_k_categorical_accuracy: 0.7582 - val_accuracy: 0.5091 - val_loss: 3.2566 - val_top_k_categorical_accuracy: 0.8545 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5736 - loss: 48.2029 - top_k_categorical_accuracy: 0.7469 - val_accuracy: 0.4909 - val_loss: 3.3154 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5624 - loss: 48.0540 - top_k_categorical_accuracy: 0.7478 - val_accuracy: 0.5091 - val_loss: 3.2340 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5653 - loss: 48.1432 - top_k_categorical_accuracy: 0.7420 - val_accuracy: 0.5273 - val_loss: 3.2862 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5833 - loss: 46.9298 - top_k_categorical_accuracy: 0.7445 - val_accuracy: 0.4727 - val_loss: 3.2747 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6069 - loss: 44.7337 - top_k_categorical_accuracy: 0.7675 - val_accuracy: 0.5091 - val_loss: 3.3866 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5721 - loss: 45.9620 - top_k_categorical_accuracy: 0.7617 - val_accuracy: 0.4545 - val_loss: 3.3496 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5777 - loss: 47.0920 - top_k_categorical_accuracy: 0.7481 - val_accuracy: 0.4909 - val_loss: 3.3151 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6046 - loss: 44.7347 - top_k_categorical_accuracy: 0.7527 - val_accuracy: 0.4545 - val_loss: 3.3624 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5691 - loss: 47.7136 - top_k_categorical_accuracy: 0.7317 - val_accuracy: 0.4182 - val_loss: 3.5191 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5912 - loss: 46.2245 - top_k_categorical_accuracy: 0.7461 - val_accuracy: 0.4545 - val_loss: 3.4786 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6145 - loss: 44.0744 - top_k_categorical_accuracy: 0.7463 - val_accuracy: 0.4182 - val_loss: 3.4070 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5784 - loss: 46.1881 - top_k_categorical_accuracy: 0.7534 - val_accuracy: 0.4364 - val_loss: 3.4249 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6144 - loss: 43.2884 - top_k_categorical_accuracy: 0.7649 - val_accuracy: 0.4364 - val_loss: 3.4354 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6313 - loss: 40.8842 - top_k_categorical_accuracy: 0.7820 - val_accuracy: 0.4727 - val_loss: 3.4263 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6214 - loss: 43.3110 - top_k_categorical_accuracy: 0.7592 - val_accuracy: 0.4727 - val_loss: 3.6225 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6461 - loss: 40.9803 - top_k_categorical_accuracy: 0.7799 - val_accuracy: 0.4727 - val_loss: 3.7996 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6263 - loss: 42.2271 - top_k_categorical_accuracy: 0.7685 - val_accuracy: 0.4545 - val_loss: 3.8337 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6293 - loss: 42.2594 - top_k_categorical_accuracy: 0.7690 - val_accuracy: 0.4727 - val_loss: 3.6791 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6226 - loss: 42.1003 - top_k_categorical_accuracy: 0.7649 - val_accuracy: 0.4727 - val_loss: 3.8666 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6222 - loss: 42.1081 - top_k_categorical_accuracy: 0.7750 - val_accuracy: 0.4909 - val_loss: 3.6123 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6323 - loss: 41.6284 - top_k_categorical_accuracy: 0.7800 - val_accuracy: 0.4727 - val_loss: 3.6231 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6140 - loss: 43.4922 - top_k_categorical_accuracy: 0.7447 - val_accuracy: 0.4545 - val_loss: 3.6146 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6469 - loss: 41.4762 - top_k_categorical_accuracy: 0.7584 - val_accuracy: 0.4727 - val_loss: 3.6756 - val_top_k_categorical_accuracy: 0.8545 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6469 - loss: 39.2382 - top_k_categorical_accuracy: 0.7735 - val_accuracy: 0.4545 - val_loss: 3.5891 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6582 - loss: 40.7319 - top_k_categorical_accuracy: 0.7682 - val_accuracy: 0.4909 - val_loss: 3.7172 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6219 - loss: 42.4440 - top_k_categorical_accuracy: 0.7599 - val_accuracy: 0.4545 - val_loss: 3.9075 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6528 - loss: 38.6138 - top_k_categorical_accuracy: 0.7909 - val_accuracy: 0.4909 - val_loss: 3.9474 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6328 - loss: 40.4906 - top_k_categorical_accuracy: 0.7783 - val_accuracy: 0.4727 - val_loss: 3.8789 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6517 - loss: 39.8452 - top_k_categorical_accuracy: 0.7709 - val_accuracy: 0.5091 - val_loss: 4.1221 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 46/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6514 - loss: 39.3597 - top_k_categorical_accuracy: 0.7870 - val_accuracy: 0.4909 - val_loss: 3.9701 - val_top_k_categorical_accuracy: 0.8545 - learning_rate: 0.0010\n",
      "Epoch 47/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6450 - loss: 40.0940 - top_k_categorical_accuracy: 0.7774 - val_accuracy: 0.4545 - val_loss: 3.9191 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 48/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6398 - loss: 39.5954 - top_k_categorical_accuracy: 0.7652 - val_accuracy: 0.4727 - val_loss: 3.9440 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 49/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6498 - loss: 38.5563 - top_k_categorical_accuracy: 0.7877 - val_accuracy: 0.4727 - val_loss: 4.0024 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 50/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6632 - loss: 37.9535 - top_k_categorical_accuracy: 0.7951 - val_accuracy: 0.4364 - val_loss: 4.0639 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.1036 - loss: 124.6961 - top_k_categorical_accuracy: 0.3035 - val_accuracy: 0.3273 - val_loss: 5.2335 - val_top_k_categorical_accuracy: 0.5636 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2534 - loss: 86.2640 - top_k_categorical_accuracy: 0.5570 - val_accuracy: 0.3091 - val_loss: 4.3778 - val_top_k_categorical_accuracy: 0.6182 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3553 - loss: 73.9314 - top_k_categorical_accuracy: 0.6387 - val_accuracy: 0.4000 - val_loss: 4.1269 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3833 - loss: 68.0642 - top_k_categorical_accuracy: 0.6505 - val_accuracy: 0.4000 - val_loss: 3.9734 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4237 - loss: 65.3082 - top_k_categorical_accuracy: 0.6731 - val_accuracy: 0.3818 - val_loss: 3.8116 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4498 - loss: 60.8917 - top_k_categorical_accuracy: 0.6987 - val_accuracy: 0.4000 - val_loss: 3.8989 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4835 - loss: 58.9304 - top_k_categorical_accuracy: 0.7076 - val_accuracy: 0.3636 - val_loss: 3.8275 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4914 - loss: 56.1388 - top_k_categorical_accuracy: 0.7111 - val_accuracy: 0.4182 - val_loss: 3.8960 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4916 - loss: 54.6784 - top_k_categorical_accuracy: 0.7265 - val_accuracy: 0.3818 - val_loss: 3.8270 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5186 - loss: 53.3548 - top_k_categorical_accuracy: 0.7407 - val_accuracy: 0.3818 - val_loss: 3.8330 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5347 - loss: 51.4412 - top_k_categorical_accuracy: 0.7478 - val_accuracy: 0.3818 - val_loss: 3.9038 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5116 - loss: 52.0996 - top_k_categorical_accuracy: 0.7351 - val_accuracy: 0.4545 - val_loss: 3.9429 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5360 - loss: 51.3445 - top_k_categorical_accuracy: 0.7324 - val_accuracy: 0.3818 - val_loss: 4.1531 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5495 - loss: 50.0660 - top_k_categorical_accuracy: 0.7414 - val_accuracy: 0.4182 - val_loss: 4.1290 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5605 - loss: 47.3923 - top_k_categorical_accuracy: 0.7536 - val_accuracy: 0.4000 - val_loss: 4.2258 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5634 - loss: 47.3386 - top_k_categorical_accuracy: 0.7589 - val_accuracy: 0.3818 - val_loss: 4.3224 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5871 - loss: 44.5444 - top_k_categorical_accuracy: 0.7717 - val_accuracy: 0.4000 - val_loss: 4.2977 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5614 - loss: 47.2695 - top_k_categorical_accuracy: 0.7651 - val_accuracy: 0.4182 - val_loss: 4.3301 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5705 - loss: 47.0232 - top_k_categorical_accuracy: 0.7542 - val_accuracy: 0.4182 - val_loss: 4.4174 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5920 - loss: 45.3259 - top_k_categorical_accuracy: 0.7536 - val_accuracy: 0.4182 - val_loss: 4.2766 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6005 - loss: 45.1021 - top_k_categorical_accuracy: 0.7730 - val_accuracy: 0.4182 - val_loss: 4.3989 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5989 - loss: 44.3679 - top_k_categorical_accuracy: 0.7594 - val_accuracy: 0.4000 - val_loss: 4.4118 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6014 - loss: 45.0491 - top_k_categorical_accuracy: 0.7521 - val_accuracy: 0.4545 - val_loss: 4.5128 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6025 - loss: 43.8779 - top_k_categorical_accuracy: 0.7651 - val_accuracy: 0.4364 - val_loss: 4.4972 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6222 - loss: 42.1801 - top_k_categorical_accuracy: 0.7790 - val_accuracy: 0.3818 - val_loss: 4.5117 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6071 - loss: 43.7755 - top_k_categorical_accuracy: 0.7628 - val_accuracy: 0.4545 - val_loss: 4.5719 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6364 - loss: 41.2408 - top_k_categorical_accuracy: 0.7845 - val_accuracy: 0.4545 - val_loss: 4.4864 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5929 - loss: 44.9872 - top_k_categorical_accuracy: 0.7571 - val_accuracy: 0.4727 - val_loss: 4.5989 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6013 - loss: 43.0421 - top_k_categorical_accuracy: 0.7668 - val_accuracy: 0.4364 - val_loss: 4.6697 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6206 - loss: 42.2810 - top_k_categorical_accuracy: 0.7616 - val_accuracy: 0.4364 - val_loss: 4.6967 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6153 - loss: 42.5862 - top_k_categorical_accuracy: 0.7554 - val_accuracy: 0.4545 - val_loss: 4.7492 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6388 - loss: 40.8370 - top_k_categorical_accuracy: 0.7746 - val_accuracy: 0.4364 - val_loss: 4.7253 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6288 - loss: 41.7109 - top_k_categorical_accuracy: 0.7674 - val_accuracy: 0.4727 - val_loss: 4.9422 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6134 - loss: 42.2552 - top_k_categorical_accuracy: 0.7648 - val_accuracy: 0.4545 - val_loss: 4.7760 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6420 - loss: 40.8081 - top_k_categorical_accuracy: 0.7667 - val_accuracy: 0.4727 - val_loss: 4.8166 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6292 - loss: 40.5185 - top_k_categorical_accuracy: 0.7802 - val_accuracy: 0.4545 - val_loss: 4.9643 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6538 - loss: 39.4898 - top_k_categorical_accuracy: 0.7626 - val_accuracy: 0.4364 - val_loss: 4.8839 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6216 - loss: 42.7602 - top_k_categorical_accuracy: 0.7599 - val_accuracy: 0.4727 - val_loss: 5.0206 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6441 - loss: 39.4122 - top_k_categorical_accuracy: 0.7890 - val_accuracy: 0.4182 - val_loss: 5.0269 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6541 - loss: 39.1972 - top_k_categorical_accuracy: 0.7712 - val_accuracy: 0.4364 - val_loss: 5.0401 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6493 - loss: 39.5248 - top_k_categorical_accuracy: 0.7705 - val_accuracy: 0.4000 - val_loss: 5.0932 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6605 - loss: 38.9792 - top_k_categorical_accuracy: 0.7840 - val_accuracy: 0.4727 - val_loss: 5.1197 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6446 - loss: 40.7440 - top_k_categorical_accuracy: 0.7748 - val_accuracy: 0.4727 - val_loss: 5.0137 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6531 - loss: 39.1329 - top_k_categorical_accuracy: 0.7903 - val_accuracy: 0.4182 - val_loss: 5.1029 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6512 - loss: 38.5227 - top_k_categorical_accuracy: 0.7898 - val_accuracy: 0.4364 - val_loss: 5.0730 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.0886 - loss: 126.4799 - top_k_categorical_accuracy: 0.2880 - val_accuracy: 0.4000 - val_loss: 4.2583 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2846 - loss: 86.1905 - top_k_categorical_accuracy: 0.5455 - val_accuracy: 0.4727 - val_loss: 3.8047 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3308 - loss: 76.4045 - top_k_categorical_accuracy: 0.6134 - val_accuracy: 0.4545 - val_loss: 3.5221 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3645 - loss: 72.7552 - top_k_categorical_accuracy: 0.6276 - val_accuracy: 0.5091 - val_loss: 3.3842 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4071 - loss: 66.2002 - top_k_categorical_accuracy: 0.6752 - val_accuracy: 0.5091 - val_loss: 3.4108 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4259 - loss: 62.7897 - top_k_categorical_accuracy: 0.6874 - val_accuracy: 0.5091 - val_loss: 3.3641 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4741 - loss: 57.4763 - top_k_categorical_accuracy: 0.7125 - val_accuracy: 0.4727 - val_loss: 3.3038 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4599 - loss: 59.2365 - top_k_categorical_accuracy: 0.6854 - val_accuracy: 0.4909 - val_loss: 3.2610 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4634 - loss: 58.8134 - top_k_categorical_accuracy: 0.7023 - val_accuracy: 0.4727 - val_loss: 3.3733 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4904 - loss: 56.2166 - top_k_categorical_accuracy: 0.7146 - val_accuracy: 0.4727 - val_loss: 3.3408 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5043 - loss: 54.5468 - top_k_categorical_accuracy: 0.7194 - val_accuracy: 0.4909 - val_loss: 3.3954 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5344 - loss: 51.3130 - top_k_categorical_accuracy: 0.7563 - val_accuracy: 0.5091 - val_loss: 3.5314 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5184 - loss: 52.9555 - top_k_categorical_accuracy: 0.7273 - val_accuracy: 0.4727 - val_loss: 3.6169 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5457 - loss: 50.4183 - top_k_categorical_accuracy: 0.7331 - val_accuracy: 0.5091 - val_loss: 3.4906 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5548 - loss: 49.3362 - top_k_categorical_accuracy: 0.7411 - val_accuracy: 0.4909 - val_loss: 3.5915 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5575 - loss: 49.2662 - top_k_categorical_accuracy: 0.7433 - val_accuracy: 0.4727 - val_loss: 3.5861 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5659 - loss: 47.3935 - top_k_categorical_accuracy: 0.7493 - val_accuracy: 0.4909 - val_loss: 3.7176 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5747 - loss: 47.1857 - top_k_categorical_accuracy: 0.7513 - val_accuracy: 0.4545 - val_loss: 3.6938 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5737 - loss: 46.9005 - top_k_categorical_accuracy: 0.7587 - val_accuracy: 0.4364 - val_loss: 3.6140 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5754 - loss: 46.6037 - top_k_categorical_accuracy: 0.7656 - val_accuracy: 0.4909 - val_loss: 3.6910 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5654 - loss: 47.9231 - top_k_categorical_accuracy: 0.7417 - val_accuracy: 0.4364 - val_loss: 3.6229 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5911 - loss: 45.9148 - top_k_categorical_accuracy: 0.7486 - val_accuracy: 0.4727 - val_loss: 3.6574 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5987 - loss: 44.8579 - top_k_categorical_accuracy: 0.7553 - val_accuracy: 0.4727 - val_loss: 3.7565 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5958 - loss: 46.1071 - top_k_categorical_accuracy: 0.7487 - val_accuracy: 0.5091 - val_loss: 3.7468 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5954 - loss: 45.0578 - top_k_categorical_accuracy: 0.7611 - val_accuracy: 0.4727 - val_loss: 3.7859 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6162 - loss: 43.1187 - top_k_categorical_accuracy: 0.7623 - val_accuracy: 0.4727 - val_loss: 3.7943 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6159 - loss: 42.7733 - top_k_categorical_accuracy: 0.7672 - val_accuracy: 0.4909 - val_loss: 3.8589 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5942 - loss: 44.8899 - top_k_categorical_accuracy: 0.7427 - val_accuracy: 0.4909 - val_loss: 3.7990 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5987 - loss: 44.3073 - top_k_categorical_accuracy: 0.7530 - val_accuracy: 0.4545 - val_loss: 3.9181 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6165 - loss: 42.6375 - top_k_categorical_accuracy: 0.7662 - val_accuracy: 0.4727 - val_loss: 4.0031 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6172 - loss: 42.4996 - top_k_categorical_accuracy: 0.7671 - val_accuracy: 0.4364 - val_loss: 4.1186 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6228 - loss: 42.3330 - top_k_categorical_accuracy: 0.7697 - val_accuracy: 0.4545 - val_loss: 4.1646 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6187 - loss: 43.1101 - top_k_categorical_accuracy: 0.7494 - val_accuracy: 0.4545 - val_loss: 4.0045 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6161 - loss: 42.6661 - top_k_categorical_accuracy: 0.7729 - val_accuracy: 0.4727 - val_loss: 3.8587 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6291 - loss: 40.8124 - top_k_categorical_accuracy: 0.7875 - val_accuracy: 0.5091 - val_loss: 3.9787 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6314 - loss: 40.8369 - top_k_categorical_accuracy: 0.7820 - val_accuracy: 0.5091 - val_loss: 3.9397 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6293 - loss: 41.3955 - top_k_categorical_accuracy: 0.7631 - val_accuracy: 0.5091 - val_loss: 4.1081 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6409 - loss: 41.7588 - top_k_categorical_accuracy: 0.7626 - val_accuracy: 0.4727 - val_loss: 4.1322 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6328 - loss: 41.0742 - top_k_categorical_accuracy: 0.7740 - val_accuracy: 0.4909 - val_loss: 4.1519 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6316 - loss: 41.1396 - top_k_categorical_accuracy: 0.7744 - val_accuracy: 0.5091 - val_loss: 4.2921 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6352 - loss: 40.4405 - top_k_categorical_accuracy: 0.7765 - val_accuracy: 0.5091 - val_loss: 4.1274 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6331 - loss: 39.8513 - top_k_categorical_accuracy: 0.7772 - val_accuracy: 0.4909 - val_loss: 4.2052 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6405 - loss: 39.8724 - top_k_categorical_accuracy: 0.7703 - val_accuracy: 0.4909 - val_loss: 4.3122 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6301 - loss: 40.6957 - top_k_categorical_accuracy: 0.7727 - val_accuracy: 0.4909 - val_loss: 4.3205 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6710 - loss: 38.3866 - top_k_categorical_accuracy: 0.7844 - val_accuracy: 0.4545 - val_loss: 4.3421 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 46/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6365 - loss: 39.6542 - top_k_categorical_accuracy: 0.7754 - val_accuracy: 0.4364 - val_loss: 4.2874 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 47/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6363 - loss: 40.2089 - top_k_categorical_accuracy: 0.7817 - val_accuracy: 0.4727 - val_loss: 4.3895 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 48/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6432 - loss: 39.1004 - top_k_categorical_accuracy: 0.7691 - val_accuracy: 0.4545 - val_loss: 4.4516 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.0964 - loss: 125.7591 - top_k_categorical_accuracy: 0.2994 - val_accuracy: 0.3455 - val_loss: 3.9735 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2814 - loss: 86.8270 - top_k_categorical_accuracy: 0.5671 - val_accuracy: 0.4909 - val_loss: 3.2837 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3248 - loss: 76.1204 - top_k_categorical_accuracy: 0.6107 - val_accuracy: 0.5091 - val_loss: 3.1278 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3723 - loss: 71.6998 - top_k_categorical_accuracy: 0.6439 - val_accuracy: 0.5455 - val_loss: 2.9658 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3958 - loss: 66.8963 - top_k_categorical_accuracy: 0.6638 - val_accuracy: 0.4727 - val_loss: 3.0739 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4276 - loss: 62.2805 - top_k_categorical_accuracy: 0.6883 - val_accuracy: 0.5091 - val_loss: 2.8729 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4737 - loss: 59.2133 - top_k_categorical_accuracy: 0.6870 - val_accuracy: 0.5273 - val_loss: 2.8964 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4594 - loss: 59.9496 - top_k_categorical_accuracy: 0.7077 - val_accuracy: 0.5091 - val_loss: 2.8958 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5028 - loss: 54.7774 - top_k_categorical_accuracy: 0.7233 - val_accuracy: 0.5273 - val_loss: 2.9384 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4912 - loss: 54.9103 - top_k_categorical_accuracy: 0.7207 - val_accuracy: 0.5273 - val_loss: 2.8471 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4999 - loss: 54.4944 - top_k_categorical_accuracy: 0.7183 - val_accuracy: 0.5455 - val_loss: 2.9563 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5365 - loss: 52.0774 - top_k_categorical_accuracy: 0.7339 - val_accuracy: 0.5636 - val_loss: 2.9388 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5290 - loss: 52.2971 - top_k_categorical_accuracy: 0.7361 - val_accuracy: 0.5455 - val_loss: 2.8605 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5433 - loss: 49.6120 - top_k_categorical_accuracy: 0.7587 - val_accuracy: 0.5273 - val_loss: 2.7587 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5532 - loss: 50.0441 - top_k_categorical_accuracy: 0.7417 - val_accuracy: 0.5455 - val_loss: 2.9092 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5613 - loss: 48.5230 - top_k_categorical_accuracy: 0.7492 - val_accuracy: 0.5455 - val_loss: 2.8985 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5470 - loss: 49.1720 - top_k_categorical_accuracy: 0.7461 - val_accuracy: 0.4909 - val_loss: 3.0208 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5728 - loss: 47.5416 - top_k_categorical_accuracy: 0.7469 - val_accuracy: 0.4909 - val_loss: 2.9899 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5632 - loss: 48.0059 - top_k_categorical_accuracy: 0.7554 - val_accuracy: 0.5455 - val_loss: 2.8619 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5822 - loss: 46.1614 - top_k_categorical_accuracy: 0.7640 - val_accuracy: 0.5455 - val_loss: 2.8805 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5739 - loss: 47.2376 - top_k_categorical_accuracy: 0.7504 - val_accuracy: 0.5455 - val_loss: 2.9334 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5944 - loss: 44.5671 - top_k_categorical_accuracy: 0.7678 - val_accuracy: 0.5636 - val_loss: 2.9070 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5725 - loss: 46.9247 - top_k_categorical_accuracy: 0.7368 - val_accuracy: 0.5091 - val_loss: 3.0246 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5880 - loss: 46.8991 - top_k_categorical_accuracy: 0.7476 - val_accuracy: 0.5636 - val_loss: 3.0499 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6075 - loss: 42.9338 - top_k_categorical_accuracy: 0.7764 - val_accuracy: 0.5455 - val_loss: 3.1170 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5890 - loss: 46.7601 - top_k_categorical_accuracy: 0.7505 - val_accuracy: 0.5273 - val_loss: 3.1923 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6069 - loss: 44.2634 - top_k_categorical_accuracy: 0.7730 - val_accuracy: 0.5273 - val_loss: 3.1524 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6147 - loss: 42.8038 - top_k_categorical_accuracy: 0.7758 - val_accuracy: 0.5273 - val_loss: 3.1778 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6041 - loss: 43.7456 - top_k_categorical_accuracy: 0.7653 - val_accuracy: 0.5636 - val_loss: 3.1139 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6128 - loss: 42.7124 - top_k_categorical_accuracy: 0.7682 - val_accuracy: 0.5818 - val_loss: 3.0952 - val_top_k_categorical_accuracy: 0.8545 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6274 - loss: 41.6239 - top_k_categorical_accuracy: 0.7714 - val_accuracy: 0.5818 - val_loss: 3.1427 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6227 - loss: 41.9476 - top_k_categorical_accuracy: 0.7706 - val_accuracy: 0.5273 - val_loss: 3.3002 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6202 - loss: 42.2160 - top_k_categorical_accuracy: 0.7705 - val_accuracy: 0.5636 - val_loss: 3.2041 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5955 - loss: 45.4513 - top_k_categorical_accuracy: 0.7438 - val_accuracy: 0.5273 - val_loss: 3.2683 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6298 - loss: 40.8580 - top_k_categorical_accuracy: 0.7811 - val_accuracy: 0.5455 - val_loss: 3.2227 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6302 - loss: 41.2189 - top_k_categorical_accuracy: 0.7859 - val_accuracy: 0.5091 - val_loss: 3.2764 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6214 - loss: 41.8791 - top_k_categorical_accuracy: 0.7695 - val_accuracy: 0.5091 - val_loss: 3.3015 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6312 - loss: 40.8856 - top_k_categorical_accuracy: 0.7741 - val_accuracy: 0.4909 - val_loss: 3.3350 - val_top_k_categorical_accuracy: 0.8545 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6484 - loss: 41.1396 - top_k_categorical_accuracy: 0.7777 - val_accuracy: 0.5273 - val_loss: 3.3677 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6437 - loss: 40.5502 - top_k_categorical_accuracy: 0.7767 - val_accuracy: 0.5091 - val_loss: 3.4350 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6330 - loss: 41.1197 - top_k_categorical_accuracy: 0.7841 - val_accuracy: 0.4909 - val_loss: 3.4227 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6479 - loss: 40.1594 - top_k_categorical_accuracy: 0.7823 - val_accuracy: 0.5273 - val_loss: 3.4073 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6531 - loss: 38.4092 - top_k_categorical_accuracy: 0.7986 - val_accuracy: 0.5455 - val_loss: 3.4074 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6336 - loss: 40.7034 - top_k_categorical_accuracy: 0.7773 - val_accuracy: 0.5091 - val_loss: 3.3967 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6407 - loss: 40.9212 - top_k_categorical_accuracy: 0.7676 - val_accuracy: 0.5273 - val_loss: 3.3886 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 46/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6474 - loss: 39.9444 - top_k_categorical_accuracy: 0.7769 - val_accuracy: 0.5091 - val_loss: 3.4629 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 47/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6365 - loss: 40.6009 - top_k_categorical_accuracy: 0.7725 - val_accuracy: 0.5273 - val_loss: 3.3596 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 48/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6519 - loss: 39.5306 - top_k_categorical_accuracy: 0.7872 - val_accuracy: 0.5455 - val_loss: 3.4705 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 49/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6518 - loss: 39.2926 - top_k_categorical_accuracy: 0.7786 - val_accuracy: 0.4909 - val_loss: 3.3405 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 50/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6649 - loss: 38.3626 - top_k_categorical_accuracy: 0.7875 - val_accuracy: 0.5091 - val_loss: 3.4411 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 51/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6284 - loss: 40.6213 - top_k_categorical_accuracy: 0.7659 - val_accuracy: 0.5091 - val_loss: 3.4989 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 52/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6461 - loss: 39.1966 - top_k_categorical_accuracy: 0.7901 - val_accuracy: 0.4909 - val_loss: 3.6381 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 53/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6430 - loss: 38.9932 - top_k_categorical_accuracy: 0.7818 - val_accuracy: 0.5273 - val_loss: 3.5954 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 54/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6582 - loss: 38.9953 - top_k_categorical_accuracy: 0.7816 - val_accuracy: 0.5091 - val_loss: 3.5512 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Accuracy: 0.5273\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.0972 - loss: 124.1529 - top_k_categorical_accuracy: 0.3174 - val_accuracy: 0.4000 - val_loss: 4.5571 - val_top_k_categorical_accuracy: 0.6000 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2678 - loss: 86.4192 - top_k_categorical_accuracy: 0.5501 - val_accuracy: 0.4000 - val_loss: 3.8785 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3300 - loss: 74.6209 - top_k_categorical_accuracy: 0.6225 - val_accuracy: 0.4000 - val_loss: 3.5375 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3710 - loss: 71.0339 - top_k_categorical_accuracy: 0.6417 - val_accuracy: 0.4182 - val_loss: 3.3739 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4085 - loss: 64.9106 - top_k_categorical_accuracy: 0.6761 - val_accuracy: 0.4000 - val_loss: 3.4349 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4315 - loss: 62.1015 - top_k_categorical_accuracy: 0.6953 - val_accuracy: 0.4182 - val_loss: 3.3466 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4549 - loss: 60.2757 - top_k_categorical_accuracy: 0.6983 - val_accuracy: 0.4909 - val_loss: 3.3797 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4749 - loss: 57.5320 - top_k_categorical_accuracy: 0.7176 - val_accuracy: 0.4727 - val_loss: 3.4787 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4750 - loss: 56.1530 - top_k_categorical_accuracy: 0.7275 - val_accuracy: 0.4364 - val_loss: 3.5298 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5077 - loss: 53.9171 - top_k_categorical_accuracy: 0.7225 - val_accuracy: 0.4000 - val_loss: 3.5829 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5340 - loss: 51.4850 - top_k_categorical_accuracy: 0.7468 - val_accuracy: 0.4364 - val_loss: 3.6664 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5076 - loss: 53.1632 - top_k_categorical_accuracy: 0.7283 - val_accuracy: 0.5273 - val_loss: 3.4425 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5397 - loss: 48.8660 - top_k_categorical_accuracy: 0.7626 - val_accuracy: 0.4727 - val_loss: 3.5017 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5477 - loss: 49.8417 - top_k_categorical_accuracy: 0.7344 - val_accuracy: 0.4545 - val_loss: 3.5834 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5556 - loss: 47.7504 - top_k_categorical_accuracy: 0.7549 - val_accuracy: 0.4545 - val_loss: 3.5813 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5835 - loss: 47.0947 - top_k_categorical_accuracy: 0.7536 - val_accuracy: 0.5091 - val_loss: 3.6871 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5777 - loss: 46.8635 - top_k_categorical_accuracy: 0.7587 - val_accuracy: 0.4364 - val_loss: 3.9318 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5712 - loss: 47.9469 - top_k_categorical_accuracy: 0.7397 - val_accuracy: 0.5273 - val_loss: 3.7469 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5757 - loss: 46.3158 - top_k_categorical_accuracy: 0.7688 - val_accuracy: 0.5091 - val_loss: 3.8158 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5946 - loss: 44.7749 - top_k_categorical_accuracy: 0.7660 - val_accuracy: 0.4909 - val_loss: 3.8927 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5906 - loss: 45.5391 - top_k_categorical_accuracy: 0.7540 - val_accuracy: 0.4727 - val_loss: 3.9674 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5734 - loss: 46.2448 - top_k_categorical_accuracy: 0.7476 - val_accuracy: 0.4545 - val_loss: 4.0946 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5934 - loss: 45.1227 - top_k_categorical_accuracy: 0.7567 - val_accuracy: 0.4727 - val_loss: 4.1428 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6030 - loss: 44.1737 - top_k_categorical_accuracy: 0.7615 - val_accuracy: 0.4182 - val_loss: 4.1470 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6113 - loss: 43.3656 - top_k_categorical_accuracy: 0.7685 - val_accuracy: 0.4545 - val_loss: 4.0601 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6232 - loss: 42.2566 - top_k_categorical_accuracy: 0.7767 - val_accuracy: 0.4545 - val_loss: 4.1947 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6265 - loss: 42.0028 - top_k_categorical_accuracy: 0.7780 - val_accuracy: 0.4909 - val_loss: 4.3857 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6235 - loss: 42.1990 - top_k_categorical_accuracy: 0.7768 - val_accuracy: 0.4727 - val_loss: 4.2834 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6134 - loss: 43.7705 - top_k_categorical_accuracy: 0.7595 - val_accuracy: 0.4545 - val_loss: 4.2471 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6083 - loss: 44.4827 - top_k_categorical_accuracy: 0.7594 - val_accuracy: 0.4545 - val_loss: 4.2695 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6128 - loss: 42.5083 - top_k_categorical_accuracy: 0.7720 - val_accuracy: 0.4727 - val_loss: 4.1676 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6264 - loss: 41.9799 - top_k_categorical_accuracy: 0.7686 - val_accuracy: 0.4545 - val_loss: 4.3054 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6362 - loss: 41.5577 - top_k_categorical_accuracy: 0.7700 - val_accuracy: 0.4545 - val_loss: 4.2931 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6269 - loss: 41.7711 - top_k_categorical_accuracy: 0.7676 - val_accuracy: 0.4364 - val_loss: 4.2533 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6274 - loss: 41.3904 - top_k_categorical_accuracy: 0.7735 - val_accuracy: 0.4364 - val_loss: 4.4201 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6245 - loss: 40.6232 - top_k_categorical_accuracy: 0.7788 - val_accuracy: 0.4545 - val_loss: 4.3667 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6468 - loss: 40.1478 - top_k_categorical_accuracy: 0.7821 - val_accuracy: 0.4364 - val_loss: 4.5573 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6192 - loss: 42.1109 - top_k_categorical_accuracy: 0.7607 - val_accuracy: 0.4545 - val_loss: 4.4903 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6385 - loss: 40.4015 - top_k_categorical_accuracy: 0.7707 - val_accuracy: 0.4182 - val_loss: 4.4794 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6310 - loss: 40.5668 - top_k_categorical_accuracy: 0.7711 - val_accuracy: 0.4545 - val_loss: 4.4426 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6348 - loss: 40.5583 - top_k_categorical_accuracy: 0.7636 - val_accuracy: 0.4364 - val_loss: 4.5075 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6527 - loss: 39.3952 - top_k_categorical_accuracy: 0.7746 - val_accuracy: 0.4364 - val_loss: 4.4985 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6476 - loss: 38.4999 - top_k_categorical_accuracy: 0.7787 - val_accuracy: 0.4182 - val_loss: 4.6777 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6625 - loss: 38.1525 - top_k_categorical_accuracy: 0.7992 - val_accuracy: 0.4182 - val_loss: 4.8176 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6461 - loss: 38.7240 - top_k_categorical_accuracy: 0.7940 - val_accuracy: 0.4364 - val_loss: 4.6836 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 46/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6353 - loss: 40.1348 - top_k_categorical_accuracy: 0.7714 - val_accuracy: 0.4364 - val_loss: 4.7670 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0884 - loss: 128.6754 - top_k_categorical_accuracy: 0.2612 - val_accuracy: 0.4182 - val_loss: 4.9674 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2770 - loss: 85.4745 - top_k_categorical_accuracy: 0.5443 - val_accuracy: 0.4364 - val_loss: 4.2114 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3466 - loss: 74.5980 - top_k_categorical_accuracy: 0.6191 - val_accuracy: 0.4182 - val_loss: 3.8393 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3759 - loss: 69.3089 - top_k_categorical_accuracy: 0.6490 - val_accuracy: 0.4909 - val_loss: 3.7242 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4291 - loss: 62.7222 - top_k_categorical_accuracy: 0.7014 - val_accuracy: 0.4545 - val_loss: 3.6166 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4490 - loss: 60.3903 - top_k_categorical_accuracy: 0.6900 - val_accuracy: 0.4364 - val_loss: 3.6813 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4813 - loss: 57.3967 - top_k_categorical_accuracy: 0.7123 - val_accuracy: 0.4364 - val_loss: 3.6887 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4886 - loss: 56.0178 - top_k_categorical_accuracy: 0.7303 - val_accuracy: 0.4545 - val_loss: 3.6419 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5013 - loss: 55.4168 - top_k_categorical_accuracy: 0.7140 - val_accuracy: 0.4182 - val_loss: 3.8030 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4971 - loss: 55.5562 - top_k_categorical_accuracy: 0.7218 - val_accuracy: 0.4364 - val_loss: 3.8242 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5107 - loss: 53.3506 - top_k_categorical_accuracy: 0.7267 - val_accuracy: 0.4000 - val_loss: 3.8732 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5469 - loss: 51.1624 - top_k_categorical_accuracy: 0.7447 - val_accuracy: 0.4000 - val_loss: 3.8245 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5559 - loss: 49.2828 - top_k_categorical_accuracy: 0.7518 - val_accuracy: 0.4000 - val_loss: 3.6520 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5389 - loss: 50.1251 - top_k_categorical_accuracy: 0.7341 - val_accuracy: 0.4000 - val_loss: 3.8603 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5698 - loss: 47.7355 - top_k_categorical_accuracy: 0.7588 - val_accuracy: 0.4182 - val_loss: 3.7110 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5645 - loss: 47.0416 - top_k_categorical_accuracy: 0.7529 - val_accuracy: 0.4182 - val_loss: 3.7261 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5786 - loss: 46.1060 - top_k_categorical_accuracy: 0.7608 - val_accuracy: 0.4182 - val_loss: 3.9105 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5763 - loss: 45.9278 - top_k_categorical_accuracy: 0.7642 - val_accuracy: 0.4364 - val_loss: 3.8053 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5764 - loss: 46.4728 - top_k_categorical_accuracy: 0.7613 - val_accuracy: 0.4182 - val_loss: 3.8795 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5787 - loss: 45.9114 - top_k_categorical_accuracy: 0.7591 - val_accuracy: 0.3818 - val_loss: 4.0629 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5796 - loss: 46.5907 - top_k_categorical_accuracy: 0.7515 - val_accuracy: 0.4000 - val_loss: 4.0163 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5992 - loss: 44.6417 - top_k_categorical_accuracy: 0.7681 - val_accuracy: 0.4364 - val_loss: 4.0729 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6349 - loss: 41.7113 - top_k_categorical_accuracy: 0.7773 - val_accuracy: 0.4545 - val_loss: 4.1321 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5859 - loss: 44.8461 - top_k_categorical_accuracy: 0.7550 - val_accuracy: 0.4182 - val_loss: 4.2229 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5983 - loss: 44.9938 - top_k_categorical_accuracy: 0.7542 - val_accuracy: 0.4182 - val_loss: 4.1042 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6116 - loss: 43.6846 - top_k_categorical_accuracy: 0.7629 - val_accuracy: 0.4000 - val_loss: 4.1609 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6066 - loss: 43.8906 - top_k_categorical_accuracy: 0.7548 - val_accuracy: 0.4364 - val_loss: 4.2084 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6145 - loss: 43.3244 - top_k_categorical_accuracy: 0.7621 - val_accuracy: 0.4364 - val_loss: 4.1505 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6186 - loss: 41.7281 - top_k_categorical_accuracy: 0.7655 - val_accuracy: 0.4182 - val_loss: 4.2272 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6248 - loss: 41.5575 - top_k_categorical_accuracy: 0.7674 - val_accuracy: 0.4909 - val_loss: 4.1692 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6456 - loss: 40.1798 - top_k_categorical_accuracy: 0.7947 - val_accuracy: 0.4364 - val_loss: 4.2082 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6325 - loss: 41.7473 - top_k_categorical_accuracy: 0.7648 - val_accuracy: 0.4364 - val_loss: 4.1179 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6113 - loss: 42.1449 - top_k_categorical_accuracy: 0.7602 - val_accuracy: 0.4727 - val_loss: 4.1953 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6205 - loss: 41.6145 - top_k_categorical_accuracy: 0.7594 - val_accuracy: 0.4182 - val_loss: 4.3484 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6296 - loss: 40.7481 - top_k_categorical_accuracy: 0.7873 - val_accuracy: 0.4727 - val_loss: 4.0693 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6299 - loss: 40.2221 - top_k_categorical_accuracy: 0.7829 - val_accuracy: 0.5091 - val_loss: 3.9919 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6422 - loss: 40.4716 - top_k_categorical_accuracy: 0.7724 - val_accuracy: 0.4364 - val_loss: 4.0919 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6353 - loss: 41.2540 - top_k_categorical_accuracy: 0.7705 - val_accuracy: 0.4364 - val_loss: 4.1462 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6384 - loss: 39.8295 - top_k_categorical_accuracy: 0.7762 - val_accuracy: 0.3818 - val_loss: 4.2749 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6284 - loss: 41.0030 - top_k_categorical_accuracy: 0.7728 - val_accuracy: 0.4000 - val_loss: 4.3108 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6443 - loss: 39.7968 - top_k_categorical_accuracy: 0.7673 - val_accuracy: 0.4182 - val_loss: 4.4044 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6428 - loss: 40.0244 - top_k_categorical_accuracy: 0.7854 - val_accuracy: 0.3818 - val_loss: 4.5697 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6563 - loss: 39.4735 - top_k_categorical_accuracy: 0.7724 - val_accuracy: 0.4545 - val_loss: 4.5108 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6393 - loss: 39.4466 - top_k_categorical_accuracy: 0.7843 - val_accuracy: 0.4000 - val_loss: 4.5241 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6531 - loss: 38.2972 - top_k_categorical_accuracy: 0.7964 - val_accuracy: 0.3818 - val_loss: 4.5370 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.1025 - loss: 125.2523 - top_k_categorical_accuracy: 0.2957 - val_accuracy: 0.3455 - val_loss: 4.7740 - val_top_k_categorical_accuracy: 0.6182 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2750 - loss: 84.9203 - top_k_categorical_accuracy: 0.5603 - val_accuracy: 0.3455 - val_loss: 4.1787 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3609 - loss: 73.3260 - top_k_categorical_accuracy: 0.6337 - val_accuracy: 0.4182 - val_loss: 3.6957 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3963 - loss: 68.8582 - top_k_categorical_accuracy: 0.6502 - val_accuracy: 0.4545 - val_loss: 3.6196 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4119 - loss: 65.5674 - top_k_categorical_accuracy: 0.6793 - val_accuracy: 0.4000 - val_loss: 3.6195 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4615 - loss: 60.7925 - top_k_categorical_accuracy: 0.6953 - val_accuracy: 0.4182 - val_loss: 3.5016 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4734 - loss: 57.9072 - top_k_categorical_accuracy: 0.7160 - val_accuracy: 0.4182 - val_loss: 3.4427 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4958 - loss: 54.8309 - top_k_categorical_accuracy: 0.7293 - val_accuracy: 0.4182 - val_loss: 3.5558 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5232 - loss: 53.8240 - top_k_categorical_accuracy: 0.7290 - val_accuracy: 0.3818 - val_loss: 3.4398 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5092 - loss: 53.4243 - top_k_categorical_accuracy: 0.7312 - val_accuracy: 0.4182 - val_loss: 3.5855 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5503 - loss: 50.7387 - top_k_categorical_accuracy: 0.7460 - val_accuracy: 0.4000 - val_loss: 3.4977 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5325 - loss: 50.5942 - top_k_categorical_accuracy: 0.7387 - val_accuracy: 0.4182 - val_loss: 3.5370 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5405 - loss: 50.8861 - top_k_categorical_accuracy: 0.7375 - val_accuracy: 0.4182 - val_loss: 3.5889 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5486 - loss: 48.3603 - top_k_categorical_accuracy: 0.7591 - val_accuracy: 0.4364 - val_loss: 3.6359 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5569 - loss: 49.1833 - top_k_categorical_accuracy: 0.7418 - val_accuracy: 0.4364 - val_loss: 3.6231 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5750 - loss: 46.4136 - top_k_categorical_accuracy: 0.7711 - val_accuracy: 0.4545 - val_loss: 3.6438 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5933 - loss: 46.2117 - top_k_categorical_accuracy: 0.7603 - val_accuracy: 0.4545 - val_loss: 3.7526 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5818 - loss: 46.9448 - top_k_categorical_accuracy: 0.7408 - val_accuracy: 0.4182 - val_loss: 3.6950 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5816 - loss: 45.2099 - top_k_categorical_accuracy: 0.7594 - val_accuracy: 0.4364 - val_loss: 3.8110 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6043 - loss: 44.1368 - top_k_categorical_accuracy: 0.7650 - val_accuracy: 0.4182 - val_loss: 3.7633 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6113 - loss: 43.7086 - top_k_categorical_accuracy: 0.7644 - val_accuracy: 0.4182 - val_loss: 3.8961 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5892 - loss: 45.8100 - top_k_categorical_accuracy: 0.7580 - val_accuracy: 0.4545 - val_loss: 3.9135 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5899 - loss: 45.7140 - top_k_categorical_accuracy: 0.7566 - val_accuracy: 0.4545 - val_loss: 3.9453 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6105 - loss: 43.7081 - top_k_categorical_accuracy: 0.7669 - val_accuracy: 0.4182 - val_loss: 3.9784 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6127 - loss: 43.6097 - top_k_categorical_accuracy: 0.7576 - val_accuracy: 0.4182 - val_loss: 4.1355 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6151 - loss: 42.1689 - top_k_categorical_accuracy: 0.7786 - val_accuracy: 0.3455 - val_loss: 4.0970 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5927 - loss: 45.3998 - top_k_categorical_accuracy: 0.7466 - val_accuracy: 0.3818 - val_loss: 4.1498 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6142 - loss: 43.7693 - top_k_categorical_accuracy: 0.7554 - val_accuracy: 0.4545 - val_loss: 4.1873 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6182 - loss: 41.8477 - top_k_categorical_accuracy: 0.7809 - val_accuracy: 0.4545 - val_loss: 4.3313 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6389 - loss: 40.2537 - top_k_categorical_accuracy: 0.7853 - val_accuracy: 0.4000 - val_loss: 4.2599 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6295 - loss: 40.9776 - top_k_categorical_accuracy: 0.7756 - val_accuracy: 0.4000 - val_loss: 4.4529 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6425 - loss: 39.3800 - top_k_categorical_accuracy: 0.7878 - val_accuracy: 0.4364 - val_loss: 4.3874 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6343 - loss: 40.5355 - top_k_categorical_accuracy: 0.7728 - val_accuracy: 0.4364 - val_loss: 4.5390 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6313 - loss: 41.2001 - top_k_categorical_accuracy: 0.7729 - val_accuracy: 0.4182 - val_loss: 4.6518 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6232 - loss: 42.6074 - top_k_categorical_accuracy: 0.7737 - val_accuracy: 0.4000 - val_loss: 4.6190 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6302 - loss: 40.1181 - top_k_categorical_accuracy: 0.7801 - val_accuracy: 0.4182 - val_loss: 4.6353 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6239 - loss: 41.3063 - top_k_categorical_accuracy: 0.7694 - val_accuracy: 0.4000 - val_loss: 4.5824 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6304 - loss: 41.4627 - top_k_categorical_accuracy: 0.7625 - val_accuracy: 0.4545 - val_loss: 4.5258 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6378 - loss: 41.1333 - top_k_categorical_accuracy: 0.7551 - val_accuracy: 0.4727 - val_loss: 4.6986 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6553 - loss: 38.5852 - top_k_categorical_accuracy: 0.7765 - val_accuracy: 0.4545 - val_loss: 4.5096 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6368 - loss: 39.5431 - top_k_categorical_accuracy: 0.7701 - val_accuracy: 0.4364 - val_loss: 4.4776 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6398 - loss: 40.2114 - top_k_categorical_accuracy: 0.7643 - val_accuracy: 0.4000 - val_loss: 4.4996 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6475 - loss: 39.0428 - top_k_categorical_accuracy: 0.7789 - val_accuracy: 0.4182 - val_loss: 4.5308 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6531 - loss: 38.3412 - top_k_categorical_accuracy: 0.7763 - val_accuracy: 0.4182 - val_loss: 4.4346 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6645 - loss: 36.9873 - top_k_categorical_accuracy: 0.8060 - val_accuracy: 0.4000 - val_loss: 4.5502 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 46/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6459 - loss: 40.6207 - top_k_categorical_accuracy: 0.7670 - val_accuracy: 0.4000 - val_loss: 4.6608 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 47/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6589 - loss: 38.4024 - top_k_categorical_accuracy: 0.7867 - val_accuracy: 0.4182 - val_loss: 4.5287 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 48/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6648 - loss: 37.5606 - top_k_categorical_accuracy: 0.7991 - val_accuracy: 0.4000 - val_loss: 4.5931 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 49/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6680 - loss: 36.8686 - top_k_categorical_accuracy: 0.7932 - val_accuracy: 0.4000 - val_loss: 4.5808 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.0846 - loss: 125.3075 - top_k_categorical_accuracy: 0.2899 - val_accuracy: 0.3091 - val_loss: 4.2581 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2736 - loss: 86.6253 - top_k_categorical_accuracy: 0.5509 - val_accuracy: 0.4182 - val_loss: 3.6359 - val_top_k_categorical_accuracy: 0.6182 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3572 - loss: 72.7718 - top_k_categorical_accuracy: 0.6363 - val_accuracy: 0.4364 - val_loss: 3.3975 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4011 - loss: 68.7028 - top_k_categorical_accuracy: 0.6489 - val_accuracy: 0.4364 - val_loss: 3.4103 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4148 - loss: 64.7600 - top_k_categorical_accuracy: 0.6767 - val_accuracy: 0.4909 - val_loss: 3.3465 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4495 - loss: 60.5540 - top_k_categorical_accuracy: 0.6936 - val_accuracy: 0.4727 - val_loss: 3.4889 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4730 - loss: 58.8586 - top_k_categorical_accuracy: 0.7107 - val_accuracy: 0.4727 - val_loss: 3.6051 - val_top_k_categorical_accuracy: 0.6182 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4737 - loss: 57.6101 - top_k_categorical_accuracy: 0.7076 - val_accuracy: 0.4364 - val_loss: 3.5557 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4925 - loss: 55.1404 - top_k_categorical_accuracy: 0.7220 - val_accuracy: 0.4364 - val_loss: 3.6237 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4894 - loss: 55.8048 - top_k_categorical_accuracy: 0.7164 - val_accuracy: 0.4545 - val_loss: 3.6728 - val_top_k_categorical_accuracy: 0.6182 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5247 - loss: 51.2737 - top_k_categorical_accuracy: 0.7410 - val_accuracy: 0.4364 - val_loss: 3.7250 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5186 - loss: 51.2978 - top_k_categorical_accuracy: 0.7468 - val_accuracy: 0.4545 - val_loss: 3.7450 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5499 - loss: 49.4433 - top_k_categorical_accuracy: 0.7448 - val_accuracy: 0.4182 - val_loss: 3.7680 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5582 - loss: 49.0629 - top_k_categorical_accuracy: 0.7458 - val_accuracy: 0.4364 - val_loss: 3.7767 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5726 - loss: 47.6703 - top_k_categorical_accuracy: 0.7541 - val_accuracy: 0.4364 - val_loss: 3.8296 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5779 - loss: 46.7141 - top_k_categorical_accuracy: 0.7597 - val_accuracy: 0.4364 - val_loss: 3.8608 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5823 - loss: 45.4099 - top_k_categorical_accuracy: 0.7644 - val_accuracy: 0.4545 - val_loss: 4.0211 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5879 - loss: 46.4113 - top_k_categorical_accuracy: 0.7493 - val_accuracy: 0.4545 - val_loss: 4.0023 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6001 - loss: 44.1488 - top_k_categorical_accuracy: 0.7700 - val_accuracy: 0.4727 - val_loss: 4.0294 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5992 - loss: 45.1901 - top_k_categorical_accuracy: 0.7699 - val_accuracy: 0.4727 - val_loss: 3.9915 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6128 - loss: 43.8206 - top_k_categorical_accuracy: 0.7569 - val_accuracy: 0.4364 - val_loss: 4.0374 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6018 - loss: 44.4301 - top_k_categorical_accuracy: 0.7671 - val_accuracy: 0.4545 - val_loss: 4.1012 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5889 - loss: 45.2603 - top_k_categorical_accuracy: 0.7487 - val_accuracy: 0.4727 - val_loss: 4.2372 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6189 - loss: 42.2266 - top_k_categorical_accuracy: 0.7763 - val_accuracy: 0.4182 - val_loss: 4.3498 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5997 - loss: 43.6335 - top_k_categorical_accuracy: 0.7650 - val_accuracy: 0.4364 - val_loss: 4.3026 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6302 - loss: 41.1170 - top_k_categorical_accuracy: 0.7805 - val_accuracy: 0.4182 - val_loss: 4.2881 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6130 - loss: 43.5222 - top_k_categorical_accuracy: 0.7557 - val_accuracy: 0.4182 - val_loss: 4.1163 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5989 - loss: 43.0638 - top_k_categorical_accuracy: 0.7660 - val_accuracy: 0.4545 - val_loss: 4.2736 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6234 - loss: 41.9296 - top_k_categorical_accuracy: 0.7724 - val_accuracy: 0.4727 - val_loss: 4.3234 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6193 - loss: 42.5347 - top_k_categorical_accuracy: 0.7583 - val_accuracy: 0.4364 - val_loss: 4.2879 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6291 - loss: 40.9292 - top_k_categorical_accuracy: 0.7732 - val_accuracy: 0.4545 - val_loss: 4.3631 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6239 - loss: 40.7469 - top_k_categorical_accuracy: 0.7726 - val_accuracy: 0.4182 - val_loss: 4.2658 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6416 - loss: 40.0221 - top_k_categorical_accuracy: 0.7741 - val_accuracy: 0.4727 - val_loss: 4.4298 - val_top_k_categorical_accuracy: 0.6182 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6275 - loss: 41.2365 - top_k_categorical_accuracy: 0.7732 - val_accuracy: 0.4727 - val_loss: 4.4173 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6415 - loss: 40.3357 - top_k_categorical_accuracy: 0.7788 - val_accuracy: 0.4182 - val_loss: 4.5213 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6364 - loss: 40.9420 - top_k_categorical_accuracy: 0.7616 - val_accuracy: 0.4000 - val_loss: 4.5706 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6502 - loss: 39.5495 - top_k_categorical_accuracy: 0.7872 - val_accuracy: 0.4364 - val_loss: 4.6322 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6596 - loss: 38.6027 - top_k_categorical_accuracy: 0.7974 - val_accuracy: 0.4545 - val_loss: 4.4983 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6465 - loss: 39.4800 - top_k_categorical_accuracy: 0.7752 - val_accuracy: 0.4364 - val_loss: 4.5440 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6315 - loss: 41.3236 - top_k_categorical_accuracy: 0.7642 - val_accuracy: 0.4364 - val_loss: 4.5192 - val_top_k_categorical_accuracy: 0.6182 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6500 - loss: 39.6925 - top_k_categorical_accuracy: 0.7744 - val_accuracy: 0.4364 - val_loss: 4.6865 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6466 - loss: 40.0234 - top_k_categorical_accuracy: 0.7706 - val_accuracy: 0.4727 - val_loss: 4.6574 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6594 - loss: 38.9553 - top_k_categorical_accuracy: 0.7682 - val_accuracy: 0.4182 - val_loss: 4.6771 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6573 - loss: 38.8775 - top_k_categorical_accuracy: 0.7823 - val_accuracy: 0.4000 - val_loss: 4.7371 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6462 - loss: 39.3103 - top_k_categorical_accuracy: 0.7772 - val_accuracy: 0.4364 - val_loss: 4.8270 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.0969 - loss: 125.9689 - top_k_categorical_accuracy: 0.2854 - val_accuracy: 0.3636 - val_loss: 4.5797 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2634 - loss: 86.1922 - top_k_categorical_accuracy: 0.5259 - val_accuracy: 0.4182 - val_loss: 3.7858 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3460 - loss: 75.9033 - top_k_categorical_accuracy: 0.6127 - val_accuracy: 0.4000 - val_loss: 3.6643 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3932 - loss: 69.0098 - top_k_categorical_accuracy: 0.6406 - val_accuracy: 0.4364 - val_loss: 3.8077 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4000 - loss: 66.0803 - top_k_categorical_accuracy: 0.6776 - val_accuracy: 0.4182 - val_loss: 3.6921 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4311 - loss: 62.4293 - top_k_categorical_accuracy: 0.6937 - val_accuracy: 0.4364 - val_loss: 3.7234 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4425 - loss: 60.7847 - top_k_categorical_accuracy: 0.6943 - val_accuracy: 0.4545 - val_loss: 3.7008 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4764 - loss: 58.1021 - top_k_categorical_accuracy: 0.7039 - val_accuracy: 0.4364 - val_loss: 3.6466 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4881 - loss: 55.9811 - top_k_categorical_accuracy: 0.7065 - val_accuracy: 0.4545 - val_loss: 3.7312 - val_top_k_categorical_accuracy: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4882 - loss: 56.4432 - top_k_categorical_accuracy: 0.6940 - val_accuracy: 0.4545 - val_loss: 3.6977 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5165 - loss: 53.0056 - top_k_categorical_accuracy: 0.7213 - val_accuracy: 0.4182 - val_loss: 3.7343 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5123 - loss: 53.3024 - top_k_categorical_accuracy: 0.7190 - val_accuracy: 0.4364 - val_loss: 3.8496 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5382 - loss: 50.4483 - top_k_categorical_accuracy: 0.7330 - val_accuracy: 0.4182 - val_loss: 3.7113 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5384 - loss: 50.9648 - top_k_categorical_accuracy: 0.7350 - val_accuracy: 0.3818 - val_loss: 3.8670 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5658 - loss: 47.8292 - top_k_categorical_accuracy: 0.7539 - val_accuracy: 0.4364 - val_loss: 3.8269 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5424 - loss: 48.6677 - top_k_categorical_accuracy: 0.7448 - val_accuracy: 0.4364 - val_loss: 4.0752 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5422 - loss: 50.7788 - top_k_categorical_accuracy: 0.7386 - val_accuracy: 0.4182 - val_loss: 4.0705 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5591 - loss: 48.7236 - top_k_categorical_accuracy: 0.7522 - val_accuracy: 0.4000 - val_loss: 4.1482 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5782 - loss: 46.3745 - top_k_categorical_accuracy: 0.7511 - val_accuracy: 0.4182 - val_loss: 4.1822 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5558 - loss: 48.9187 - top_k_categorical_accuracy: 0.7358 - val_accuracy: 0.4364 - val_loss: 4.1509 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5759 - loss: 46.6712 - top_k_categorical_accuracy: 0.7520 - val_accuracy: 0.4182 - val_loss: 4.0567 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5899 - loss: 44.7980 - top_k_categorical_accuracy: 0.7617 - val_accuracy: 0.4364 - val_loss: 4.1072 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5983 - loss: 44.3170 - top_k_categorical_accuracy: 0.7585 - val_accuracy: 0.4182 - val_loss: 4.2747 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5970 - loss: 44.8683 - top_k_categorical_accuracy: 0.7563 - val_accuracy: 0.4364 - val_loss: 4.3011 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6040 - loss: 44.5193 - top_k_categorical_accuracy: 0.7566 - val_accuracy: 0.4182 - val_loss: 4.2677 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6098 - loss: 42.5036 - top_k_categorical_accuracy: 0.7814 - val_accuracy: 0.4364 - val_loss: 4.3648 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6246 - loss: 42.1570 - top_k_categorical_accuracy: 0.7594 - val_accuracy: 0.4364 - val_loss: 4.3207 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6287 - loss: 41.7226 - top_k_categorical_accuracy: 0.7704 - val_accuracy: 0.4545 - val_loss: 4.4766 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6199 - loss: 42.2544 - top_k_categorical_accuracy: 0.7706 - val_accuracy: 0.4000 - val_loss: 4.4628 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6274 - loss: 42.1818 - top_k_categorical_accuracy: 0.7740 - val_accuracy: 0.4364 - val_loss: 4.6029 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6232 - loss: 42.2566 - top_k_categorical_accuracy: 0.7787 - val_accuracy: 0.4000 - val_loss: 4.4898 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6317 - loss: 41.1875 - top_k_categorical_accuracy: 0.7666 - val_accuracy: 0.4000 - val_loss: 4.6517 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6446 - loss: 40.5072 - top_k_categorical_accuracy: 0.7704 - val_accuracy: 0.4182 - val_loss: 4.7406 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6121 - loss: 42.7649 - top_k_categorical_accuracy: 0.7608 - val_accuracy: 0.4000 - val_loss: 4.8759 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6373 - loss: 41.0923 - top_k_categorical_accuracy: 0.7735 - val_accuracy: 0.4364 - val_loss: 4.9338 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5988 - loss: 43.4466 - top_k_categorical_accuracy: 0.7592 - val_accuracy: 0.4182 - val_loss: 5.0226 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6164 - loss: 43.0069 - top_k_categorical_accuracy: 0.7493 - val_accuracy: 0.4000 - val_loss: 5.0547 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6373 - loss: 40.2921 - top_k_categorical_accuracy: 0.7684 - val_accuracy: 0.4000 - val_loss: 4.9102 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6445 - loss: 40.6190 - top_k_categorical_accuracy: 0.7599 - val_accuracy: 0.4000 - val_loss: 4.9375 - val_top_k_categorical_accuracy: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6378 - loss: 40.5612 - top_k_categorical_accuracy: 0.7670 - val_accuracy: 0.3818 - val_loss: 5.1290 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6368 - loss: 40.0664 - top_k_categorical_accuracy: 0.7789 - val_accuracy: 0.3818 - val_loss: 5.1916 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6488 - loss: 40.0003 - top_k_categorical_accuracy: 0.7731 - val_accuracy: 0.3636 - val_loss: 4.9916 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6529 - loss: 39.8122 - top_k_categorical_accuracy: 0.7725 - val_accuracy: 0.4182 - val_loss: 5.0864 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6487 - loss: 39.8218 - top_k_categorical_accuracy: 0.7766 - val_accuracy: 0.4182 - val_loss: 4.9579 - val_top_k_categorical_accuracy: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6499 - loss: 39.4680 - top_k_categorical_accuracy: 0.7788 - val_accuracy: 0.3636 - val_loss: 5.0302 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 46/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6401 - loss: 38.8139 - top_k_categorical_accuracy: 0.7880 - val_accuracy: 0.4000 - val_loss: 4.9782 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 47/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6396 - loss: 39.8715 - top_k_categorical_accuracy: 0.7647 - val_accuracy: 0.3818 - val_loss: 5.0862 - val_top_k_categorical_accuracy: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 48/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6520 - loss: 40.0224 - top_k_categorical_accuracy: 0.7737 - val_accuracy: 0.3818 - val_loss: 5.0032 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.0959 - loss: 125.3729 - top_k_categorical_accuracy: 0.3040 - val_accuracy: 0.3273 - val_loss: 5.0428 - val_top_k_categorical_accuracy: 0.6727 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2940 - loss: 85.1543 - top_k_categorical_accuracy: 0.5565 - val_accuracy: 0.4000 - val_loss: 4.1505 - val_top_k_categorical_accuracy: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3258 - loss: 77.3308 - top_k_categorical_accuracy: 0.5967 - val_accuracy: 0.4545 - val_loss: 3.7365 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3791 - loss: 69.8982 - top_k_categorical_accuracy: 0.6451 - val_accuracy: 0.4364 - val_loss: 3.6684 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4230 - loss: 63.6902 - top_k_categorical_accuracy: 0.6799 - val_accuracy: 0.4727 - val_loss: 3.4099 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4421 - loss: 62.6754 - top_k_categorical_accuracy: 0.6882 - val_accuracy: 0.4909 - val_loss: 3.3825 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4849 - loss: 57.2337 - top_k_categorical_accuracy: 0.7170 - val_accuracy: 0.5273 - val_loss: 3.3691 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4649 - loss: 58.2630 - top_k_categorical_accuracy: 0.6969 - val_accuracy: 0.5636 - val_loss: 3.4084 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4956 - loss: 55.1642 - top_k_categorical_accuracy: 0.7283 - val_accuracy: 0.5273 - val_loss: 3.4309 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5083 - loss: 54.2677 - top_k_categorical_accuracy: 0.7249 - val_accuracy: 0.5273 - val_loss: 3.4859 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5266 - loss: 52.5261 - top_k_categorical_accuracy: 0.7340 - val_accuracy: 0.5091 - val_loss: 3.5396 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5269 - loss: 51.7681 - top_k_categorical_accuracy: 0.7382 - val_accuracy: 0.4909 - val_loss: 3.5769 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5377 - loss: 50.8245 - top_k_categorical_accuracy: 0.7393 - val_accuracy: 0.5273 - val_loss: 3.5226 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5409 - loss: 50.5901 - top_k_categorical_accuracy: 0.7380 - val_accuracy: 0.5455 - val_loss: 3.5426 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5517 - loss: 48.8859 - top_k_categorical_accuracy: 0.7391 - val_accuracy: 0.5636 - val_loss: 3.4024 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5713 - loss: 46.5692 - top_k_categorical_accuracy: 0.7572 - val_accuracy: 0.5091 - val_loss: 3.5573 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5745 - loss: 47.1913 - top_k_categorical_accuracy: 0.7491 - val_accuracy: 0.5455 - val_loss: 3.5087 - val_top_k_categorical_accuracy: 0.8364 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5921 - loss: 44.3769 - top_k_categorical_accuracy: 0.7643 - val_accuracy: 0.5091 - val_loss: 3.4872 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5931 - loss: 44.5555 - top_k_categorical_accuracy: 0.7765 - val_accuracy: 0.4909 - val_loss: 3.5985 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5875 - loss: 44.7692 - top_k_categorical_accuracy: 0.7615 - val_accuracy: 0.5455 - val_loss: 3.6250 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5896 - loss: 45.0088 - top_k_categorical_accuracy: 0.7698 - val_accuracy: 0.5455 - val_loss: 3.5311 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5909 - loss: 44.1774 - top_k_categorical_accuracy: 0.7813 - val_accuracy: 0.5273 - val_loss: 3.7029 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6204 - loss: 43.0682 - top_k_categorical_accuracy: 0.7652 - val_accuracy: 0.5273 - val_loss: 3.5432 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6243 - loss: 42.1378 - top_k_categorical_accuracy: 0.7851 - val_accuracy: 0.5455 - val_loss: 3.6233 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5985 - loss: 43.6739 - top_k_categorical_accuracy: 0.7631 - val_accuracy: 0.5091 - val_loss: 3.6541 - val_top_k_categorical_accuracy: 0.8182 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6313 - loss: 42.3282 - top_k_categorical_accuracy: 0.7637 - val_accuracy: 0.5091 - val_loss: 3.6799 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6173 - loss: 41.8088 - top_k_categorical_accuracy: 0.7783 - val_accuracy: 0.5091 - val_loss: 3.6280 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6294 - loss: 41.5464 - top_k_categorical_accuracy: 0.7770 - val_accuracy: 0.4727 - val_loss: 3.5973 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6121 - loss: 43.4627 - top_k_categorical_accuracy: 0.7633 - val_accuracy: 0.5273 - val_loss: 3.5933 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6265 - loss: 42.3726 - top_k_categorical_accuracy: 0.7720 - val_accuracy: 0.5091 - val_loss: 3.6952 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6126 - loss: 42.7597 - top_k_categorical_accuracy: 0.7685 - val_accuracy: 0.4727 - val_loss: 3.7728 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6497 - loss: 40.0550 - top_k_categorical_accuracy: 0.7724 - val_accuracy: 0.5455 - val_loss: 3.7562 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6466 - loss: 40.0927 - top_k_categorical_accuracy: 0.7791 - val_accuracy: 0.5091 - val_loss: 3.7966 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6515 - loss: 39.7647 - top_k_categorical_accuracy: 0.7867 - val_accuracy: 0.5273 - val_loss: 3.8675 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6500 - loss: 39.6105 - top_k_categorical_accuracy: 0.7794 - val_accuracy: 0.5091 - val_loss: 3.8259 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6405 - loss: 40.8234 - top_k_categorical_accuracy: 0.7670 - val_accuracy: 0.5273 - val_loss: 3.8139 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6662 - loss: 37.8091 - top_k_categorical_accuracy: 0.7936 - val_accuracy: 0.5455 - val_loss: 3.8249 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6693 - loss: 37.7448 - top_k_categorical_accuracy: 0.8036 - val_accuracy: 0.5455 - val_loss: 3.9695 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6370 - loss: 40.1291 - top_k_categorical_accuracy: 0.7751 - val_accuracy: 0.5091 - val_loss: 4.0111 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6498 - loss: 40.1331 - top_k_categorical_accuracy: 0.7651 - val_accuracy: 0.5273 - val_loss: 3.8922 - val_top_k_categorical_accuracy: 0.7818 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6635 - loss: 38.2289 - top_k_categorical_accuracy: 0.7878 - val_accuracy: 0.5273 - val_loss: 4.1941 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6320 - loss: 41.1332 - top_k_categorical_accuracy: 0.7694 - val_accuracy: 0.5091 - val_loss: 4.0231 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6662 - loss: 37.1874 - top_k_categorical_accuracy: 0.8001 - val_accuracy: 0.5091 - val_loss: 4.1345 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6472 - loss: 38.7149 - top_k_categorical_accuracy: 0.7752 - val_accuracy: 0.4909 - val_loss: 4.1460 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6487 - loss: 39.2031 - top_k_categorical_accuracy: 0.7806 - val_accuracy: 0.4909 - val_loss: 3.9682 - val_top_k_categorical_accuracy: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 46/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6593 - loss: 37.7781 - top_k_categorical_accuracy: 0.7904 - val_accuracy: 0.5273 - val_loss: 3.9632 - val_top_k_categorical_accuracy: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 47/1000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6618 - loss: 38.0444 - top_k_categorical_accuracy: 0.7883 - val_accuracy: 0.5091 - val_loss: 3.9608 - val_top_k_categorical_accuracy: 0.8000 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "0.5272727272727272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Plot training history\\nplt.plot(history.history['accuracy'], label='train accuracy')\\nplt.plot(history.history['val_accuracy'], label='val accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend()\\nplt.show()\\n\\nplt.plot(history.history['loss'], label='train loss')\\nplt.plot(history.history['val_loss'], label='val loss')\\nplt.xlabel('Epoch')\\nplt.ylabel('Loss')\\nplt.legend()\\nplt.show()\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MLP obican, 15 puta\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import collections\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "# Function to add Gaussian noise\n",
    "def add_gaussian_noise(image):\n",
    "    row, col, ch = image.shape\n",
    "    mean = 0\n",
    "    var = 0.1\n",
    "    sigma = var ** 0.5\n",
    "    gauss = np.random.normal(mean, sigma, (row, col, ch))\n",
    "    noisy = image + gauss\n",
    "    return np.clip(noisy, 0, 1)\n",
    "\n",
    "# Function to apply Gaussian blur\n",
    "def add_blur(image):\n",
    "    return cv2.GaussianBlur(image, (5, 5), 0)\n",
    "\n",
    "# Function to adjust contrast\n",
    "def adjust_contrast(image, alpha=1.0, beta=0):\n",
    "    return cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "\n",
    "trainings = 15\n",
    "best_accuracy = 0\n",
    "\n",
    "for i in range(trainings):\n",
    "    X = X.reshape(-1, 20, 20, 1)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    label_names = label_encoder.classes_\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.1, stratify=y_encoded)\n",
    "    \n",
    "    y_train_categorical = to_categorical(y_train, num_classes=len(label_encoder.classes_))\n",
    "    y_test_categorical = to_categorical(y_test, num_classes=len(label_encoder.classes_))\n",
    "    \n",
    "    # Apply SMOTE to balance the dataset\n",
    "    X_train_flatten = X_train.reshape(X_train.shape[0], -1)\n",
    "    smote = SMOTE()\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_flatten, np.argmax(y_train_categorical, axis=1))\n",
    "    \n",
    "    # Reshape X_train back to original shape\n",
    "    X_train_resampled = X_train_resampled.reshape(-1, 20, 20, 1)\n",
    "    y_train_resampled_categorical = np.eye(len(label_encoder.classes_))[y_train_resampled]\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_counts = collections.Counter(np.argmax(y_train_resampled_categorical, axis=1))\n",
    "    total_samples = sum(class_counts.values())\n",
    "    class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "    \n",
    "    # Augment the data\n",
    "    augmented_data = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for img, label in zip(X_train_resampled, y_train_resampled_categorical):\n",
    "        augmented_data.append(img)\n",
    "        augmented_labels.append(label)\n",
    "        \n",
    "        # Add Gaussian noise\n",
    "        noisy_image = add_gaussian_noise(img)\n",
    "        noisy_image = noisy_image.reshape(20, 20, 1)  # Ensure correct shape\n",
    "        augmented_data.append(noisy_image)\n",
    "        augmented_labels.append(label)\n",
    "        \n",
    "        # Add blur\n",
    "        blurred_image = add_blur(img)\n",
    "        blurred_image = blurred_image.reshape(20, 20, 1)  # Ensure correct shape\n",
    "        augmented_data.append(blurred_image)\n",
    "        augmented_labels.append(label)\n",
    "        \n",
    "        # Adjust contrast\n",
    "        contrast_image = adjust_contrast(img, alpha=random.uniform(0.5, 1.5), beta=random.uniform(-50, 50))\n",
    "        contrast_image = contrast_image.reshape(20, 20, 1)  # Ensure correct shape\n",
    "        augmented_data.append(contrast_image)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "    X_train_augmented = np.array(augmented_data)\n",
    "    y_train_augmented = np.array(augmented_labels)\n",
    "    \n",
    "    # Define the MLP model\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(20, 20, 1)),\n",
    "        Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(label_encoder.classes_), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy', 'top_k_categorical_accuracy'])\n",
    "    \n",
    "    # Callbacks for learning rate adjustment and early stopping\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=40, min_lr=1e-6)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_train_augmented, y_train_augmented,\n",
    "                        validation_data=(X_test, y_test_categorical),\n",
    "                        epochs=1000,\n",
    "                        callbacks=[reduce_lr, early_stopping],\n",
    "                        class_weight=class_weights,\n",
    "                        batch_size=64\n",
    "                        )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "print(best_accuracy)\n",
    "\n",
    "# Print classification report\n",
    "#print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "'''\n",
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1119208b-28bd-450f-b699-56a067e3a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 0.9145313501358032; compile_metrics of 100.0%\n",
      "Score for fold 2: loss of 0.8997101187705994; compile_metrics of 100.0%\n",
      "Score for fold 3: loss of 5.18798828125; compile_metrics of 0.0%\n",
      "Score for fold 4: loss of 5.27087926864624; compile_metrics of 0.0%\n",
      "Score for fold 5: loss of 0.8512699007987976; compile_metrics of 100.0%\n",
      "Score for fold 6: loss of 1.3072110414505005; compile_metrics of 100.0%\n",
      "Score for fold 7: loss of 2.512240171432495; compile_metrics of 0.0%\n",
      "Score for fold 8: loss of 1.5243921279907227; compile_metrics of 100.0%\n",
      "Score for fold 9: loss of 0.8665581345558167; compile_metrics of 100.0%\n",
      "Score for fold 10: loss of 2.663489818572998; compile_metrics of 0.0%\n",
      "Score for fold 11: loss of 1.3502180576324463; compile_metrics of 100.0%\n",
      "Score for fold 12: loss of 3.0292389392852783; compile_metrics of 0.0%\n",
      "Score for fold 13: loss of 1.3630461692810059; compile_metrics of 100.0%\n",
      "Score for fold 14: loss of 5.265138149261475; compile_metrics of 0.0%\n",
      "Score for fold 15: loss of 2.1779792308807373; compile_metrics of 0.0%\n",
      "Score for fold 16: loss of 4.600186347961426; compile_metrics of 0.0%\n",
      "Score for fold 17: loss of 0.9702556729316711; compile_metrics of 100.0%\n",
      "Score for fold 18: loss of 7.65270471572876; compile_metrics of 0.0%\n",
      "Score for fold 19: loss of 5.420086860656738; compile_metrics of 0.0%\n",
      "Score for fold 20: loss of 2.763669490814209; compile_metrics of 0.0%\n",
      "Score for fold 21: loss of 1.019710898399353; compile_metrics of 100.0%\n",
      "Score for fold 22: loss of 1.4367300271987915; compile_metrics of 100.0%\n",
      "Score for fold 23: loss of 6.769132614135742; compile_metrics of 0.0%\n",
      "Score for fold 24: loss of 0.9914399981498718; compile_metrics of 100.0%\n",
      "Score for fold 25: loss of 0.8522976636886597; compile_metrics of 100.0%\n",
      "Score for fold 26: loss of 0.9962201118469238; compile_metrics of 100.0%\n",
      "Score for fold 27: loss of 1.1184160709381104; compile_metrics of 100.0%\n",
      "Score for fold 28: loss of 0.8942587375640869; compile_metrics of 100.0%\n",
      "Score for fold 29: loss of 0.8536155223846436; compile_metrics of 100.0%\n",
      "Score for fold 30: loss of 1.0487735271453857; compile_metrics of 100.0%\n",
      "Score for fold 31: loss of 2.5986111164093018; compile_metrics of 0.0%\n",
      "Score for fold 32: loss of 0.8556042909622192; compile_metrics of 100.0%\n",
      "Score for fold 33: loss of 10.650633811950684; compile_metrics of 0.0%\n",
      "Score for fold 34: loss of 3.422433853149414; compile_metrics of 0.0%\n",
      "Score for fold 35: loss of 1.016284704208374; compile_metrics of 100.0%\n",
      "Score for fold 36: loss of 0.8496214747428894; compile_metrics of 100.0%\n",
      "Score for fold 37: loss of 0.865243673324585; compile_metrics of 100.0%\n",
      "Score for fold 38: loss of 0.8860505819320679; compile_metrics of 100.0%\n",
      "Score for fold 39: loss of 1.188276767730713; compile_metrics of 100.0%\n",
      "Score for fold 40: loss of 0.8571082353591919; compile_metrics of 100.0%\n",
      "Score for fold 41: loss of 2.778978109359741; compile_metrics of 0.0%\n",
      "Score for fold 42: loss of 0.8476248383522034; compile_metrics of 100.0%\n",
      "Score for fold 43: loss of 0.8637148141860962; compile_metrics of 100.0%\n",
      "Score for fold 44: loss of 0.9180042147636414; compile_metrics of 100.0%\n",
      "Score for fold 45: loss of 1.1472538709640503; compile_metrics of 100.0%\n",
      "Score for fold 46: loss of 4.383771896362305; compile_metrics of 0.0%\n",
      "Score for fold 47: loss of 1.0646260976791382; compile_metrics of 100.0%\n",
      "Score for fold 48: loss of 1.1228376626968384; compile_metrics of 100.0%\n",
      "Score for fold 49: loss of 5.926088809967041; compile_metrics of 0.0%\n",
      "Score for fold 50: loss of 1.43735933303833; compile_metrics of 100.0%\n",
      "Score for fold 51: loss of 1.018784523010254; compile_metrics of 100.0%\n",
      "Score for fold 52: loss of 0.8999790549278259; compile_metrics of 100.0%\n",
      "Score for fold 53: loss of 1.0636838674545288; compile_metrics of 100.0%\n",
      "Score for fold 54: loss of 1.3621835708618164; compile_metrics of 100.0%\n",
      "Score for fold 55: loss of 3.968229293823242; compile_metrics of 0.0%\n",
      "Score for fold 56: loss of 2.4329519271850586; compile_metrics of 0.0%\n",
      "Score for fold 57: loss of 1.108647108078003; compile_metrics of 100.0%\n",
      "Score for fold 58: loss of 2.145414352416992; compile_metrics of 0.0%\n",
      "Score for fold 59: loss of 1.1898925304412842; compile_metrics of 100.0%\n",
      "Score for fold 60: loss of 0.9813845753669739; compile_metrics of 100.0%\n",
      "Score for fold 61: loss of 1.4515748023986816; compile_metrics of 100.0%\n",
      "Score for fold 62: loss of 1.0760406255722046; compile_metrics of 100.0%\n",
      "Score for fold 63: loss of 0.9202590584754944; compile_metrics of 100.0%\n",
      "Score for fold 64: loss of 3.6321418285369873; compile_metrics of 0.0%\n",
      "Score for fold 65: loss of 1.098732829093933; compile_metrics of 100.0%\n",
      "Score for fold 66: loss of 0.8965722322463989; compile_metrics of 100.0%\n",
      "Score for fold 67: loss of 0.9632938504219055; compile_metrics of 100.0%\n",
      "Score for fold 68: loss of 1.8310900926589966; compile_metrics of 100.0%\n",
      "Score for fold 69: loss of 7.672354698181152; compile_metrics of 0.0%\n",
      "Score for fold 70: loss of 1.0073388814926147; compile_metrics of 100.0%\n",
      "Score for fold 71: loss of 0.9069358110427856; compile_metrics of 100.0%\n",
      "Score for fold 72: loss of 0.950287938117981; compile_metrics of 100.0%\n",
      "Score for fold 73: loss of 1.002694845199585; compile_metrics of 100.0%\n",
      "Score for fold 74: loss of 2.7553863525390625; compile_metrics of 0.0%\n",
      "Score for fold 75: loss of 1.0388094186782837; compile_metrics of 100.0%\n",
      "Score for fold 76: loss of 0.9377893209457397; compile_metrics of 100.0%\n",
      "Score for fold 77: loss of 0.9358014464378357; compile_metrics of 100.0%\n",
      "Score for fold 78: loss of 1.1500808000564575; compile_metrics of 100.0%\n",
      "Score for fold 79: loss of 0.9729105830192566; compile_metrics of 100.0%\n",
      "Score for fold 80: loss of 0.8851261138916016; compile_metrics of 100.0%\n",
      "Score for fold 81: loss of 3.198911428451538; compile_metrics of 0.0%\n",
      "Score for fold 82: loss of 6.230937480926514; compile_metrics of 0.0%\n",
      "Score for fold 83: loss of 1.092820644378662; compile_metrics of 100.0%\n",
      "Score for fold 84: loss of 1.0357728004455566; compile_metrics of 100.0%\n",
      "Score for fold 85: loss of 2.2176527976989746; compile_metrics of 0.0%\n",
      "Score for fold 86: loss of 5.443781852722168; compile_metrics of 0.0%\n",
      "Score for fold 87: loss of 0.9510019421577454; compile_metrics of 100.0%\n",
      "Score for fold 88: loss of 0.9825817942619324; compile_metrics of 100.0%\n",
      "Score for fold 89: loss of 1.9529452323913574; compile_metrics of 100.0%\n",
      "Score for fold 90: loss of 2.2843568325042725; compile_metrics of 0.0%\n",
      "Score for fold 91: loss of 3.3209431171417236; compile_metrics of 0.0%\n",
      "Score for fold 92: loss of 4.080124855041504; compile_metrics of 0.0%\n",
      "Score for fold 93: loss of 2.754002332687378; compile_metrics of 0.0%\n",
      "Score for fold 94: loss of 2.717539072036743; compile_metrics of 0.0%\n",
      "Score for fold 95: loss of 2.905186176300049; compile_metrics of 0.0%\n",
      "Score for fold 96: loss of 0.8674067854881287; compile_metrics of 100.0%\n",
      "Score for fold 97: loss of 0.8453837633132935; compile_metrics of 100.0%\n",
      "Score for fold 98: loss of 3.619534969329834; compile_metrics of 0.0%\n",
      "Score for fold 99: loss of 0.9100253582000732; compile_metrics of 100.0%\n",
      "Score for fold 100: loss of 0.8853994607925415; compile_metrics of 100.0%\n",
      "Score for fold 101: loss of 0.8757191300392151; compile_metrics of 100.0%\n",
      "Score for fold 102: loss of 0.8487359285354614; compile_metrics of 100.0%\n",
      "Score for fold 103: loss of 0.8502488136291504; compile_metrics of 100.0%\n",
      "Score for fold 104: loss of 0.8770296573638916; compile_metrics of 100.0%\n",
      "Score for fold 105: loss of 0.8583393692970276; compile_metrics of 100.0%\n",
      "Score for fold 106: loss of 0.9544982314109802; compile_metrics of 100.0%\n",
      "Score for fold 107: loss of 2.0895586013793945; compile_metrics of 100.0%\n",
      "Score for fold 108: loss of 0.868798553943634; compile_metrics of 100.0%\n",
      "Score for fold 109: loss of 1.029897928237915; compile_metrics of 100.0%\n",
      "Score for fold 110: loss of 0.8422600030899048; compile_metrics of 100.0%\n",
      "Score for fold 111: loss of 0.8510485291481018; compile_metrics of 100.0%\n",
      "Score for fold 112: loss of 1.0252915620803833; compile_metrics of 100.0%\n",
      "Score for fold 113: loss of 0.8971385359764099; compile_metrics of 100.0%\n",
      "Score for fold 114: loss of 0.8570410013198853; compile_metrics of 100.0%\n",
      "Score for fold 115: loss of 2.5421040058135986; compile_metrics of 0.0%\n",
      "Score for fold 116: loss of 0.8439052104949951; compile_metrics of 100.0%\n",
      "Score for fold 117: loss of 0.9441524744033813; compile_metrics of 100.0%\n",
      "Score for fold 118: loss of 0.8566249012947083; compile_metrics of 100.0%\n",
      "Score for fold 119: loss of 4.417215347290039; compile_metrics of 0.0%\n",
      "Score for fold 120: loss of 0.8772983551025391; compile_metrics of 100.0%\n",
      "Score for fold 121: loss of 6.678630828857422; compile_metrics of 0.0%\n",
      "Score for fold 122: loss of 0.8506032228469849; compile_metrics of 100.0%\n",
      "Score for fold 123: loss of 0.844548761844635; compile_metrics of 100.0%\n",
      "Score for fold 124: loss of 4.96046257019043; compile_metrics of 0.0%\n",
      "Score for fold 125: loss of 3.323199987411499; compile_metrics of 0.0%\n",
      "Score for fold 126: loss of 0.8494036197662354; compile_metrics of 100.0%\n",
      "Score for fold 127: loss of 0.8655701875686646; compile_metrics of 100.0%\n",
      "Score for fold 128: loss of 0.8427124619483948; compile_metrics of 100.0%\n",
      "Score for fold 129: loss of 0.8583852648735046; compile_metrics of 100.0%\n",
      "Score for fold 130: loss of 0.8481633067131042; compile_metrics of 100.0%\n",
      "Score for fold 131: loss of 0.858261227607727; compile_metrics of 100.0%\n",
      "Score for fold 132: loss of 0.8486145734786987; compile_metrics of 100.0%\n",
      "Score for fold 133: loss of 0.843243420124054; compile_metrics of 100.0%\n",
      "Score for fold 134: loss of 0.875340461730957; compile_metrics of 100.0%\n",
      "Score for fold 135: loss of 1.042438268661499; compile_metrics of 100.0%\n",
      "Score for fold 136: loss of 0.946558952331543; compile_metrics of 100.0%\n",
      "Score for fold 137: loss of 1.1735543012619019; compile_metrics of 100.0%\n",
      "Score for fold 138: loss of 4.585023880004883; compile_metrics of 0.0%\n",
      "Score for fold 139: loss of 2.8207719326019287; compile_metrics of 0.0%\n",
      "Score for fold 140: loss of 1.4197571277618408; compile_metrics of 100.0%\n",
      "Score for fold 141: loss of 0.9894739985466003; compile_metrics of 100.0%\n",
      "Score for fold 142: loss of 2.641016721725464; compile_metrics of 0.0%\n",
      "Score for fold 143: loss of 4.459473609924316; compile_metrics of 0.0%\n",
      "Score for fold 144: loss of 0.903637170791626; compile_metrics of 100.0%\n",
      "Score for fold 145: loss of 1.2223024368286133; compile_metrics of 100.0%\n",
      "Score for fold 146: loss of 0.9341103434562683; compile_metrics of 100.0%\n",
      "Score for fold 147: loss of 0.9614943861961365; compile_metrics of 100.0%\n",
      "Score for fold 148: loss of 1.0447571277618408; compile_metrics of 100.0%\n",
      "Score for fold 149: loss of 4.580891132354736; compile_metrics of 0.0%\n",
      "Score for fold 150: loss of 1.623137354850769; compile_metrics of 100.0%\n",
      "Score for fold 151: loss of 3.0571906566619873; compile_metrics of 0.0%\n",
      "Score for fold 152: loss of 5.05222225189209; compile_metrics of 0.0%\n",
      "Score for fold 153: loss of 0.9414388537406921; compile_metrics of 100.0%\n",
      "Score for fold 154: loss of 0.9513896703720093; compile_metrics of 100.0%\n",
      "Score for fold 155: loss of 0.9970871210098267; compile_metrics of 100.0%\n",
      "Score for fold 156: loss of 2.469743013381958; compile_metrics of 0.0%\n",
      "Score for fold 157: loss of 1.5135303735733032; compile_metrics of 100.0%\n",
      "Score for fold 158: loss of 0.892777144908905; compile_metrics of 100.0%\n",
      "Score for fold 159: loss of 6.663459777832031; compile_metrics of 0.0%\n",
      "Score for fold 160: loss of 3.201796770095825; compile_metrics of 0.0%\n",
      "Score for fold 161: loss of 1.2834351062774658; compile_metrics of 100.0%\n",
      "Score for fold 162: loss of 1.4165918827056885; compile_metrics of 100.0%\n",
      "Score for fold 163: loss of 3.608611583709717; compile_metrics of 0.0%\n",
      "Score for fold 164: loss of 1.013505458831787; compile_metrics of 100.0%\n",
      "Score for fold 165: loss of 2.4509665966033936; compile_metrics of 0.0%\n",
      "Score for fold 166: loss of 0.9840571284294128; compile_metrics of 100.0%\n",
      "Score for fold 167: loss of 3.515469551086426; compile_metrics of 0.0%\n",
      "Score for fold 168: loss of 4.139593601226807; compile_metrics of 0.0%\n",
      "Score for fold 169: loss of 2.1948046684265137; compile_metrics of 0.0%\n",
      "Score for fold 170: loss of 1.9040757417678833; compile_metrics of 100.0%\n",
      "Score for fold 171: loss of 2.4330427646636963; compile_metrics of 0.0%\n",
      "Score for fold 172: loss of 9.265478134155273; compile_metrics of 0.0%\n",
      "Score for fold 173: loss of 1.3637393712997437; compile_metrics of 100.0%\n",
      "Score for fold 174: loss of 0.8832404017448425; compile_metrics of 100.0%\n",
      "Score for fold 175: loss of 0.8674540519714355; compile_metrics of 100.0%\n",
      "Score for fold 176: loss of 1.0323398113250732; compile_metrics of 100.0%\n",
      "Score for fold 177: loss of 4.5135979652404785; compile_metrics of 0.0%\n",
      "Score for fold 178: loss of 0.9060455560684204; compile_metrics of 100.0%\n",
      "Score for fold 179: loss of 4.041301250457764; compile_metrics of 0.0%\n",
      "Score for fold 180: loss of 0.8793611526489258; compile_metrics of 100.0%\n",
      "Score for fold 181: loss of 1.007790446281433; compile_metrics of 100.0%\n",
      "Score for fold 182: loss of 4.3650736808776855; compile_metrics of 0.0%\n",
      "Score for fold 183: loss of 0.9729588627815247; compile_metrics of 100.0%\n",
      "Score for fold 184: loss of 1.2608150243759155; compile_metrics of 100.0%\n",
      "Score for fold 185: loss of 1.1290364265441895; compile_metrics of 100.0%\n",
      "Score for fold 186: loss of 0.9676791429519653; compile_metrics of 100.0%\n",
      "Score for fold 187: loss of 3.1401355266571045; compile_metrics of 0.0%\n",
      "Score for fold 188: loss of 0.9027136564254761; compile_metrics of 100.0%\n",
      "Score for fold 189: loss of 1.0563855171203613; compile_metrics of 100.0%\n",
      "Score for fold 190: loss of 2.1845340728759766; compile_metrics of 0.0%\n",
      "Score for fold 191: loss of 4.012030601501465; compile_metrics of 0.0%\n",
      "Score for fold 192: loss of 1.1500426530838013; compile_metrics of 100.0%\n",
      "Score for fold 193: loss of 0.858735978603363; compile_metrics of 100.0%\n",
      "Score for fold 194: loss of 3.5254149436950684; compile_metrics of 0.0%\n",
      "Score for fold 195: loss of 0.8655185699462891; compile_metrics of 100.0%\n",
      "Score for fold 196: loss of 4.445860385894775; compile_metrics of 0.0%\n",
      "Score for fold 197: loss of 0.8958170413970947; compile_metrics of 100.0%\n",
      "Score for fold 198: loss of 4.186395645141602; compile_metrics of 0.0%\n",
      "Score for fold 199: loss of 1.6231493949890137; compile_metrics of 100.0%\n",
      "Score for fold 200: loss of 5.213723182678223; compile_metrics of 0.0%\n",
      "Score for fold 201: loss of 4.592926502227783; compile_metrics of 0.0%\n",
      "Score for fold 202: loss of 3.8238515853881836; compile_metrics of 0.0%\n",
      "Score for fold 203: loss of 1.5331189632415771; compile_metrics of 100.0%\n",
      "Score for fold 204: loss of 1.9469194412231445; compile_metrics of 100.0%\n",
      "Score for fold 205: loss of 1.6485645771026611; compile_metrics of 100.0%\n",
      "Score for fold 206: loss of 0.8736959099769592; compile_metrics of 100.0%\n",
      "Score for fold 207: loss of 1.9564417600631714; compile_metrics of 100.0%\n",
      "Score for fold 208: loss of 0.8894238471984863; compile_metrics of 100.0%\n",
      "Score for fold 209: loss of 2.249124526977539; compile_metrics of 100.0%\n",
      "Score for fold 210: loss of 1.157512903213501; compile_metrics of 100.0%\n",
      "Score for fold 211: loss of 4.04813289642334; compile_metrics of 0.0%\n",
      "Score for fold 212: loss of 1.4706069231033325; compile_metrics of 100.0%\n",
      "Score for fold 213: loss of 1.7207614183425903; compile_metrics of 100.0%\n",
      "Score for fold 214: loss of 3.872807025909424; compile_metrics of 0.0%\n",
      "Score for fold 215: loss of 1.6601011753082275; compile_metrics of 100.0%\n",
      "Score for fold 216: loss of 1.2671796083450317; compile_metrics of 100.0%\n",
      "Score for fold 217: loss of 9.151823043823242; compile_metrics of 0.0%\n",
      "Score for fold 218: loss of 3.41133713722229; compile_metrics of 0.0%\n",
      "Score for fold 219: loss of 1.5242996215820312; compile_metrics of 100.0%\n",
      "Score for fold 220: loss of 4.526025772094727; compile_metrics of 0.0%\n",
      "Score for fold 221: loss of 4.809184551239014; compile_metrics of 0.0%\n",
      "Score for fold 222: loss of 0.9819000959396362; compile_metrics of 100.0%\n",
      "Score for fold 223: loss of 4.509710311889648; compile_metrics of 0.0%\n",
      "Score for fold 224: loss of 1.0820006132125854; compile_metrics of 100.0%\n",
      "Score for fold 225: loss of 2.493654489517212; compile_metrics of 0.0%\n",
      "Score for fold 226: loss of 1.1396369934082031; compile_metrics of 100.0%\n",
      "Score for fold 227: loss of 1.1565322875976562; compile_metrics of 100.0%\n",
      "Score for fold 228: loss of 1.1396799087524414; compile_metrics of 100.0%\n",
      "Score for fold 229: loss of 1.3473470211029053; compile_metrics of 100.0%\n",
      "Score for fold 230: loss of 1.2807239294052124; compile_metrics of 100.0%\n",
      "Score for fold 231: loss of 0.8975663781166077; compile_metrics of 100.0%\n",
      "Score for fold 232: loss of 3.504795551300049; compile_metrics of 0.0%\n",
      "Score for fold 233: loss of 5.2884745597839355; compile_metrics of 0.0%\n",
      "Score for fold 234: loss of 1.4628809690475464; compile_metrics of 100.0%\n",
      "Score for fold 235: loss of 1.563801646232605; compile_metrics of 100.0%\n",
      "Score for fold 236: loss of 3.3461806774139404; compile_metrics of 0.0%\n",
      "Score for fold 237: loss of 0.8631141185760498; compile_metrics of 100.0%\n",
      "Score for fold 238: loss of 3.130690574645996; compile_metrics of 0.0%\n",
      "Score for fold 239: loss of 1.5758824348449707; compile_metrics of 100.0%\n",
      "Score for fold 240: loss of 7.754516124725342; compile_metrics of 0.0%\n",
      "Score for fold 241: loss of 0.943807065486908; compile_metrics of 100.0%\n",
      "Score for fold 242: loss of 1.6853501796722412; compile_metrics of 100.0%\n",
      "Score for fold 243: loss of 1.612069010734558; compile_metrics of 100.0%\n",
      "Score for fold 244: loss of 0.9322299957275391; compile_metrics of 100.0%\n",
      "Score for fold 245: loss of 5.855983257293701; compile_metrics of 0.0%\n",
      "Score for fold 246: loss of 0.8973303437232971; compile_metrics of 100.0%\n",
      "Score for fold 247: loss of 0.9579850435256958; compile_metrics of 100.0%\n",
      "Score for fold 248: loss of 4.164032459259033; compile_metrics of 0.0%\n",
      "Score for fold 249: loss of 2.961075782775879; compile_metrics of 0.0%\n",
      "Score for fold 250: loss of 1.0691900253295898; compile_metrics of 100.0%\n",
      "Score for fold 251: loss of 6.608582019805908; compile_metrics of 0.0%\n",
      "Score for fold 252: loss of 1.0435467958450317; compile_metrics of 100.0%\n",
      "Score for fold 253: loss of 0.9752589464187622; compile_metrics of 100.0%\n",
      "Score for fold 254: loss of 9.656205177307129; compile_metrics of 0.0%\n",
      "Score for fold 255: loss of 2.7057058811187744; compile_metrics of 0.0%\n",
      "Score for fold 256: loss of 6.151993751525879; compile_metrics of 0.0%\n",
      "Score for fold 257: loss of 9.549145698547363; compile_metrics of 0.0%\n",
      "Score for fold 258: loss of 1.6350444555282593; compile_metrics of 100.0%\n",
      "Score for fold 259: loss of 1.140688180923462; compile_metrics of 100.0%\n",
      "Score for fold 260: loss of 1.272387981414795; compile_metrics of 100.0%\n",
      "Score for fold 261: loss of 2.2854979038238525; compile_metrics of 100.0%\n",
      "Score for fold 262: loss of 0.8673141598701477; compile_metrics of 100.0%\n",
      "Score for fold 263: loss of 2.5854382514953613; compile_metrics of 0.0%\n",
      "Score for fold 264: loss of 0.8433277606964111; compile_metrics of 100.0%\n",
      "Score for fold 265: loss of 0.8572008013725281; compile_metrics of 100.0%\n",
      "Score for fold 266: loss of 2.721238613128662; compile_metrics of 0.0%\n",
      "Score for fold 267: loss of 0.8481435775756836; compile_metrics of 100.0%\n",
      "Score for fold 268: loss of 1.1650007963180542; compile_metrics of 100.0%\n",
      "Score for fold 269: loss of 0.8490662574768066; compile_metrics of 100.0%\n",
      "Score for fold 270: loss of 2.633624315261841; compile_metrics of 0.0%\n",
      "Score for fold 271: loss of 0.8498359322547913; compile_metrics of 100.0%\n",
      "Score for fold 272: loss of 5.160952091217041; compile_metrics of 0.0%\n",
      "Score for fold 273: loss of 0.8826664686203003; compile_metrics of 100.0%\n",
      "Score for fold 274: loss of 0.8504940867424011; compile_metrics of 100.0%\n",
      "Score for fold 275: loss of 1.1376429796218872; compile_metrics of 100.0%\n",
      "Score for fold 276: loss of 0.8587802648544312; compile_metrics of 100.0%\n",
      "Score for fold 277: loss of 0.8746435046195984; compile_metrics of 100.0%\n",
      "Score for fold 278: loss of 0.8494434952735901; compile_metrics of 100.0%\n",
      "Score for fold 279: loss of 0.8498833179473877; compile_metrics of 100.0%\n",
      "Score for fold 280: loss of 0.8521544933319092; compile_metrics of 100.0%\n",
      "Score for fold 281: loss of 1.0890833139419556; compile_metrics of 100.0%\n",
      "Score for fold 282: loss of 0.880096971988678; compile_metrics of 100.0%\n",
      "Score for fold 283: loss of 1.3260064125061035; compile_metrics of 100.0%\n",
      "Score for fold 284: loss of 0.8473911285400391; compile_metrics of 100.0%\n",
      "Score for fold 285: loss of 0.8939700722694397; compile_metrics of 100.0%\n",
      "Score for fold 286: loss of 4.434677600860596; compile_metrics of 0.0%\n",
      "Score for fold 287: loss of 0.8508908152580261; compile_metrics of 100.0%\n",
      "Score for fold 288: loss of 0.8983324766159058; compile_metrics of 100.0%\n",
      "Score for fold 289: loss of 0.9843370318412781; compile_metrics of 100.0%\n",
      "Score for fold 290: loss of 0.8517184853553772; compile_metrics of 100.0%\n",
      "Score for fold 291: loss of 3.3782148361206055; compile_metrics of 0.0%\n",
      "Score for fold 292: loss of 1.2917580604553223; compile_metrics of 100.0%\n",
      "Score for fold 293: loss of 1.144906759262085; compile_metrics of 100.0%\n",
      "Score for fold 294: loss of 5.7757158279418945; compile_metrics of 0.0%\n",
      "Score for fold 295: loss of 0.8478230834007263; compile_metrics of 100.0%\n",
      "Score for fold 296: loss of 1.5614266395568848; compile_metrics of 100.0%\n",
      "Score for fold 297: loss of 1.2176244258880615; compile_metrics of 100.0%\n",
      "Score for fold 298: loss of 0.9958539605140686; compile_metrics of 100.0%\n",
      "Score for fold 299: loss of 3.170675039291382; compile_metrics of 0.0%\n",
      "Score for fold 300: loss of 1.173718810081482; compile_metrics of 100.0%\n",
      "Score for fold 301: loss of 1.2617276906967163; compile_metrics of 100.0%\n",
      "Score for fold 302: loss of 7.932820796966553; compile_metrics of 0.0%\n",
      "Score for fold 303: loss of 1.0100228786468506; compile_metrics of 100.0%\n",
      "Score for fold 304: loss of 1.0789541006088257; compile_metrics of 100.0%\n",
      "Score for fold 305: loss of 1.0157586336135864; compile_metrics of 100.0%\n",
      "Score for fold 306: loss of 4.519046306610107; compile_metrics of 0.0%\n",
      "Score for fold 307: loss of 4.454129219055176; compile_metrics of 0.0%\n",
      "Score for fold 308: loss of 5.8411784172058105; compile_metrics of 0.0%\n",
      "Score for fold 309: loss of 2.989814281463623; compile_metrics of 0.0%\n",
      "Score for fold 310: loss of 1.769945740699768; compile_metrics of 100.0%\n",
      "Score for fold 311: loss of 1.3872629404067993; compile_metrics of 100.0%\n",
      "Score for fold 312: loss of 1.3379818201065063; compile_metrics of 100.0%\n",
      "Score for fold 313: loss of 6.436212062835693; compile_metrics of 0.0%\n",
      "Score for fold 314: loss of 2.8640477657318115; compile_metrics of 0.0%\n",
      "Score for fold 315: loss of 1.1514062881469727; compile_metrics of 100.0%\n",
      "Score for fold 316: loss of 1.3635591268539429; compile_metrics of 100.0%\n",
      "Score for fold 317: loss of 1.0330979824066162; compile_metrics of 100.0%\n",
      "Score for fold 318: loss of 2.9557766914367676; compile_metrics of 0.0%\n",
      "Score for fold 319: loss of 5.021607875823975; compile_metrics of 0.0%\n",
      "Score for fold 320: loss of 3.1465868949890137; compile_metrics of 0.0%\n",
      "Score for fold 321: loss of 4.859615325927734; compile_metrics of 0.0%\n",
      "Score for fold 322: loss of 6.504964351654053; compile_metrics of 0.0%\n",
      "Score for fold 323: loss of 3.5826098918914795; compile_metrics of 0.0%\n",
      "Score for fold 324: loss of 3.3974528312683105; compile_metrics of 0.0%\n",
      "Score for fold 325: loss of 1.2962771654129028; compile_metrics of 100.0%\n",
      "Score for fold 326: loss of 8.11217975616455; compile_metrics of 0.0%\n",
      "Score for fold 327: loss of 1.2693532705307007; compile_metrics of 100.0%\n",
      "Score for fold 328: loss of 3.3092093467712402; compile_metrics of 0.0%\n",
      "Score for fold 329: loss of 1.3978208303451538; compile_metrics of 100.0%\n",
      "Score for fold 330: loss of 1.219752550125122; compile_metrics of 100.0%\n",
      "Score for fold 331: loss of 1.2913975715637207; compile_metrics of 100.0%\n",
      "Score for fold 332: loss of 3.7751498222351074; compile_metrics of 0.0%\n",
      "Score for fold 333: loss of 1.1600545644760132; compile_metrics of 100.0%\n",
      "Score for fold 334: loss of 1.3920832872390747; compile_metrics of 100.0%\n",
      "Score for fold 335: loss of 4.015491485595703; compile_metrics of 0.0%\n",
      "Score for fold 336: loss of 3.241015911102295; compile_metrics of 0.0%\n",
      "Score for fold 337: loss of 9.854778289794922; compile_metrics of 0.0%\n",
      "Score for fold 338: loss of 7.1056060791015625; compile_metrics of 0.0%\n",
      "Score for fold 339: loss of 9.575594902038574; compile_metrics of 0.0%\n",
      "Score for fold 340: loss of 1.5303739309310913; compile_metrics of 100.0%\n",
      "Score for fold 341: loss of 0.9527623057365417; compile_metrics of 100.0%\n",
      "Score for fold 342: loss of 1.1338540315628052; compile_metrics of 100.0%\n",
      "Score for fold 343: loss of 1.1969587802886963; compile_metrics of 100.0%\n",
      "Score for fold 344: loss of 0.8984581232070923; compile_metrics of 100.0%\n",
      "Score for fold 345: loss of 0.9781526923179626; compile_metrics of 100.0%\n",
      "Score for fold 346: loss of 1.453646183013916; compile_metrics of 100.0%\n",
      "Score for fold 347: loss of 0.9219076037406921; compile_metrics of 100.0%\n",
      "Score for fold 348: loss of 1.0221309661865234; compile_metrics of 100.0%\n",
      "Score for fold 349: loss of 2.193153142929077; compile_metrics of 100.0%\n",
      "Score for fold 350: loss of 0.8715349435806274; compile_metrics of 100.0%\n",
      "Score for fold 351: loss of 0.8845039010047913; compile_metrics of 100.0%\n",
      "Score for fold 352: loss of 0.8740363717079163; compile_metrics of 100.0%\n",
      "Score for fold 353: loss of 4.014443874359131; compile_metrics of 0.0%\n",
      "Score for fold 354: loss of 0.9377525448799133; compile_metrics of 100.0%\n",
      "Score for fold 355: loss of 0.874453067779541; compile_metrics of 100.0%\n",
      "Score for fold 356: loss of 0.875803530216217; compile_metrics of 100.0%\n",
      "Score for fold 357: loss of 0.8745123147964478; compile_metrics of 100.0%\n",
      "Score for fold 358: loss of 1.1358495950698853; compile_metrics of 100.0%\n",
      "Score for fold 359: loss of 0.905798077583313; compile_metrics of 100.0%\n",
      "Score for fold 360: loss of 1.196414828300476; compile_metrics of 100.0%\n",
      "Score for fold 361: loss of 0.8608114719390869; compile_metrics of 100.0%\n",
      "Score for fold 362: loss of 0.9142574667930603; compile_metrics of 100.0%\n",
      "Score for fold 363: loss of 0.8920106291770935; compile_metrics of 100.0%\n",
      "Score for fold 364: loss of 0.907257080078125; compile_metrics of 100.0%\n",
      "Score for fold 365: loss of 0.9912732839584351; compile_metrics of 100.0%\n",
      "Score for fold 366: loss of 1.0097646713256836; compile_metrics of 100.0%\n",
      "Score for fold 367: loss of 0.8452950119972229; compile_metrics of 100.0%\n",
      "Score for fold 368: loss of 0.8597180247306824; compile_metrics of 100.0%\n",
      "Score for fold 369: loss of 0.8594998717308044; compile_metrics of 100.0%\n",
      "Score for fold 370: loss of 0.9031014442443848; compile_metrics of 100.0%\n",
      "Score for fold 371: loss of 1.0246700048446655; compile_metrics of 100.0%\n",
      "Score for fold 372: loss of 0.8723406195640564; compile_metrics of 100.0%\n",
      "Score for fold 373: loss of 3.7357330322265625; compile_metrics of 0.0%\n",
      "Score for fold 374: loss of 0.8517526388168335; compile_metrics of 100.0%\n",
      "Score for fold 375: loss of 6.657891750335693; compile_metrics of 0.0%\n",
      "Score for fold 376: loss of 3.565439462661743; compile_metrics of 0.0%\n",
      "Score for fold 377: loss of 4.787084579467773; compile_metrics of 0.0%\n",
      "Score for fold 378: loss of 0.8588809370994568; compile_metrics of 100.0%\n",
      "Score for fold 379: loss of 0.9811761379241943; compile_metrics of 100.0%\n",
      "Score for fold 380: loss of 3.507692337036133; compile_metrics of 0.0%\n",
      "Score for fold 381: loss of 2.4508912563323975; compile_metrics of 0.0%\n",
      "Score for fold 382: loss of 2.6034562587738037; compile_metrics of 0.0%\n",
      "Score for fold 383: loss of 3.7655715942382812; compile_metrics of 0.0%\n",
      "Score for fold 384: loss of 1.674576997756958; compile_metrics of 100.0%\n",
      "Score for fold 385: loss of 6.435464859008789; compile_metrics of 0.0%\n",
      "Score for fold 386: loss of 1.3051568269729614; compile_metrics of 100.0%\n",
      "Score for fold 387: loss of 0.8471894264221191; compile_metrics of 100.0%\n",
      "Score for fold 388: loss of 0.9241212010383606; compile_metrics of 100.0%\n",
      "Score for fold 389: loss of 4.589362621307373; compile_metrics of 0.0%\n",
      "Score for fold 390: loss of 1.1900503635406494; compile_metrics of 100.0%\n",
      "Score for fold 391: loss of 1.0583935976028442; compile_metrics of 100.0%\n",
      "Score for fold 392: loss of 2.8429954051971436; compile_metrics of 0.0%\n",
      "Score for fold 393: loss of 8.738680839538574; compile_metrics of 0.0%\n",
      "Score for fold 394: loss of 2.9355812072753906; compile_metrics of 0.0%\n",
      "Score for fold 395: loss of 1.0060216188430786; compile_metrics of 100.0%\n",
      "Score for fold 396: loss of 2.6921873092651367; compile_metrics of 0.0%\n",
      "Score for fold 397: loss of 1.0157511234283447; compile_metrics of 100.0%\n",
      "Score for fold 398: loss of 0.9667556881904602; compile_metrics of 100.0%\n",
      "Score for fold 399: loss of 0.8523995280265808; compile_metrics of 100.0%\n",
      "Score for fold 400: loss of 0.8825660943984985; compile_metrics of 100.0%\n",
      "Score for fold 401: loss of 0.9969438314437866; compile_metrics of 100.0%\n",
      "Score for fold 402: loss of 1.9642162322998047; compile_metrics of 100.0%\n",
      "Score for fold 403: loss of 2.391787528991699; compile_metrics of 0.0%\n",
      "Score for fold 404: loss of 3.927360773086548; compile_metrics of 0.0%\n",
      "Score for fold 405: loss of 5.0722246170043945; compile_metrics of 0.0%\n",
      "Score for fold 406: loss of 3.5384254455566406; compile_metrics of 0.0%\n",
      "Score for fold 407: loss of 3.484746217727661; compile_metrics of 0.0%\n",
      "Score for fold 408: loss of 3.830873489379883; compile_metrics of 0.0%\n",
      "Score for fold 409: loss of 6.34848165512085; compile_metrics of 0.0%\n",
      "Score for fold 410: loss of 6.238144397735596; compile_metrics of 0.0%\n",
      "Score for fold 411: loss of 1.2610890865325928; compile_metrics of 100.0%\n",
      "Score for fold 412: loss of 4.736393451690674; compile_metrics of 0.0%\n",
      "Score for fold 413: loss of 1.7625651359558105; compile_metrics of 100.0%\n",
      "Score for fold 414: loss of 5.012626647949219; compile_metrics of 0.0%\n",
      "Score for fold 415: loss of 2.1779661178588867; compile_metrics of 100.0%\n",
      "Score for fold 416: loss of 5.591870307922363; compile_metrics of 0.0%\n",
      "Score for fold 417: loss of 2.9185383319854736; compile_metrics of 0.0%\n",
      "Score for fold 418: loss of 5.890451431274414; compile_metrics of 0.0%\n",
      "Score for fold 419: loss of 3.551107406616211; compile_metrics of 0.0%\n",
      "Score for fold 420: loss of 1.1133108139038086; compile_metrics of 100.0%\n",
      "Score for fold 421: loss of 4.27368688583374; compile_metrics of 0.0%\n",
      "Score for fold 422: loss of 0.931184709072113; compile_metrics of 100.0%\n",
      "Score for fold 423: loss of 2.38320255279541; compile_metrics of 0.0%\n",
      "Score for fold 424: loss of 5.267359733581543; compile_metrics of 0.0%\n",
      "Score for fold 425: loss of 0.9657809138298035; compile_metrics of 100.0%\n",
      "Score for fold 426: loss of 5.369721412658691; compile_metrics of 0.0%\n",
      "Score for fold 427: loss of 1.0198718309402466; compile_metrics of 100.0%\n",
      "Score for fold 428: loss of 1.2317463159561157; compile_metrics of 100.0%\n",
      "Score for fold 429: loss of 0.8989838361740112; compile_metrics of 100.0%\n",
      "Score for fold 430: loss of 0.8677964806556702; compile_metrics of 100.0%\n",
      "Score for fold 431: loss of 3.930673599243164; compile_metrics of 0.0%\n",
      "Score for fold 432: loss of 9.267595291137695; compile_metrics of 0.0%\n",
      "Score for fold 433: loss of 5.60447359085083; compile_metrics of 0.0%\n",
      "Score for fold 434: loss of 0.9728865623474121; compile_metrics of 100.0%\n",
      "Score for fold 435: loss of 0.9395511150360107; compile_metrics of 100.0%\n",
      "Score for fold 436: loss of 7.066920280456543; compile_metrics of 0.0%\n",
      "Score for fold 437: loss of 0.9028868079185486; compile_metrics of 100.0%\n",
      "Score for fold 438: loss of 0.941817581653595; compile_metrics of 100.0%\n",
      "Score for fold 439: loss of 1.1316767930984497; compile_metrics of 100.0%\n",
      "Score for fold 440: loss of 1.1677120923995972; compile_metrics of 100.0%\n",
      "Score for fold 441: loss of 1.1524488925933838; compile_metrics of 100.0%\n",
      "Score for fold 442: loss of 0.8736048936843872; compile_metrics of 100.0%\n",
      "Score for fold 443: loss of 7.754158020019531; compile_metrics of 0.0%\n",
      "Score for fold 444: loss of 1.1109637022018433; compile_metrics of 100.0%\n",
      "Score for fold 445: loss of 0.9722211956977844; compile_metrics of 100.0%\n",
      "Score for fold 446: loss of 6.2608642578125; compile_metrics of 0.0%\n",
      "Score for fold 447: loss of 3.9154727458953857; compile_metrics of 0.0%\n",
      "Score for fold 448: loss of 5.73952579498291; compile_metrics of 0.0%\n",
      "Score for fold 449: loss of 0.8522388935089111; compile_metrics of 100.0%\n",
      "Score for fold 450: loss of 1.1021989583969116; compile_metrics of 100.0%\n",
      "Score for fold 451: loss of 5.916237831115723; compile_metrics of 0.0%\n",
      "Score for fold 452: loss of 0.8631327748298645; compile_metrics of 100.0%\n",
      "Score for fold 453: loss of 4.682063579559326; compile_metrics of 0.0%\n",
      "Score for fold 454: loss of 1.2076034545898438; compile_metrics of 100.0%\n",
      "Score for fold 455: loss of 5.630715370178223; compile_metrics of 0.0%\n",
      "Score for fold 456: loss of 0.862305760383606; compile_metrics of 100.0%\n",
      "Score for fold 457: loss of 2.664862871170044; compile_metrics of 0.0%\n",
      "Score for fold 458: loss of 1.06349778175354; compile_metrics of 100.0%\n",
      "Score for fold 459: loss of 0.9886334538459778; compile_metrics of 100.0%\n",
      "Score for fold 460: loss of 2.3784704208374023; compile_metrics of 0.0%\n",
      "Score for fold 461: loss of 0.8519549369812012; compile_metrics of 100.0%\n",
      "Score for fold 462: loss of 0.9534243941307068; compile_metrics of 100.0%\n",
      "Score for fold 463: loss of 0.9181550145149231; compile_metrics of 100.0%\n",
      "Score for fold 464: loss of 1.68165123462677; compile_metrics of 100.0%\n",
      "Score for fold 465: loss of 0.9628011584281921; compile_metrics of 100.0%\n",
      "Score for fold 466: loss of 1.0179420709609985; compile_metrics of 100.0%\n",
      "Score for fold 467: loss of 1.8748878240585327; compile_metrics of 100.0%\n",
      "Score for fold 468: loss of 5.245460033416748; compile_metrics of 0.0%\n",
      "Score for fold 469: loss of 3.905306100845337; compile_metrics of 0.0%\n",
      "Score for fold 470: loss of 1.128383755683899; compile_metrics of 100.0%\n",
      "Score for fold 471: loss of 1.9305542707443237; compile_metrics of 100.0%\n",
      "Score for fold 472: loss of 3.250541925430298; compile_metrics of 0.0%\n",
      "Score for fold 473: loss of 1.7031760215759277; compile_metrics of 100.0%\n",
      "Score for fold 474: loss of 2.6337392330169678; compile_metrics of 0.0%\n",
      "Score for fold 475: loss of 0.9416050910949707; compile_metrics of 100.0%\n",
      "Score for fold 476: loss of 5.787972927093506; compile_metrics of 0.0%\n",
      "Score for fold 477: loss of 0.895718514919281; compile_metrics of 100.0%\n",
      "Score for fold 478: loss of 0.9654291868209839; compile_metrics of 100.0%\n",
      "Score for fold 479: loss of 1.1258184909820557; compile_metrics of 100.0%\n",
      "Score for fold 480: loss of 3.0998830795288086; compile_metrics of 0.0%\n",
      "Score for fold 481: loss of 0.8942877054214478; compile_metrics of 100.0%\n",
      "Score for fold 482: loss of 1.1342461109161377; compile_metrics of 100.0%\n",
      "Score for fold 483: loss of 0.8484440445899963; compile_metrics of 100.0%\n",
      "Score for fold 484: loss of 0.9966463446617126; compile_metrics of 100.0%\n",
      "Score for fold 485: loss of 0.934831440448761; compile_metrics of 100.0%\n",
      "Score for fold 486: loss of 0.8481729030609131; compile_metrics of 100.0%\n",
      "Score for fold 487: loss of 1.399198293685913; compile_metrics of 100.0%\n",
      "Score for fold 488: loss of 5.648273468017578; compile_metrics of 0.0%\n",
      "Score for fold 489: loss of 0.8738768100738525; compile_metrics of 100.0%\n",
      "Score for fold 490: loss of 0.8583182096481323; compile_metrics of 100.0%\n",
      "Score for fold 491: loss of 0.8855317831039429; compile_metrics of 100.0%\n",
      "Score for fold 492: loss of 1.0291144847869873; compile_metrics of 100.0%\n",
      "Score for fold 493: loss of 1.934589147567749; compile_metrics of 0.0%\n",
      "Score for fold 494: loss of 0.8921712636947632; compile_metrics of 100.0%\n",
      "Score for fold 495: loss of 0.8602950572967529; compile_metrics of 100.0%\n",
      "Score for fold 496: loss of 1.2888462543487549; compile_metrics of 100.0%\n",
      "Score for fold 497: loss of 1.2910066843032837; compile_metrics of 100.0%\n",
      "Score for fold 498: loss of 0.8522692322731018; compile_metrics of 100.0%\n",
      "Score for fold 499: loss of 3.328951835632324; compile_metrics of 0.0%\n",
      "Score for fold 500: loss of 1.020725131034851; compile_metrics of 100.0%\n",
      "Score for fold 501: loss of 1.1115247011184692; compile_metrics of 100.0%\n",
      "Score for fold 502: loss of 2.921600341796875; compile_metrics of 0.0%\n",
      "Score for fold 503: loss of 1.046860933303833; compile_metrics of 100.0%\n",
      "Score for fold 504: loss of 3.3162894248962402; compile_metrics of 0.0%\n",
      "Score for fold 505: loss of 3.6917219161987305; compile_metrics of 0.0%\n",
      "Score for fold 506: loss of 5.6500020027160645; compile_metrics of 0.0%\n",
      "Score for fold 507: loss of 0.9393284916877747; compile_metrics of 100.0%\n",
      "Score for fold 508: loss of 4.090847015380859; compile_metrics of 0.0%\n",
      "Score for fold 509: loss of 4.45186710357666; compile_metrics of 0.0%\n",
      "Score for fold 510: loss of 6.551788330078125; compile_metrics of 0.0%\n",
      "Score for fold 511: loss of 2.8068902492523193; compile_metrics of 0.0%\n",
      "Score for fold 512: loss of 3.134007215499878; compile_metrics of 0.0%\n",
      "Score for fold 513: loss of 0.9536581635475159; compile_metrics of 100.0%\n",
      "Score for fold 514: loss of 1.8619239330291748; compile_metrics of 100.0%\n",
      "Score for fold 515: loss of 3.315091848373413; compile_metrics of 0.0%\n",
      "Score for fold 516: loss of 1.056910514831543; compile_metrics of 100.0%\n",
      "Score for fold 517: loss of 1.0535497665405273; compile_metrics of 100.0%\n",
      "Score for fold 518: loss of 3.3491766452789307; compile_metrics of 0.0%\n",
      "Score for fold 519: loss of 1.0750502347946167; compile_metrics of 100.0%\n",
      "Score for fold 520: loss of 0.8711564540863037; compile_metrics of 100.0%\n",
      "Score for fold 521: loss of 0.9117313027381897; compile_metrics of 100.0%\n",
      "Score for fold 522: loss of 0.8851953148841858; compile_metrics of 100.0%\n",
      "Score for fold 523: loss of 0.9197556972503662; compile_metrics of 100.0%\n",
      "Score for fold 524: loss of 1.038914680480957; compile_metrics of 100.0%\n",
      "Score for fold 525: loss of 0.8827887773513794; compile_metrics of 100.0%\n",
      "Score for fold 526: loss of 3.8366634845733643; compile_metrics of 0.0%\n",
      "Score for fold 527: loss of 0.9066535234451294; compile_metrics of 100.0%\n",
      "Score for fold 528: loss of 1.0293089151382446; compile_metrics of 100.0%\n",
      "Score for fold 529: loss of 2.4019601345062256; compile_metrics of 0.0%\n",
      "Score for fold 530: loss of 1.3652039766311646; compile_metrics of 100.0%\n",
      "Score for fold 531: loss of 4.022776126861572; compile_metrics of 0.0%\n",
      "Score for fold 532: loss of 1.00943922996521; compile_metrics of 100.0%\n",
      "Score for fold 533: loss of 1.4633793830871582; compile_metrics of 100.0%\n",
      "Score for fold 534: loss of 3.6590521335601807; compile_metrics of 0.0%\n",
      "Score for fold 535: loss of 0.9532613754272461; compile_metrics of 100.0%\n",
      "Score for fold 536: loss of 1.1464579105377197; compile_metrics of 100.0%\n",
      "Score for fold 537: loss of 3.8279504776000977; compile_metrics of 0.0%\n",
      "Score for fold 538: loss of 5.602502346038818; compile_metrics of 0.0%\n",
      "Score for fold 539: loss of 1.229465126991272; compile_metrics of 100.0%\n",
      "Score for fold 540: loss of 7.015792369842529; compile_metrics of 0.0%\n",
      "Score for fold 541: loss of 0.9629337787628174; compile_metrics of 100.0%\n",
      "Score for fold 542: loss of 0.9987808465957642; compile_metrics of 100.0%\n",
      "Score for fold 543: loss of 1.1227718591690063; compile_metrics of 100.0%\n",
      "Score for fold 544: loss of 1.1582074165344238; compile_metrics of 100.0%\n",
      "Score for fold 545: loss of 4.625802040100098; compile_metrics of 0.0%\n",
      "Score for fold 546: loss of 0.9863466024398804; compile_metrics of 100.0%\n",
      "Average accuracy over all folds: 64.46886446886447\n",
      "Average loss over all folds: 2.2746849730337932\n",
      "Problematic samples (indices with loss > 3.0): [2, 3, 11, 13, 15, 17, 18, 22, 32, 33, 45, 48, 54, 63, 68, 80, 81, 85, 90, 91, 97, 118, 120, 123, 124, 137, 142, 148, 150, 151, 158, 159, 162, 166, 167, 171, 176, 178, 181, 186, 190, 193, 195, 197, 199, 200, 201, 210, 213, 216, 217, 219, 220, 222, 231, 232, 235, 237, 239, 244, 247, 250, 253, 255, 256, 271, 285, 290, 293, 298, 301, 305, 306, 307, 312, 318, 319, 320, 321, 322, 323, 325, 327, 331, 334, 335, 336, 337, 338, 352, 372, 374, 375, 376, 379, 382, 384, 388, 392, 403, 404, 405, 406, 407, 408, 409, 411, 413, 415, 417, 418, 420, 423, 425, 430, 431, 432, 435, 442, 445, 446, 447, 450, 452, 454, 467, 468, 471, 475, 479, 487, 498, 503, 504, 505, 507, 508, 509, 511, 514, 517, 525, 530, 533, 536, 537, 539, 544]\n",
      "Number of samples after removing problematic samples: 398\n"
     ]
    }
   ],
   "source": [
    "#LEave one out cross validation\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import collections\n",
    "\n",
    "# Assuming X and y are already defined and X contains grayscale images\n",
    "X = X.reshape(-1, 20, 20, 1)  # Reshape to include channel dimension\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "y_categorical = to_categorical(y_encoded, num_classes=len(label_encoder.classes_))\n",
    "\n",
    "# Define leave-one-out cross-validation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Initialize variables to store results\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "fold_no = 1\n",
    "\n",
    "problematic_samples = []\n",
    "high_loss_threshold = 3.0  # Define a threshold for high loss\n",
    "\n",
    "for train, test in loo.split(X, y_encoded):\n",
    "    model = Sequential([\n",
    "        Input(shape=(grid_size[0], grid_size[1], 1)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(np.unique(y)), activation='softmax', kernel_regularizer=l2(0.001))\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_counts = collections.Counter(y_encoded[train])\n",
    "    total_samples = sum(class_counts.values())\n",
    "    class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "\n",
    "    # Callbacks\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=30, min_lr=1e-6)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X[train], to_categorical(y_encoded[train], num_classes=len(label_encoder.classes_)),\n",
    "                        class_weight=class_weights,\n",
    "                        epochs=300,\n",
    "                        validation_data=(X[test], to_categorical(y_encoded[test], num_classes=len(label_encoder.classes_))),\n",
    "                        callbacks=[reduce_lr, early_stopping], verbose = 0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    scores = model.evaluate(X[test], to_categorical(y_encoded[test], num_classes=len(label_encoder.classes_)), verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    # Store problematic samples\n",
    "    if scores[0] > high_loss_threshold:\n",
    "        problematic_samples.append(test[0])\n",
    "\n",
    "    fold_no += 1\n",
    "\n",
    "# Print the average accuracy and loss over all folds\n",
    "print('Average accuracy over all folds:', np.mean(acc_per_fold))\n",
    "print('Average loss over all folds:', np.mean(loss_per_fold))\n",
    "\n",
    "# Print and remove problematic samples\n",
    "print(f'Problematic samples (indices with loss > {high_loss_threshold}):', problematic_samples)\n",
    "\n",
    "# Remove problematic samples from the dataset\n",
    "X_filtered = np.delete(X, problematic_samples, axis=0)\n",
    "y_filtered = np.delete(y_encoded, problematic_samples, axis=0)\n",
    "\n",
    "print('Number of samples after removing problematic samples:', X_filtered.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e8ab8837-de73-4fef-8fb1-0022b43dc212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.5273 with parameters: {}\n"
     ]
    }
   ],
   "source": [
    "#SVM obican\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "import collections\n",
    "\n",
    "\n",
    "trainings = 50\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "\n",
    "for i in range(trainings):\n",
    "    X = X.reshape(-1, 20, 20, 1)\n",
    "    X_flatten = X.reshape(X.shape[0], -1)  # Flatten the images\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    label_names = label_encoder.classes_\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_flatten, y_encoded, test_size=0.2, stratify=y_encoded)\n",
    "    \n",
    "    # Apply SMOTE to balance the dataset\n",
    "    smote = SMOTE()\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Train SVM\n",
    "    svm_model = SVC(kernel='rbf', probability=True)\n",
    "    svm_model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    if (accuracy > best_accuracy):\n",
    "        best_accuracy = accuracy\n",
    "    # Print classification report\n",
    "    #print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Best accuracy: {best_accuracy:.4f} with parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6402c2e-d7fa-426c-b86b-71b7893e2969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 145ms/step - loss: 1382.1094 - val_loss: 1415.4240\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1365.4756 - val_loss: 1412.9691\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1351.6201 - val_loss: 1410.8870\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1357.7664 - val_loss: 1409.2706\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1361.1189 - val_loss: 1407.9911\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1349.0718 - val_loss: 1407.1056\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1365.2745 - val_loss: 1406.5431\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1335.7625 - val_loss: 1405.9238\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1360.2411 - val_loss: 1405.6412\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1356.0171 - val_loss: 1405.5718\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1355.4091 - val_loss: 1405.4985\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1344.3927 - val_loss: 1405.4791\n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1355.7386 - val_loss: 1405.4723\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1328.3209 - val_loss: 1405.4607\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1344.8757 - val_loss: 1405.4459\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1339.2799 - val_loss: 1405.4438\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1346.6959 - val_loss: 1405.4468\n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1332.9758 - val_loss: 1405.4510\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1345.7737 - val_loss: 1405.4498\n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1346.2715 - val_loss: 1405.4364\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1357.4735 - val_loss: 1405.4229\n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1351.1556 - val_loss: 1405.4172\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1347.5209 - val_loss: 1405.4126\n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1344.6909 - val_loss: 1405.4064\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1344.7289 - val_loss: 1405.4054\n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1345.5754 - val_loss: 1405.4061\n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1364.6396 - val_loss: 1405.4098\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1352.5116 - val_loss: 1405.4126\n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1362.3574 - val_loss: 1405.4087\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1353.4993 - val_loss: 1405.4034\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1346.4097 - val_loss: 1405.3977\n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1341.3287 - val_loss: 1405.3949\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1356.4432 - val_loss: 1405.3944\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1348.5873 - val_loss: 1405.3936\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1338.1954 - val_loss: 1405.3921\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1344.8564 - val_loss: 1405.3916\n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1347.5729 - val_loss: 1405.3894\n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1360.1628 - val_loss: 1405.3896\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1349.0562 - val_loss: 1405.3877\n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1348.5916 - val_loss: 1405.3835\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1351.7474 - val_loss: 1405.3813\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1359.0952 - val_loss: 1405.3811\n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1353.5076 - val_loss: 1405.3816\n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1333.3497 - val_loss: 1405.3802\n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1347.3877 - val_loss: 1405.3754\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1341.8569 - val_loss: 1405.3741\n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1343.2716 - val_loss: 1405.3751\n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1332.1771 - val_loss: 1405.3773\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1339.2397 - val_loss: 1405.3751\n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1346.9504 - val_loss: 1405.3715\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.0338 - loss: 5.0458 - val_accuracy: 0.1000 - val_loss: 8.1676 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1275 - loss: 4.1216 - val_accuracy: 0.0364 - val_loss: 7.1914 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1940 - loss: 3.5888 - val_accuracy: 0.0636 - val_loss: 5.6641 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2182 - loss: 3.3319 - val_accuracy: 0.1455 - val_loss: 5.0593 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2738 - loss: 3.1006 - val_accuracy: 0.1545 - val_loss: 4.6660 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2891 - loss: 3.0240 - val_accuracy: 0.1636 - val_loss: 4.3390 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2980 - loss: 2.8668 - val_accuracy: 0.1909 - val_loss: 4.1821 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3423 - loss: 2.8316 - val_accuracy: 0.1818 - val_loss: 4.0808 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3308 - loss: 2.6819 - val_accuracy: 0.2273 - val_loss: 3.8857 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3089 - loss: 2.7845 - val_accuracy: 0.1909 - val_loss: 3.7402 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3294 - loss: 2.6962 - val_accuracy: 0.2273 - val_loss: 3.7203 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3847 - loss: 2.5771 - val_accuracy: 0.2182 - val_loss: 3.6988 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3566 - loss: 2.5921 - val_accuracy: 0.2455 - val_loss: 3.6588 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3654 - loss: 2.5525 - val_accuracy: 0.2455 - val_loss: 3.6809 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3500 - loss: 2.5446 - val_accuracy: 0.2545 - val_loss: 3.6167 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4652 - loss: 2.2288 - val_accuracy: 0.2727 - val_loss: 3.5563 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4069 - loss: 2.3827 - val_accuracy: 0.2727 - val_loss: 3.5212 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4677 - loss: 2.1420 - val_accuracy: 0.2545 - val_loss: 3.5098 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4329 - loss: 2.3426 - val_accuracy: 0.2727 - val_loss: 3.5134 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4311 - loss: 2.3451 - val_accuracy: 0.3000 - val_loss: 3.4537 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4313 - loss: 2.2343 - val_accuracy: 0.3273 - val_loss: 3.3870 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5018 - loss: 2.0484 - val_accuracy: 0.3182 - val_loss: 3.4295 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4252 - loss: 2.2648 - val_accuracy: 0.3273 - val_loss: 3.4154 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4673 - loss: 2.0867 - val_accuracy: 0.2818 - val_loss: 3.4777 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4923 - loss: 2.0668 - val_accuracy: 0.2909 - val_loss: 3.4774 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5091 - loss: 1.9917 - val_accuracy: 0.3000 - val_loss: 3.5257 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4990 - loss: 2.1137 - val_accuracy: 0.3182 - val_loss: 3.5084 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5364 - loss: 1.8983 - val_accuracy: 0.2818 - val_loss: 3.3915 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5096 - loss: 2.0261 - val_accuracy: 0.3091 - val_loss: 3.2724 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5482 - loss: 1.8341 - val_accuracy: 0.3636 - val_loss: 3.2901 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5037 - loss: 2.0073 - val_accuracy: 0.3545 - val_loss: 3.3838 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5201 - loss: 1.8701 - val_accuracy: 0.3455 - val_loss: 3.3592 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5120 - loss: 1.9237 - val_accuracy: 0.3364 - val_loss: 3.3361 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5345 - loss: 1.8449 - val_accuracy: 0.3273 - val_loss: 3.4447 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5304 - loss: 1.9464 - val_accuracy: 0.3455 - val_loss: 3.5123 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5398 - loss: 1.8403 - val_accuracy: 0.3364 - val_loss: 3.5544 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5256 - loss: 1.7934 - val_accuracy: 0.3273 - val_loss: 3.5179 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5793 - loss: 1.7752 - val_accuracy: 0.3182 - val_loss: 3.5611 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5961 - loss: 1.7316 - val_accuracy: 0.3273 - val_loss: 3.5963 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5682 - loss: 1.7613 - val_accuracy: 0.3000 - val_loss: 3.5679 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5997 - loss: 1.6942 - val_accuracy: 0.3182 - val_loss: 3.5574 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5718 - loss: 1.7378 - val_accuracy: 0.3091 - val_loss: 3.5094 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5666 - loss: 1.6902 - val_accuracy: 0.3091 - val_loss: 3.5686 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5624 - loss: 1.6851 - val_accuracy: 0.3091 - val_loss: 3.5637 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6041 - loss: 1.6166 - val_accuracy: 0.3091 - val_loss: 3.5580 - learning_rate: 0.0010\n",
      "Epoch 46/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5953 - loss: 1.6241 - val_accuracy: 0.3273 - val_loss: 3.5403 - learning_rate: 0.0010\n",
      "Epoch 47/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5985 - loss: 1.6587 - val_accuracy: 0.3364 - val_loss: 3.5361 - learning_rate: 0.0010\n",
      "Epoch 48/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6030 - loss: 1.5938 - val_accuracy: 0.3182 - val_loss: 3.5241 - learning_rate: 0.0010\n",
      "Epoch 49/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6026 - loss: 1.6070 - val_accuracy: 0.3182 - val_loss: 3.5677 - learning_rate: 0.0010\n",
      "Epoch 50/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5783 - loss: 1.6671 - val_accuracy: 0.3364 - val_loss: 3.6028 - learning_rate: 0.0010\n",
      "Epoch 51/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5839 - loss: 1.6484 - val_accuracy: 0.3273 - val_loss: 3.5829 - learning_rate: 0.0010\n",
      "Epoch 52/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6386 - loss: 1.5601 - val_accuracy: 0.3273 - val_loss: 3.5326 - learning_rate: 0.0010\n",
      "Epoch 53/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5940 - loss: 1.6049 - val_accuracy: 0.3455 - val_loss: 3.5779 - learning_rate: 0.0010\n",
      "Epoch 54/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6461 - loss: 1.4751 - val_accuracy: 0.3273 - val_loss: 3.6081 - learning_rate: 0.0010\n",
      "Epoch 55/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6305 - loss: 1.5337 - val_accuracy: 0.3182 - val_loss: 3.5637 - learning_rate: 0.0010\n",
      "Epoch 56/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6044 - loss: 1.5902 - val_accuracy: 0.3182 - val_loss: 3.6559 - learning_rate: 0.0010\n",
      "Epoch 57/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6361 - loss: 1.5661 - val_accuracy: 0.3455 - val_loss: 3.6417 - learning_rate: 0.0010\n",
      "Epoch 58/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6225 - loss: 1.5539 - val_accuracy: 0.3273 - val_loss: 3.6457 - learning_rate: 0.0010\n",
      "Epoch 59/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6348 - loss: 1.4607 - val_accuracy: 0.3091 - val_loss: 3.7500 - learning_rate: 0.0010\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Accuracy: 0.3091\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "targetImages\\sun_aesdwymqhhiewhyo.jpg       0.25      0.40      0.31         5\n",
      "targetImages\\sun_aewtvpwulyxhqvuv.jpg       0.00      0.00      0.00         4\n",
      "targetImages\\sun_agenkchqfjxvsppn.jpg       0.00      0.00      0.00         4\n",
      "targetImages\\sun_agoqtmhicqrakxqs.jpg       0.14      0.33      0.20         3\n",
      "targetImages\\sun_ahbwwdffoiigyxqx.jpg       0.50      0.40      0.44         5\n",
      "targetImages\\sun_ailjxpgyepocjdos.jpg       0.75      0.75      0.75         4\n",
      "targetImages\\sun_aiyjrnbytpirechl.jpg       0.43      0.75      0.55         4\n",
      "targetImages\\sun_amkeeuvicewppjqo.jpg       0.00      0.00      0.00         4\n",
      "targetImages\\sun_amssqaxrfmufhwxv.jpg       0.33      0.67      0.44         3\n",
      "targetImages\\sun_aplqnvinpntdexdw.jpg       0.00      0.00      0.00         3\n",
      "targetImages\\sun_apytdxunawwxcvdw.jpg       0.33      0.50      0.40         2\n",
      "targetImages\\sun_arsolpsoaipmhdio.jpg       0.00      0.00      0.00         4\n",
      "targetImages\\sun_ayuqdxlbdhbbgypd.jpg       0.20      0.33      0.25         3\n",
      "targetImages\\sun_aywhxcvpafxkujrj.jpg       0.00      0.00      0.00         4\n",
      "targetImages\\sun_bacfwopdcsewxkne.jpg       0.80      0.80      0.80         5\n",
      "targetImages\\sun_bgshkoltnrmilxns.jpg       0.33      0.25      0.29         4\n",
      "targetImages\\sun_bisnnysqxgpavrkh.jpg       1.00      1.00      1.00         4\n",
      "targetImages\\sun_bkxzmjorpemztasj.jpg       0.50      0.67      0.57         3\n",
      "targetImages\\sun_bpkiszwjhbnsrmtr.jpg       0.00      0.00      0.00         4\n",
      "targetImages\\sun_btjevcvrxjpeyoxz.jpg       0.33      0.25      0.29         4\n",
      "targetImages\\sun_bukzvwbkvedjexbe.jpg       0.12      0.25      0.17         4\n",
      "targetImages\\sun_burokageiugamrul.jpg       0.20      0.33      0.25         3\n",
      "targetImages\\sun_buubpayscjcxyypt.jpg       0.00      0.00      0.00         3\n",
      "targetImages\\sun_bwidvgzwofvfteaj.jpg       0.00      0.00      0.00         4\n",
      "targetImages\\sun_bxxtkmyydusifqkv.jpg       0.00      0.00      0.00         3\n",
      "targetImages\\sun_byuhjpweygtwpwbj.jpg       0.00      0.00      0.00         3\n",
      "targetImages\\sun_byziwbrwhhpxaydu.jpg       0.25      0.25      0.25         4\n",
      "targetImages\\sun_bzfcixnmhtzpmwij.jpg       0.40      0.50      0.44         4\n",
      "targetImages\\sun_bzjdnotizomfsdad.jpg       0.40      0.67      0.50         3\n",
      "targetImages\\sun_cesisdcqpsijbubk.jpg       0.00      0.00      0.00         3\n",
      "\n",
      "                             accuracy                           0.31       110\n",
      "                            macro avg       0.24      0.30      0.26       110\n",
      "                         weighted avg       0.26      0.31      0.27       110\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valerijan\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\valerijan\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\valerijan\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Encoder code copied\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import collections\n",
    "\n",
    "# Assuming X and y are already defined and X contains grayscale images\n",
    "X = X.reshape(-1, 20, 20, 1)  # Reshape to include channel dimension\n",
    "\n",
    "# Define the autoencoder\n",
    "input_img = Input(shape=(20, 20, 1))\n",
    "x = Flatten()(input_img)\n",
    "encoded = Dense(128, activation='relu')(x)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(20 * 20, activation='sigmoid')(decoded)\n",
    "decoded = Reshape((20, 20, 1))(decoded)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X, X, epochs=50, batch_size=256, shuffle=True, validation_split=0.2)\n",
    "\n",
    "# Extract the encoder\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# Encode the data\n",
    "X_encoded = encoder.predict(X)\n",
    "\n",
    "# Flatten the encoded data for the classifier\n",
    "X_encoded_flatten = X_encoded.reshape(X_encoded.shape[0], -1)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded_flatten, y_encoded, test_size=0.2, stratify=y_encoded)\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the MLP model for classification\n",
    "mlp_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train_resampled.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the MLP model\n",
    "mlp_model.compile(optimizer=Adam(learning_rate=1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks for learning rate adjustment and early stopping\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=30, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "# Train the MLP model\n",
    "history = mlp_model.fit(X_train_resampled, y_train_resampled,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=1000,\n",
    "                        callbacks=[reduce_lr, early_stopping],\n",
    "                        batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_prob = mlp_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef6cb6-66a6-4a1c-9a55-ffda7f89b7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
