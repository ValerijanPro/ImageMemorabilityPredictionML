{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9e0ac92-9d90-4356-b35d-2ebf6977985b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newest best accuracy:  0.42857142857142855\n",
      "Params:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest top3 best accuracy:  0.60317462682724\n",
      "Params top3:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest f1 best accuracy:  0.3903174603174603\n",
      "Params f1:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest per class best accuracy:  0.41111111111111115\n",
      "Params per class:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.41111111111111115\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  1\n",
      "Newest best accuracy:  0.5396825396825397\n",
      "Params:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest top3 best accuracy:  0.7142857313156128\n",
      "Params top3:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest f1 best accuracy:  0.47968253968253965\n",
      "Params f1:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest per class best accuracy:  0.5166666666666667\n",
      "Params per class:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.5166666666666667\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  2\n",
      "Newest f1 best accuracy:  0.5169047619047619\n",
      "Params f1:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest per class best accuracy:  0.5388888888888889\n",
      "Params per class:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.5388888888888889\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  3\n",
      "Newest best accuracy:  0.5714285714285714\n",
      "Params:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest top3 best accuracy:  0.761904776096344\n",
      "Params top3:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest f1 best accuracy:  0.5346825396825395\n",
      "Params f1:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest per class best accuracy:  0.5611111111111111\n",
      "Params per class:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.5611111111111111\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  4\n",
      "Current:  5\n",
      "Newest best accuracy:  0.6031746031746031\n",
      "Params:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest f1 best accuracy:  0.5566666666666666\n",
      "Params f1:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest per class best accuracy:  0.5777777777777777\n",
      "Params per class:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.5777777777777777\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  6\n",
      "Current:  7\n",
      "Current:  8\n",
      "Current:  9\n",
      "Current:  10\n",
      "Current:  11\n",
      "Current:  12\n",
      "Newest f1 best accuracy:  0.5647619047619048\n",
      "Params f1:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest per class best accuracy:  0.5944444444444444\n",
      "Params per class:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.5944444444444444\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  13\n",
      "Current:  14\n",
      "Current:  15\n",
      "Current:  16\n",
      "Newest top3 best accuracy:  0.7777777910232544\n",
      "Params top3:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  17\n",
      "Current:  18\n",
      "Current:  19\n",
      "Current:  20\n",
      "Newest top3 best accuracy:  0.8253968358039856\n",
      "Params top3:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  21\n",
      "Current:  22\n",
      "Current:  23\n",
      "Current:  24\n",
      "Current:  25\n",
      "Current:  26\n",
      "Current:  27\n",
      "Current:  28\n",
      "Current:  29\n",
      "Current:  30\n",
      "Current:  31\n",
      "Current:  32\n",
      "Current:  33\n",
      "Current:  34\n",
      "Current:  35\n",
      "Current:  36\n",
      "Current:  37\n",
      "Current:  38\n",
      "Current:  39\n",
      "Current:  40\n",
      "Current:  41\n",
      "Current:  42\n",
      "Current:  43\n",
      "Current:  44\n",
      "Current:  45\n",
      "Current:  46\n",
      "Current:  47\n",
      "Current:  48\n",
      "Current:  49\n",
      "Current:  50\n",
      "Newest best accuracy:  0.6507936507936508\n",
      "Params:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest f1 best accuracy:  0.6072222222222223\n",
      "Params f1:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest per class best accuracy:  0.65\n",
      "Params per class:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.65\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  51\n",
      "Current:  52\n",
      "Current:  53\n",
      "Current:  54\n",
      "Current:  55\n",
      "Current:  56\n",
      "Current:  57\n",
      "Current:  58\n",
      "Current:  59\n",
      "Current:  60\n",
      "Current:  61\n",
      "Current:  62\n",
      "Newest f1 best accuracy:  0.61\n",
      "Params f1:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  63\n",
      "Current:  64\n",
      "Current:  65\n",
      "Current:  66\n",
      "Current:  67\n",
      "Current:  68\n",
      "Current:  69\n",
      "Current:  70\n",
      "Current:  71\n",
      "Current:  72\n",
      "Current:  73\n",
      "Current:  74\n",
      "Current:  75\n",
      "Current:  76\n",
      "Current:  77\n",
      "Current:  78\n",
      "Current:  79\n",
      "Current:  80\n",
      "Current:  81\n",
      "Current:  82\n",
      "Current:  83\n",
      "Current:  84\n",
      "Current:  85\n",
      "Current:  86\n",
      "Current:  87\n",
      "Current:  88\n",
      "Current:  89\n",
      "Current:  90\n",
      "Current:  91\n",
      "Current:  92\n",
      "Current:  93\n",
      "Current:  94\n",
      "Current:  95\n",
      "Current:  96\n",
      "Current:  97\n",
      "Current:  98\n",
      "Current:  99\n",
      "Current:  100\n",
      "Current:  101\n",
      "Current:  102\n",
      "Current:  103\n",
      "Current:  104\n",
      "Current:  105\n",
      "Current:  106\n",
      "Current:  107\n",
      "Current:  108\n",
      "Current:  109\n",
      "Current:  110\n",
      "Current:  111\n",
      "Current:  112\n",
      "Current:  113\n",
      "Current:  114\n",
      "Current:  115\n",
      "Current:  116\n",
      "Current:  117\n",
      "Current:  118\n",
      "Current:  119\n",
      "Current:  120\n",
      "Current:  121\n",
      "Current:  122\n",
      "Current:  123\n",
      "Current:  124\n",
      "Current:  125\n",
      "Current:  126\n",
      "Current:  127\n",
      "Current:  128\n",
      "Current:  129\n",
      "Current:  130\n",
      "Current:  131\n",
      "Current:  132\n",
      "Current:  133\n",
      "Current:  134\n",
      "Current:  135\n",
      "Current:  136\n",
      "Current:  137\n",
      "Current:  138\n",
      "Current:  139\n",
      "Current:  140\n",
      "Current:  141\n",
      "Current:  142\n",
      "Current:  143\n",
      "Current:  144\n",
      "Current:  145\n",
      "Current:  146\n",
      "Current:  147\n",
      "Current:  148\n",
      "Current:  149\n",
      "Current:  150\n",
      "FINAL Best accuracy:  0.6507936507936508\n",
      "FINAL Params:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import collections\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import math\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import csv\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, balanced_accuracy_score\n",
    "import gc\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout\n",
    "\n",
    "sigmas = [1]\n",
    "#grid_sizes = [(20, 20), (30, 30), (40, 40),(50, 50), (60, 60)]\n",
    "grid_sizes = [(30, 30)]\n",
    "#gaussian_sizes = [(1, 1), (19, 19), (35, 35), (43, 43)]\n",
    "gaussian_sizes = [(43, 43)]\n",
    "#batch_sizes = [32, 48, 128, 176]\n",
    "batch_sizes = [128]\n",
    "dropouts = [0.5]\n",
    "#reg_terms = [0.001, 0.0001]\n",
    "reg_terms = [0.0001]\n",
    "learning_rates = [1e-3]\n",
    "patiences = [30]\n",
    "min_lrs = [1e-6]\n",
    "factors = [0.2]\n",
    "#test_sizes = [0.1, 0.15, 0.2]\n",
    "test_sizes = [0.1]\n",
    "results_folder = \"MODELS COMPARISON\"\n",
    "\n",
    "options =\n",
    "\n",
    "\n",
    "# Create the folder if it does not exist\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_top3_accuracy = 0\n",
    "best_f1_accuracy = 0\n",
    "best_per_class_accuracy = 0\n",
    "best_balanced_accuracy = 0\n",
    "best_params = {}\n",
    "best_params_top3 = {}\n",
    "best_params_f1 = {}\n",
    "best_params_per_class = {}\n",
    "best_params_balanced_accuracy = {}\n",
    "                                            \n",
    "distance = 610\n",
    "h_res = 1920\n",
    "v_res = 1080\n",
    "screen_w = 527\n",
    "screen_h = 296\n",
    "\n",
    "current = 0\n",
    "\n",
    "                                            \n",
    "                                            \n",
    "\n",
    "def compute_ppda(distance, h_res, v_res, screen_w, screen_h):\n",
    "    \"\"\"\n",
    "    Compute the number of pixels per degree of visual angle based on the experimental conditions.\n",
    "    \n",
    "    :param distance: int, the distance between the observer and the screen (in mm)\n",
    "    :param h_res: int, the horizontal resolution of the screen\n",
    "    :param v_res: int, the vertical resolution of the screen\n",
    "    :param screen_w: int, the width of the screen (in mm)\n",
    "    :param screen_h: int, the height of the screen (in mm)\n",
    "    :return horizontal_ppda: float, the number of pixel per degree of visual angle\n",
    "    \"\"\"\n",
    "    pxl_density_x = h_res / screen_w\n",
    "    pxl_density_y = v_res / screen_h\n",
    "    \n",
    "    d = 2 * distance * math.tan(np.deg2rad(0.5))\n",
    "    horizontal_ppda = d * ((pxl_density_x + pxl_density_y) / 2)\n",
    "    \n",
    "    return horizontal_ppda\n",
    "                                            \n",
    "ppda = compute_ppda(distance, h_res, v_res, screen_w, screen_h)\n",
    "\n",
    "for test_size in test_sizes:\n",
    "    for sigma in sigmas:\n",
    "        for factor in factors:\n",
    "            for gaussian_size in gaussian_sizes:\n",
    "                for batch_size in batch_sizes:\n",
    "                    for dropout in dropouts:\n",
    "                        for reg_term in reg_terms:\n",
    "                            for lr in learning_rates:\n",
    "                                for patience in patiences:\n",
    "                                    for min_lr in min_lrs:\n",
    "                                        for grid_size in grid_sizes:\n",
    "\n",
    "                                            def checkObserverRemembered(observer, image_path, base_dir):\n",
    "                                                csv_file_path = os.path.join(base_dir, \"..\" ,\"hit_status.csv\")\n",
    "                                                if not os.path.isfile(csv_file_path):\n",
    "                                                    print(\"Error: CSV file not found.\")\n",
    "                                                    return False\n",
    "                                                df = pd.read_csv(csv_file_path)\n",
    "                                                filtered_rows = df[(df['Setup Folder'] == observer) & (df['Image Path'] == image_path) & (df['Hit'] == 1)]\n",
    "                                                if not filtered_rows.empty:\n",
    "                                                    return True\n",
    "                                                else:\n",
    "                                                    return False\n",
    "\n",
    "                                            def bin_fixations(fixation_map):\n",
    "                                                global grid_size\n",
    "                                                height, width = fixation_map.shape\n",
    "                                                binned_map = np.zeros(grid_size)\n",
    "                                            \n",
    "                                                bin_height = height // grid_size[0]\n",
    "                                                bin_width = width // grid_size[1]\n",
    "                                            \n",
    "                                                for i in range(grid_size[0]):\n",
    "                                                    for j in range(grid_size[1]):\n",
    "                                                        bin_area = fixation_map[i*bin_height:(i+1)*bin_height, j*bin_width:(j+1)*bin_width]\n",
    "                                                        binned_map[i, j] = np.sum(bin_area)\n",
    "                                                        #ili avg?\n",
    "                                            \n",
    "                                                return binned_map\n",
    "                                            \n",
    "                                            def normalize_map(binned_map):\n",
    "                                                return binned_map / np.sum(binned_map)\n",
    "                                                #return binned_map\n",
    "                                            \n",
    "                                            def smooth_map(binned_map):\n",
    "                                                global sigma\n",
    "                                                return gaussian_filter(binned_map, sigma=sigma)\n",
    "                                            \n",
    "                                            def process_fixation_map(fixation_map):\n",
    "                                                binned_map = bin_fixations(fixation_map)\n",
    "                                                normalized_map = normalize_map(binned_map)\n",
    "                                                smoothed_map = smooth_map(normalized_map)\n",
    "                                                return smoothed_map\n",
    "                                            \n",
    "                                            def get_current_fixation_map(image_path, coordinates):\n",
    "                                                image = cv2.imread(image_path)\n",
    "                                                if image is None:\n",
    "                                                    print(f\"Image at {image_path} not found.\")\n",
    "                                                    return\n",
    "                                            \n",
    "                                                coordinates = coordinates[0:120]\n",
    "                                              \n",
    "                                                fixation_map = np.zeros((1080, 1920), dtype=np.float32)\n",
    "                                            \n",
    "                                                # Convert coordinates to pixel coordinates and update the saliency map\n",
    "                                                for x_norm, y_norm in coordinates:\n",
    "                                                    # Scale normalized coordinates to pixel coordinates for the 1920x1080 screen\n",
    "                                                    x = int(((x_norm + 1) / 2 ) * 1920)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "                                                    y = int((y_norm + 0.5) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "                                                    # Update the saliency map if coordinates are within the screen\n",
    "                                                    if 0 <= x < 1920 and 0 <= y < 1080:\n",
    "                                                        fixation_map[y, x] += 1 \n",
    "                                            \n",
    "                                            \n",
    "                                                    '''\n",
    "                                                    # Scale normalized coordinates to pixel coordinates for the 1920x1080 screen\n",
    "                                                    if(x_norm >0 and y_norm >0):\n",
    "                                                        x = int((x_norm + 1 + 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "                                                        y = int((y_norm + 0.5 + 0.05) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "                                                    if(x_norm >0 and y_norm <0):\n",
    "                                                        x = int((x_norm + 1 + 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "                                                        y = int((y_norm + 0.5 - 0.1) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "                                                    if(x_norm <0 and y_norm >0):\n",
    "                                                        x = int((x_norm + 1 - 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "                                                        y = int((y_norm + 0.5 + 0.05) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "                                                    if(x_norm <0 and y_norm <0):\n",
    "                                                        x = int((x_norm + 1 - 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "                                                        y = int((y_norm + 0.5 - 0.1) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "                                                    # Update the saliency map if coordinates are within the screen\n",
    "                                                    if 0 <= x < 1920 and 0 <= y < 1080:\n",
    "                                                        fixation_map[y, x] += 1'''\n",
    "                                                #sigma = ppda / np.sqrt(2)\n",
    "                                                #fixation_map = gaussian_filter(fixation_map, sigma = sigma)\n",
    "                                                fixation_map = cv2.GaussianBlur(fixation_map, gaussian_size, 0)\n",
    "                                                # Crop the saliency map to the 700x700 region\n",
    "                                                fixation_map = fixation_map[190:890, 610:1310]\n",
    "                                                # flip the Y coordinates\n",
    "                                                fixation_map = np.flipud(fixation_map)\n",
    "                                                return fixation_map\n",
    "                                            \n",
    "                                            def normalize_fixation_map(fixation_map):\n",
    "                                                min_val = np.min(fixation_map)\n",
    "                                                max_val = np.max(fixation_map)\n",
    "                                                normalized_fixation_map = (fixation_map - min_val) / (max_val - min_val) * 255\n",
    "                                                return normalized_fixation_map\n",
    "                                            \n",
    "                                            # 90experiments folder\n",
    "                                            base_dir = os.path.abspath(os.path.join(os.getcwd(),\"..\", \"..\", \"90experiments\"))\n",
    "                                            \n",
    "                                            fixation_maps = {}  # Dictionary to store fixation maps for each imagePath\n",
    "                                            \n",
    "                                            \n",
    "                                            for folder in os.listdir(base_dir):\n",
    "                                                folder_path = os.path.join(base_dir, folder)\n",
    "                                                if not os.path.isdir(folder_path):\n",
    "                                                    continue\n",
    "                                                match = re.search(r'\\d{1,2}$', folder)\n",
    "                                                if match:\n",
    "                                                    observer = int(match.group())\n",
    "                                                #if(observer != 5):\n",
    "                                                #    continue\n",
    "                                                if(observer == 1 or observer == 2 or observer == 49 or observer == 50 or observer == 5):\n",
    "                                                    continue\n",
    "                                            \n",
    "                                                #if(observer not in [70,71,73,74,76,77,79,80,82,83,85,87,88,89,86,90,18,57,6,45,48,60,63,69,3,9,12,21,15,27,30,33,36,42,24,66,51,54,72,75]):\n",
    "                                                #    continue\n",
    "                                                    \n",
    "                                                csv_file_path = os.path.join(folder_path, \"eye_tracker_data.csv\")\n",
    "                                                if not os.path.isfile(csv_file_path):\n",
    "                                                    continue\n",
    "                                                data = pd.read_csv(csv_file_path)\n",
    "                                            \n",
    "                                                filtered_data = data[data['ImagePath'].str.startswith('targetImages')]\n",
    "                                                \n",
    "                                                uniqueImagePaths = []\n",
    "                                                delete_rows = []\n",
    "                                            \n",
    "                                                #get only the eye-tracking data from the first viewing\n",
    "                                                index = 0\n",
    "                                                \n",
    "                                                row = filtered_data.iloc[index]\n",
    "                                                while len(uniqueImagePaths) < 10:\n",
    "                                                    row = filtered_data.iloc[index]\n",
    "                                                    if(row['ImagePath'] not in uniqueImagePaths):\n",
    "                                                        uniqueImagePaths.append(row['ImagePath'])\n",
    "                                                        lastImagePath = row['ImagePath']\n",
    "                                                        index +=1\n",
    "                                                    elif(row['ImagePath'] in uniqueImagePaths):\n",
    "                                                        index += 1    \n",
    "                                                row = filtered_data.iloc[index]\n",
    "                                            \n",
    "                                                while(row['ImagePath'] == lastImagePath):\n",
    "                                                    index +=1\n",
    "                                                    row = filtered_data.iloc[index]\n",
    "                                            \n",
    "                                                filtered_data.reset_index(drop=True, inplace=True)\n",
    "                                                filtered_data = filtered_data.iloc[:index].copy()\n",
    "                                                \n",
    "                                                grouped = filtered_data.groupby('ImagePath')\n",
    "                                            \n",
    "                                                # Generate and save fixation maps for each image in the current folder\n",
    "                                                for image_path, group in grouped:\n",
    "                                                    # Construct full image path by going one directory back from base_dir\n",
    "                                                    full_image_path = os.path.abspath(os.path.join(base_dir, \"..\", image_path))\n",
    "                                                    full_image_path = full_image_path.replace('\\\\', '/')\n",
    "                                            \n",
    "                                                    #check if current observer has remembered this image, if not, continue\n",
    "                                                    if(not checkObserverRemembered(observer, image_path, base_dir)):\n",
    "                                                        continue\n",
    "                                                    \n",
    "                                                    # Extract coordinates\n",
    "                                                    coordinates = group[['PosX', 'PosY']].values\n",
    "                                            \n",
    "                                                    current_fixation_map = get_current_fixation_map (full_image_path, coordinates)\n",
    "                                                    if(np.all(current_fixation_map == 0)):\n",
    "                                                        continue\n",
    "                                            \n",
    "                                                    current_fixation_map_20x20 = process_fixation_map(current_fixation_map)\n",
    "                                                    current_fixation_map_20x20 = normalize_fixation_map(current_fixation_map_20x20)\n",
    "                                                    \n",
    "                                                    #current_fixation_map_20x20 = (current_fixation_map)\n",
    "                                                    #add to dictionary or update it\n",
    "                                                    if image_path not in fixation_maps:\n",
    "                                                        fixation_maps[image_path] = [current_fixation_map_20x20]\n",
    "                                                    else:\n",
    "                                                        fixation_maps[image_path].append(current_fixation_map_20x20)\n",
    "                                                        \n",
    "                                            # Flatten the fixation maps and standardize them\n",
    "                                            all_fixation_maps = []\n",
    "                                            labels = []\n",
    "                                            \n",
    "                                            for image_path, maps in fixation_maps.items():\n",
    "                                                for fixation_map in maps:\n",
    "                                                    all_fixation_maps.append(fixation_map.flatten())\n",
    "                                                    labels.append(image_path)\n",
    "                                            \n",
    "                                            X = np.array(all_fixation_maps)\n",
    "                                            y = np.array(labels)\n",
    "      \n",
    "                                            #training\n",
    "                                            from tensorflow.keras.models import Sequential\n",
    "                                            from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "                                            from tensorflow.keras.optimizers import Adam\n",
    "                                            from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "                                            from sklearn.metrics import classification_report, accuracy_score\n",
    "                                            from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "                                            from tensorflow.keras.regularizers import l2\n",
    "                                            from imblearn.over_sampling import SMOTE\n",
    "                                            import collections\n",
    "                                            import numpy as np\n",
    "                                            import matplotlib.pyplot as plt\n",
    "                                            import json\n",
    "                                            \n",
    "                                            trainings = 15\n",
    "                                            \n",
    "                                            for i in range(trainings):\n",
    "                                                X = X.reshape(-1, grid_size[0], grid_size[1], 1)\n",
    "                                            \n",
    "                                                label_encoder = LabelEncoder()\n",
    "                                                y_encoded = label_encoder.fit_transform(y)\n",
    "                                                label_names = label_encoder.classes_\n",
    "                                                \n",
    "                                                X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=test_size, stratify=y_encoded)\n",
    "                                                \n",
    "                                                y_train_categorical = to_categorical(y_train, num_classes=len(label_encoder.classes_))\n",
    "                                                y_test_categorical = to_categorical(y_test, num_classes=len(label_encoder.classes_))\n",
    "                                                \n",
    "                                                '''\n",
    "                                                # Flatten X_train for SMOTE\n",
    "                                                X_train_flatten = X_train.reshape(X_train.shape[0], -1)\n",
    "                                                \n",
    "                                                # Apply SMOTE to balance the dataset\n",
    "                                                smote = SMOTE()\n",
    "                                                X_train_resampled, y_train_resampled = smote.fit_resample(X_train_flatten, np.argmax(y_train_categorical, axis=1))\n",
    "                                                \n",
    "                                                # Reshape X_train back to original shape\n",
    "                                                X_train_resampled = X_train_resampled.reshape(-1, grid_size[0], grid_size[1], 1)\n",
    "                                                y_train_resampled_categorical = np.eye(len(label_encoder.classes_))[y_train_resampled]\n",
    "                                                \n",
    "                                                # Calculate class weights\n",
    "                                                class_counts = collections.Counter(np.argmax(y_train_resampled_categorical, axis=1))\n",
    "                                                total_samples = sum(class_counts.values())\n",
    "                                                class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "                                                #print(\"Class counts:\", class_counts)\n",
    "                                                #print(\"Class weights:\", class_weights)\n",
    "                                                '''\n",
    "                                                # Define the MLP model\n",
    "                                                \n",
    "                                                model = Sequential([\n",
    "                                                    Input(shape=(grid_size[0], grid_size[1], 1)),\n",
    "                                                    Flatten(),\n",
    "                                                    Dense(512, activation='relu', kernel_regularizer=l2(reg_term)),\n",
    "                                                    BatchNormalization(),\n",
    "                                                    Dropout(dropout),\n",
    "                                                    Dense(256, activation='relu', kernel_regularizer=l2(reg_term)),\n",
    "                                                    BatchNormalization(),\n",
    "                                                    Dropout(dropout),\n",
    "                                                    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "                                                ])\n",
    "                                                '''input_size = (32, 32)\n",
    "                                                X_train = np.array([img_to_array(array_to_img(np.repeat(img, 3, axis=2)).resize(input_size)) for img in X_train])\n",
    "                                                X_test = np.array([img_to_array(array_to_img(np.repeat(img, 3, axis=2)).resize(input_size)) for img in X_test])\n",
    "\n",
    "                                                base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "                                                model = Sequential([\n",
    "                                                    base_model,\n",
    "                                                    GlobalAveragePooling2D(),\n",
    "                                                    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "                                                    Dropout(0.5),\n",
    "                                                    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "                                                ])'''\n",
    "                                                \n",
    "                                                # Compile the model\n",
    "                                                model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3)])\n",
    "                                                \n",
    "                                                # Callbacks for learning rate adjustment and early stopping\n",
    "                                                reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=factor, patience=patience, min_lr=min_lr)\n",
    "                                                early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "                                                model_checkpoint = ModelCheckpoint(os.path.join(results_folder, 'best_model_brute_force.keras'), monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "                                                \n",
    "                                                \n",
    "                                                # Train the model\n",
    "                                                history = model.fit(X_train, y_train_categorical,\n",
    "                                                                    validation_data=(X_test, y_test_categorical),\n",
    "                                                                    epochs=1000,\n",
    "                                                                    callbacks=[reduce_lr, early_stopping, model_checkpoint],\n",
    "                                                                    #class_weight=class_weights,\n",
    "                                                                    batch_size = batch_size,\n",
    "                                                                    verbose = 0\n",
    "                                                                    )\n",
    "                                                \n",
    "                                                # Evaluate the model\n",
    "                                                y_pred_prob = model.predict(X_test, verbose = 0)\n",
    "                                                y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "                                                \n",
    "                                                # Calculate accuracy\n",
    "                                                accuracy = accuracy_score(y_test, y_pred)\n",
    "                                                top_3_accuracy = history.history['val_top_k_categorical_accuracy'][-1]\n",
    "                                                f1 = f1_score(y_test, y_pred, average='macro')  # You can use 'micro' or 'weighted' based on your needs\n",
    "                                                conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "                                                per_class_accuracy = np.mean(conf_matrix.diagonal() / conf_matrix.sum(axis=1))\n",
    "                                                balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "                                                    \n",
    "                                                if(accuracy > best_accuracy):\n",
    "                                                    best_accuracy = accuracy\n",
    "                                                    best_params = {\n",
    "                                                                   'sigma':sigma, \n",
    "                                                                   'grid_size':grid_size, \n",
    "                                                                   'gaussian_size':gaussian_size,\n",
    "                                                                   'batch_size':batch_size,\n",
    "                                                                   'dropout':dropout,\n",
    "                                                                   'reg_term':reg_term,\n",
    "                                                                   'lr':lr,\n",
    "                                                                   'patience':patience,\n",
    "                                                                   'min_lr':min_lr,\n",
    "                                                                   'factor':factor,\n",
    "                                                                   'test_size': test_size}\n",
    "                                                    print(\"Newest best accuracy: \", best_accuracy)\n",
    "                                                    print(\"Params: \", best_params)\n",
    "                                                    # Save the best model, accuracy and top-3 accuracy\n",
    "                                                    with open(os.path.join(results_folder, \"best_accuracy_MLP_search_accuracy.txt\"), \"w\") as f:\n",
    "                                                        f.write(f\"Best accuracy: {best_accuracy:.4f}\\n\")\n",
    "                                                        f.write(\"Best parameters:\\n\")\n",
    "                                                        f.write(json.dumps(best_params, indent=4))\n",
    "                                                    # Define the path to save the model\n",
    "                                                    model_save_path = os.path.join(results_folder, 'best_model.keras')\n",
    "                                                    # Save the model\n",
    "                                                    model.save(model_save_path)\n",
    "                                                        \n",
    "                                                if(top_3_accuracy > best_top3_accuracy):\n",
    "                                                    best_top3_accuracy = top_3_accuracy\n",
    "                                                    best_params_top3 = {\n",
    "                                                                   'sigma':sigma, \n",
    "                                                                   'grid_size':grid_size, \n",
    "                                                                   'gaussian_size':gaussian_size,\n",
    "                                                                   'batch_size':batch_size,\n",
    "                                                                   'dropout':dropout,\n",
    "                                                                   'reg_term':reg_term,\n",
    "                                                                   'lr':lr,\n",
    "                                                                   'patience':patience,\n",
    "                                                                   'min_lr':min_lr,\n",
    "                                                                   'factor':factor,\n",
    "                                                                   'test_size': test_size}\n",
    "                                                    print(\"Newest top3 best accuracy: \", best_top3_accuracy)\n",
    "                                                    print(\"Params top3: \", best_params_top3)\n",
    "                                                    # Save the best model, accuracy and top-3 accuracy\n",
    "                                                    with open(os.path.join(results_folder, \"best_accuracy_MLP_search_top3_accuracy.txt\"), \"w\") as f:\n",
    "                                                        f.write(f\"Top 3 accuracy: {best_top3_accuracy:.4f}\\n\")\n",
    "                                                        f.write(\"Best top3 parameters:\\n\")\n",
    "                                                        f.write(json.dumps(best_params_top3, indent=4))\n",
    "                                                    # Define the path to save the model\n",
    "                                                    model_save_path = os.path.join(results_folder, 'top3_model.keras')\n",
    "                                                    # Save the model\n",
    "                                                    model.save(model_save_path)\n",
    "\n",
    "                                                if(f1 > best_f1_accuracy):\n",
    "                                                    best_f1_accuracy = f1\n",
    "                                                    best_params_f1 = {\n",
    "                                                                   'sigma':sigma, \n",
    "                                                                   'grid_size':grid_size, \n",
    "                                                                   'gaussian_size':gaussian_size,\n",
    "                                                                   'batch_size':batch_size,\n",
    "                                                                   'dropout':dropout,\n",
    "                                                                   'reg_term':reg_term,\n",
    "                                                                   'lr':lr,\n",
    "                                                                   'patience':patience,\n",
    "                                                                   'min_lr':min_lr,\n",
    "                                                                   'factor':factor,\n",
    "                                                                   'test_size': test_size}\n",
    "                                                    print(\"Newest f1 best accuracy: \", best_f1_accuracy)\n",
    "                                                    print(\"Params f1: \", best_params_f1)\n",
    "                                                    # Save the best model, accuracy and top-3 accuracy\n",
    "                                                    with open(os.path.join(results_folder, \"best_accuracy_MLP_search_f1_accuracy.txt\"), \"w\") as f:\n",
    "                                                        f.write(f\"Top f1 accuracy: {best_f1_accuracy:.4f}\\n\")\n",
    "                                                        f.write(\"Best f1 parameters:\\n\")\n",
    "                                                        f.write(json.dumps(best_params_f1, indent=4))\n",
    "                                                    # Define the path to save the model\n",
    "                                                    model_save_path = os.path.join(results_folder, 'f1_model.keras')\n",
    "                                                    # Save the model\n",
    "                                                    model.save(model_save_path)\n",
    "\n",
    "                                                if(per_class_accuracy > best_per_class_accuracy):\n",
    "                                                    best_per_class_accuracy = per_class_accuracy\n",
    "                                                    best_params_per_class = {\n",
    "                                                                   'sigma':sigma, \n",
    "                                                                   'grid_size':grid_size, \n",
    "                                                                   'gaussian_size':gaussian_size,\n",
    "                                                                   'batch_size':batch_size,\n",
    "                                                                   'dropout':dropout,\n",
    "                                                                   'reg_term':reg_term,\n",
    "                                                                   'lr':lr,\n",
    "                                                                   'patience':patience,\n",
    "                                                                   'min_lr':min_lr,\n",
    "                                                                   'factor':factor,\n",
    "                                                                   'test_size': test_size}\n",
    "                                                    print(\"Newest per class best accuracy: \", best_per_class_accuracy)\n",
    "                                                    print(\"Params per class: \", best_params_per_class)\n",
    "                                                    # Save the best model, accuracy and top-3 accuracy\n",
    "                                                    with open(os.path.join(results_folder, \"best_accuracy_MLP_search_per_class_accuracy.txt\"), \"w\") as f:\n",
    "                                                        f.write(f\"Top per_class accuracy: {best_per_class_accuracy:.4f}\\n\")\n",
    "                                                        f.write(\"Best per class parameters:\\n\")\n",
    "                                                        f.write(json.dumps(best_params_per_class, indent=4))\n",
    "                                                    # Define the path to save the model\n",
    "                                                    model_save_path = os.path.join(results_folder, 'per_class_model.keras')\n",
    "                                                    # Save the model\n",
    "                                                    model.save(model_save_path)\n",
    "\n",
    "                                                if(balanced_accuracy > best_balanced_accuracy):\n",
    "                                                    best_balanced_accuracy = balanced_accuracy\n",
    "                                                    best_params_balanced = {\n",
    "                                                                   'sigma':sigma, \n",
    "                                                                   'grid_size':grid_size, \n",
    "                                                                   'gaussian_size':gaussian_size,\n",
    "                                                                   'batch_size':batch_size,\n",
    "                                                                   'dropout':dropout,\n",
    "                                                                   'reg_term':reg_term,\n",
    "                                                                   'lr':lr,\n",
    "                                                                   'patience':patience,\n",
    "                                                                   'min_lr':min_lr,\n",
    "                                                                   'factor':factor,\n",
    "                                                                   'test_size': test_size}\n",
    "                                                    print(\"Newest balanced best accuracy: \", best_balanced_accuracy)\n",
    "                                                    print(\"Params balanced accuracy: \", best_params_balanced)\n",
    "                                                    # Save the best model, accuracy and top-3 accuracy\n",
    "                                                    with open(os.path.join(results_folder, \"best_accuracy_MLP_search_balanced_accuracy.txt\"), \"w\") as f:\n",
    "                                                        f.write(f\"Top balanced accuracy: {best_balanced_accuracy:.4f}\\n\")\n",
    "                                                        f.write(\"Best balanced accuracy parameters:\\n\")\n",
    "                                                        f.write(json.dumps(best_params_balanced, indent=4))\n",
    "                                                    # Define the path to save the model\n",
    "                                                    model_save_path = os.path.join(results_folder, 'balanced_accuracy_model.keras')\n",
    "                                                    # Save the model\n",
    "                                                    model.save(model_save_path)\n",
    "                                                current += 1\n",
    "                                                print(\"Current: \", current)\n",
    "                                            # Clear large data structures\n",
    "                                            del fixation_maps, all_fixation_maps, X, y\n",
    "                                            gc.collect()\n",
    "                                            \n",
    "                                            # Clear Keras session state\n",
    "                                            tf.keras.backend.clear_session()\n",
    "\n",
    "                                            # Additionally, delete the model and history to ensure they don't consume memory\n",
    "                                            del model, history, X_train, X_test, y_train, y_test, y_train_categorical, y_test_categorical, y_pred_prob, y_pred, conf_matrix\n",
    "                                            gc.collect()\n",
    "\n",
    "\n",
    "print(\"FINAL Best accuracy: \", best_accuracy)\n",
    "print(\"FINAL Params: \", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50a2851b-1d87-4d42-9323-73b4b3a45130",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\valerijan\\\\Desktop\\\\masterrad\\\\dataAnalysis\\\\90experiments'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 191\u001b[0m\n\u001b[0;32m    186\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m90experiments\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    188\u001b[0m fixation_maps \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# Dictionary to store fixation maps for each imagePath\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(base_dir):\n\u001b[0;32m    192\u001b[0m     folder_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, folder)\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(folder_path):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\valerijan\\\\Desktop\\\\masterrad\\\\dataAnalysis\\\\90experiments'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import collections\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import math\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import csv\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, balanced_accuracy_score\n",
    "import gc\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout\n",
    "\n",
    "sigmas = [1]\n",
    "#grid_sizes = [(20, 20), (30, 30), (40, 40),(50, 50), (60, 60)]\n",
    "grid_sizes = [(30, 30)]\n",
    "#gaussian_sizes = [(1, 1), (19, 19), (35, 35), (43, 43)]\n",
    "gaussian_sizes = [(43, 43)]\n",
    "#batch_sizes = [32, 48, 128, 176]\n",
    "batch_sizes = [128]\n",
    "dropouts = [0.5]\n",
    "#reg_terms = [0.001, 0.0001]\n",
    "reg_terms = [0.0001]\n",
    "learning_rates = [1e-3]\n",
    "patiences = [30]\n",
    "min_lrs = [1e-6]\n",
    "factors = [0.2]\n",
    "#test_sizes = [0.1, 0.15, 0.2]\n",
    "test_sizes = [0.1]\n",
    "results_folder = \"models comparison all\"\n",
    "\n",
    "options = [\"svm\", \"randomforest\", \"mlp\", \"vgg\", \"mobilenetv2\", \"resnet\", \"cnn\"]\n",
    "#options = [\"randomforest\", \"svm\"]\n",
    "model_accuracies = {}\n",
    "\n",
    "# Create the folder if it does not exist\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "# 90experiments folder\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(),\"..\",\"..\", \"90experiments\"))\n",
    "\n",
    "best_accuracy = 0\n",
    "best_top3_accuracy = 0\n",
    "best_f1_accuracy = 0\n",
    "best_per_class_accuracy = 0\n",
    "best_balanced_accuracy = 0\n",
    "best_params = {}\n",
    "best_params_top3 = {}\n",
    "best_params_f1 = {}\n",
    "best_params_per_class = {}\n",
    "best_params_balanced_accuracy = {}\n",
    "                                            \n",
    "distance = 610\n",
    "h_res = 1920\n",
    "v_res = 1080\n",
    "screen_w = 527\n",
    "screen_h = 296\n",
    "\n",
    "current = 0\n",
    "\n",
    "                                            \n",
    "                                            \n",
    "\n",
    "def compute_ppda(distance, h_res, v_res, screen_w, screen_h):\n",
    "    \"\"\"\n",
    "    Compute the number of pixels per degree of visual angle based on the experimental conditions.\n",
    "    \n",
    "    :param distance: int, the distance between the observer and the screen (in mm)\n",
    "    :param h_res: int, the horizontal resolution of the screen\n",
    "    :param v_res: int, the vertical resolution of the screen\n",
    "    :param screen_w: int, the width of the screen (in mm)\n",
    "    :param screen_h: int, the height of the screen (in mm)\n",
    "    :return horizontal_ppda: float, the number of pixel per degree of visual angle\n",
    "    \"\"\"\n",
    "    pxl_density_x = h_res / screen_w\n",
    "    pxl_density_y = v_res / screen_h\n",
    "    \n",
    "    d = 2 * distance * math.tan(np.deg2rad(0.5))\n",
    "    horizontal_ppda = d * ((pxl_density_x + pxl_density_y) / 2)\n",
    "    \n",
    "    return horizontal_ppda\n",
    "                                            \n",
    "ppda = compute_ppda(distance, h_res, v_res, screen_w, screen_h)\n",
    "\n",
    "for option in options:\n",
    "    for test_size in test_sizes:\n",
    "        for sigma in sigmas:\n",
    "            for factor in factors:\n",
    "                for gaussian_size in gaussian_sizes:\n",
    "                    for batch_size in batch_sizes:\n",
    "                        for dropout in dropouts:\n",
    "                            for reg_term in reg_terms:\n",
    "                                for lr in learning_rates:\n",
    "                                    for patience in patiences:\n",
    "                                        for min_lr in min_lrs:\n",
    "                                            for grid_size in grid_sizes:\n",
    "    \n",
    "                                                def checkObserverRemembered(observer, image_path, base_dir):\n",
    "                                                    csv_file_path = os.path.join(base_dir, \"..\" ,\"hit_status.csv\")\n",
    "                                                    if not os.path.isfile(csv_file_path):\n",
    "                                                        print(\"Error: CSV file not found.\")\n",
    "                                                        return False\n",
    "                                                    df = pd.read_csv(csv_file_path)\n",
    "                                                    filtered_rows = df[(df['Setup Folder'] == observer) & (df['Image Path'] == image_path) & (df['Hit'] == 1)]\n",
    "                                                    if not filtered_rows.empty:\n",
    "                                                        return True\n",
    "                                                    else:\n",
    "                                                        return False\n",
    "    \n",
    "                                                def bin_fixations(fixation_map):\n",
    "                                                    global grid_size\n",
    "                                                    height, width = fixation_map.shape\n",
    "                                                    binned_map = np.zeros(grid_size)\n",
    "                                                \n",
    "                                                    bin_height = height // grid_size[0]\n",
    "                                                    bin_width = width // grid_size[1]\n",
    "                                                \n",
    "                                                    for i in range(grid_size[0]):\n",
    "                                                        for j in range(grid_size[1]):\n",
    "                                                            bin_area = fixation_map[i*bin_height:(i+1)*bin_height, j*bin_width:(j+1)*bin_width]\n",
    "                                                            binned_map[i, j] = np.sum(bin_area)\n",
    "                                                            #ili avg?\n",
    "                                                \n",
    "                                                    return binned_map\n",
    "                                                \n",
    "                                                def normalize_map(binned_map):\n",
    "                                                    return binned_map / np.sum(binned_map)\n",
    "                                                    #return binned_map\n",
    "                                                \n",
    "                                                def smooth_map(binned_map):\n",
    "                                                    global sigma\n",
    "                                                    return gaussian_filter(binned_map, sigma=sigma)\n",
    "                                                \n",
    "                                                def process_fixation_map(fixation_map):\n",
    "                                                    binned_map = bin_fixations(fixation_map)\n",
    "                                                    normalized_map = normalize_map(binned_map)\n",
    "                                                    smoothed_map = smooth_map(normalized_map)\n",
    "                                                    return smoothed_map\n",
    "                                                \n",
    "                                                def get_current_fixation_map(image_path, coordinates):\n",
    "                                                    image = cv2.imread(image_path)\n",
    "                                                    if image is None:\n",
    "                                                        print(f\"Image at {image_path} not found.\")\n",
    "                                                        return\n",
    "                                                \n",
    "                                                    coordinates = coordinates[0:120]\n",
    "                                                  \n",
    "                                                    fixation_map = np.zeros((1080, 1920), dtype=np.float32)\n",
    "                                                \n",
    "                                                    # Convert coordinates to pixel coordinates and update the saliency map\n",
    "                                                    for x_norm, y_norm in coordinates:\n",
    "                                                        # Scale normalized coordinates to pixel coordinates for the 1920x1080 screen\n",
    "                                                        x = int(((x_norm + 1) / 2 ) * 1920)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "                                                        y = int((y_norm + 0.5) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "                                                        # Update the saliency map if coordinates are within the screen\n",
    "                                                        if 0 <= x < 1920 and 0 <= y < 1080:\n",
    "                                                            fixation_map[y, x] += 1 \n",
    "\n",
    "                                                    fixation_map = cv2.GaussianBlur(fixation_map, gaussian_size, 0)\n",
    "                                                    # Crop the saliency map to the 700x700 region\n",
    "                                                    fixation_map = fixation_map[190:890, 610:1310]\n",
    "                                                    # flip the Y coordinates\n",
    "                                                    fixation_map = np.flipud(fixation_map)\n",
    "                                                    return fixation_map\n",
    "                                                \n",
    "                                                def normalize_fixation_map(fixation_map):\n",
    "                                                    min_val = np.min(fixation_map)\n",
    "                                                    max_val = np.max(fixation_map)\n",
    "                                                    normalized_fixation_map = (fixation_map - min_val) / (max_val - min_val) * 255\n",
    "                                                    return normalized_fixation_map\n",
    "                                                \n",
    "                                                \n",
    "                                                \n",
    "                                                fixation_maps = {}  # Dictionary to store fixation maps for each imagePath\n",
    "                                                \n",
    "                                                \n",
    "                                                for folder in os.listdir(base_dir):\n",
    "                                                    folder_path = os.path.join(base_dir, folder)\n",
    "                                                    if not os.path.isdir(folder_path):\n",
    "                                                        continue\n",
    "                                                    match = re.search(r'\\d{1,2}$', folder)\n",
    "                                                    if match:\n",
    "                                                        observer = int(match.group())\n",
    "\n",
    "                                                    if(observer == 1 or observer == 2 or observer == 49 or observer == 50 or observer == 5):\n",
    "                                                        continue\n",
    "\n",
    "                                                    csv_file_path = os.path.join(folder_path, \"eye_tracker_data.csv\")\n",
    "                                                    if not os.path.isfile(csv_file_path):\n",
    "                                                        continue\n",
    "                                                    data = pd.read_csv(csv_file_path)\n",
    "                                                \n",
    "                                                    filtered_data = data[data['ImagePath'].str.startswith('targetImages')]\n",
    "                                                    \n",
    "                                                    uniqueImagePaths = []\n",
    "                                                    delete_rows = []\n",
    "                                                \n",
    "                                                    #get only the eye-tracking data from the first viewing\n",
    "                                                    index = 0\n",
    "                                                    \n",
    "                                                    row = filtered_data.iloc[index]\n",
    "                                                    while len(uniqueImagePaths) < 10:\n",
    "                                                        row = filtered_data.iloc[index]\n",
    "                                                        if(row['ImagePath'] not in uniqueImagePaths):\n",
    "                                                            uniqueImagePaths.append(row['ImagePath'])\n",
    "                                                            lastImagePath = row['ImagePath']\n",
    "                                                            index +=1\n",
    "                                                        elif(row['ImagePath'] in uniqueImagePaths):\n",
    "                                                            index += 1    \n",
    "                                                    row = filtered_data.iloc[index]\n",
    "                                                \n",
    "                                                    while(row['ImagePath'] == lastImagePath):\n",
    "                                                        index +=1\n",
    "                                                        row = filtered_data.iloc[index]\n",
    "                                                \n",
    "                                                    filtered_data.reset_index(drop=True, inplace=True)\n",
    "                                                    filtered_data = filtered_data.iloc[:index].copy()\n",
    "                                                    \n",
    "                                                    grouped = filtered_data.groupby('ImagePath')\n",
    "                                                \n",
    "                                                    # Generate and save fixation maps for each image in the current folder\n",
    "                                                    for image_path, group in grouped:\n",
    "                                                        # Construct full image path by going one directory back from base_dir\n",
    "                                                        full_image_path = os.path.abspath(os.path.join(base_dir, \"..\", image_path))\n",
    "                                                        full_image_path = full_image_path.replace('\\\\', '/')\n",
    "                                                \n",
    "                                                        #check if current observer has remembered this image, if not, continue\n",
    "                                                        if(not checkObserverRemembered(observer, image_path, base_dir)):\n",
    "                                                            continue\n",
    "                                                        \n",
    "                                                        # Extract coordinates\n",
    "                                                        coordinates = group[['PosX', 'PosY']].values\n",
    "                                                \n",
    "                                                        current_fixation_map = get_current_fixation_map (full_image_path, coordinates)\n",
    "                                                        if(np.all(current_fixation_map == 0)):\n",
    "                                                            continue\n",
    "                                                \n",
    "                                                        current_fixation_map_20x20 = process_fixation_map(current_fixation_map)\n",
    "                                                        current_fixation_map_20x20 = normalize_fixation_map(current_fixation_map_20x20)\n",
    "                                                        \n",
    "                                                        #current_fixation_map_20x20 = (current_fixation_map)\n",
    "                                                        #add to dictionary or update it\n",
    "                                                        if image_path not in fixation_maps:\n",
    "                                                            fixation_maps[image_path] = [current_fixation_map_20x20]\n",
    "                                                        else:\n",
    "                                                            fixation_maps[image_path].append(current_fixation_map_20x20)\n",
    "                                                            \n",
    "                                                # Flatten the fixation maps and standardize them\n",
    "                                                all_fixation_maps = []\n",
    "                                                labels = []\n",
    "                                                \n",
    "                                                for image_path, maps in fixation_maps.items():\n",
    "                                                    for fixation_map in maps:\n",
    "                                                        all_fixation_maps.append(fixation_map.flatten())\n",
    "                                                        labels.append(image_path)\n",
    "                                                \n",
    "                                                X = np.array(all_fixation_maps)\n",
    "                                                y = np.array(labels)\n",
    "          \n",
    "                                                #training\n",
    "                                                from keras.applications import VGG16, MobileNetV2\n",
    "                                                from tensorflow.keras.models import Sequential\n",
    "                                                from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "                                                from tensorflow.keras.optimizers import Adam\n",
    "                                                from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "                                                from sklearn.metrics import classification_report, accuracy_score\n",
    "                                                from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "                                                from tensorflow.keras.regularizers import l2\n",
    "                                                from imblearn.over_sampling import SMOTE\n",
    "                                                import collections\n",
    "                                                import numpy as np\n",
    "                                                import matplotlib.pyplot as plt\n",
    "                                                import json\n",
    "                                                from sklearn.svm import SVC\n",
    "                                                from sklearn.ensemble import RandomForestClassifier\n",
    "                                                from sklearn.preprocessing import LabelEncoder\n",
    "                                                from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, balanced_accuracy_score\n",
    "                                                from keras.preprocessing.image import img_to_array, array_to_img\n",
    "                                                import joblib \n",
    "\n",
    "    \n",
    "                                                def save_model(model, model_save_path, option):\n",
    "                                                    # Check if the model is a Keras model by checking if it has a 'save' method\n",
    "                                                    if hasattr(model, 'save'):\n",
    "                                                        model.save(model_save_path)\n",
    "                                                    else:\n",
    "                                                        # For scikit-learn models, use joblib to save\n",
    "                                                        joblib.dump(model, model_save_path)\n",
    "                                                \n",
    "                                                trainings = 30\n",
    "                                                \n",
    "                                                for i in range(trainings):\n",
    "                                                    X = X.reshape(-1, grid_size[0], grid_size[1], 1)\n",
    "                                                \n",
    "                                                    label_encoder = LabelEncoder()\n",
    "                                                    y_encoded = label_encoder.fit_transform(y)\n",
    "                                                    label_names = label_encoder.classes_\n",
    "                                                    \n",
    "                                                    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=test_size, stratify=y_encoded)\n",
    "                                                    \n",
    "                                                    y_train_categorical = to_categorical(y_train, num_classes=len(label_encoder.classes_))\n",
    "                                                    y_test_categorical = to_categorical(y_test, num_classes=len(label_encoder.classes_))\n",
    "\n",
    "\n",
    "                                                    if(option == \"mlp\"):\n",
    "                                                        # Define the MLP model\n",
    "                                                        model = Sequential([\n",
    "                                                            Input(shape=(grid_size[0], grid_size[1], 1)),\n",
    "                                                            Flatten(),\n",
    "                                                            Dense(512, activation='relu', kernel_regularizer=l2(reg_term)),\n",
    "                                                            BatchNormalization(),\n",
    "                                                            Dropout(dropout),\n",
    "                                                            Dense(256, activation='relu', kernel_regularizer=l2(reg_term)),\n",
    "                                                            BatchNormalization(),\n",
    "                                                            Dropout(dropout),\n",
    "                                                            Dense(len(label_encoder.classes_), activation='softmax')\n",
    "                                                        ])\n",
    "\n",
    "                                                    elif(option == \"cnn\"):\n",
    "                                                        # Define the CNN model\n",
    "                                                        model = Sequential([\n",
    "                                                            Input(shape=(grid_size[0], grid_size[1], 1)),\n",
    "                                                            Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(reg_term)),\n",
    "                                                            BatchNormalization(),\n",
    "                                                            MaxPooling2D(pool_size=(2, 2)),\n",
    "                                                            Dropout(dropout),\n",
    "                                                            \n",
    "                                                            Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(reg_term)),\n",
    "                                                            BatchNormalization(),\n",
    "                                                            MaxPooling2D(pool_size=(2, 2)),\n",
    "                                                            Dropout(dropout),\n",
    "                    \n",
    "                                                            Flatten(),\n",
    "                                                            Dense(256, activation='relu', kernel_regularizer=l2(reg_term)),\n",
    "                                                            BatchNormalization(),\n",
    "                                                            Dropout(dropout),\n",
    "                                                            \n",
    "                                                            Dense(len(label_encoder.classes_), activation='softmax')\n",
    "                                                        ])\n",
    "                                                    elif(option == \"resnet\"):\n",
    "                                                        \n",
    "                                                        input_size = (32, 32)\n",
    "                                                        X_train = np.array([img_to_array(array_to_img(np.repeat(img, 3, axis=2)).resize(input_size)) for img in X_train])\n",
    "                                                        X_test = np.array([img_to_array(array_to_img(np.repeat(img, 3, axis=2)).resize(input_size)) for img in X_test])\n",
    "        \n",
    "                                                        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "        \n",
    "                                                        model = Sequential([\n",
    "                                                            base_model,\n",
    "                                                            GlobalAveragePooling2D(),\n",
    "                                                            Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "                                                            Dropout(0.5),\n",
    "                                                            Dense(len(label_encoder.classes_), activation='softmax')\n",
    "                                                        ])\n",
    "                                                    elif(option == \"vgg\"):\n",
    "                                                        # Define the input size\n",
    "                                                        input_size = (32, 32)\n",
    "                                                        \n",
    "                                                        # Convert X_train and X_test to have 3 channels and resize them\n",
    "                                                        X_train = np.array([img_to_array(array_to_img(np.repeat(img, 3, axis=2)).resize(input_size)) for img in X_train])\n",
    "                                                        X_test = np.array([img_to_array(array_to_img(np.repeat(img, 3, axis=2)).resize(input_size)) for img in X_test])\n",
    "                                                        \n",
    "                                                        # Load the VGG16 model without the top layers and specify input shape\n",
    "                                                        base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "                                                        \n",
    "                                                        # Build the sequential model\n",
    "                                                        model = Sequential([\n",
    "                                                            base_model,\n",
    "                                                            GlobalAveragePooling2D(),\n",
    "                                                            Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "                                                            Dropout(0.5),\n",
    "                                                            Dense(len(label_encoder.classes_), activation='softmax')\n",
    "                                                        ])\n",
    "\n",
    "                                                    elif(option == \"mobilenetv2\"):\n",
    "                                                        # Define the input size\n",
    "                                                        input_size = (32, 32)\n",
    "                                                        \n",
    "                                                        # Convert X_train and X_test to have 3 channels and resize them\n",
    "                                                        X_train = np.array([img_to_array(array_to_img(np.repeat(img, 3, axis=2)).resize(input_size)) for img in X_train])\n",
    "                                                        X_test = np.array([img_to_array(array_to_img(np.repeat(img, 3, axis=2)).resize(input_size)) for img in X_test])\n",
    "                                                        \n",
    "                                                        # Load the MobileNetV2 model without the top layers and specify input shape\n",
    "                                                        base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "                                                        \n",
    "                                                        # Build the sequential model\n",
    "                                                        model = Sequential([\n",
    "                                                            base_model,\n",
    "                                                            GlobalAveragePooling2D(),\n",
    "                                                            Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "                                                            Dropout(0.5),\n",
    "                                                            Dense(len(label_encoder.classes_), activation='softmax')\n",
    "                                                        ])\n",
    "                                                    elif option == \"svm\":\n",
    "                                                        input_size = (32, 32)\n",
    "                                                        X_train_flat = np.array([img_to_array(array_to_img(np.repeat(img, 3, axis=2)).resize(input_size)).flatten() for img in X_train])\n",
    "                                                        X_test_flat = np.array([img_to_array(array_to_img(np.repeat(img, 3, axis=2)).resize(input_size)).flatten() for img in X_test])\n",
    "                                                    \n",
    "                                                        label_encoder = LabelEncoder()\n",
    "                                                        y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "                                                        y_test_encoded = label_encoder.transform(y_test)\n",
    "                                                    \n",
    "                                                        model = SVC(kernel='linear', C=1.0, probability=True)\n",
    "                                                        model.fit(X_train_flat, y_train_encoded)\n",
    "                                                    \n",
    "                                                        # Predictions\n",
    "                                                        y_pred_prob = model.predict_proba(X_test_flat)\n",
    "                                                        y_pred = model.predict(X_test_flat)\n",
    "                                                    \n",
    "                                                    elif option == \"randomforest\":\n",
    "                                                        input_size = (32, 32)\n",
    "                                                        X_train_flat = np.array([img_to_array(array_to_img(np.repeat(img, 3, axis=2)).resize(input_size)).flatten() for img in X_train])\n",
    "                                                        X_test_flat = np.array([img_to_array(array_to_img(np.repeat(img, 3, axis=2)).resize(input_size)).flatten() for img in X_test])\n",
    "                                                    \n",
    "                                                        label_encoder = LabelEncoder()\n",
    "                                                        y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "                                                        y_test_encoded = label_encoder.transform(y_test)\n",
    "                                                    \n",
    "                                                        model = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\n",
    "                                                        model.fit(X_train_flat, y_train_encoded)\n",
    "                                                    \n",
    "                                                        # Predictions\n",
    "                                                        y_pred_prob = model.predict_proba(X_test_flat)\n",
    "                                                        y_pred = model.predict(X_test_flat)\n",
    "                                                    \n",
    "                                                    elif option == \"rusboost\":\n",
    "                                                        # Placeholder for RUSBoost implementation\n",
    "                                                        # Implement as needed for your specific case\n",
    "                                                        pass\n",
    "                                                    \n",
    "                                                    # Compile the model only for neural networks\n",
    "                                                    if option in [\"mlp\", \"resnet\", \"vgg\", \"mobilenetv2\", \"cnn\"]:\n",
    "                                                        model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy',\n",
    "                                                                      metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3)])\n",
    "                                                    \n",
    "                                                        # Callbacks for learning rate adjustment and early stopping\n",
    "                                                        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=factor, patience=patience, min_lr=min_lr)\n",
    "                                                        early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "                                                        model_checkpoint = ModelCheckpoint(os.path.join(results_folder, 'best_model_brute_force.keras'), monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "                                                    \n",
    "                                                        # Train the model\n",
    "                                                        history = model.fit(X_train, y_train_categorical,\n",
    "                                                                            validation_data=(X_test, y_test_categorical),\n",
    "                                                                            epochs=1000,\n",
    "                                                                            callbacks=[reduce_lr, early_stopping, model_checkpoint],\n",
    "                                                                            # class_weight=class_weights,\n",
    "                                                                            batch_size=batch_size,\n",
    "                                                                            verbose=0\n",
    "                                                                            )\n",
    "                                                    \n",
    "                                                        # Evaluate the model\n",
    "                                                        y_pred_prob = model.predict(X_test, verbose=0)\n",
    "                                                        y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "                                                    else:\n",
    "                                                        # For non-neural network models like SVM and RandomForest\n",
    "                                                        # We assume y_pred and y_pred_prob are already obtained above\n",
    "                                                        y_pred = model.predict(X_test_flat)\n",
    "                                                        y_pred_prob = model.predict_proba(X_test_flat)\n",
    "                                                    \n",
    "                                                    # Calculate accuracy\n",
    "                                                    accuracy = accuracy_score(y_test, y_pred)\n",
    "                                                    if((not option in model_accuracies) or accuracy > model_accuracies[option]):\n",
    "                                                        model_accuracies[option] = accuracy\n",
    "                                                    if(option in [\"mlp\", \"resnet\", \"vgg\", \"mobilenetv2\", \"cnn\"]):\n",
    "                                                        top_3_accuracy = history.history['val_top_k_categorical_accuracy'][-1]\n",
    "                                                    else:\n",
    "                                                        top_3_accuracy = 0\n",
    "                                                    f1 = f1_score(y_test, y_pred, average='macro')  # You can use 'micro' or 'weighted' based on your needs\n",
    "                                                    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "                                                    per_class_accuracy = np.mean(conf_matrix.diagonal() / conf_matrix.sum(axis=1))\n",
    "                                                    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "                                                  \n",
    "                                                        \n",
    "                                                    if(accuracy > best_accuracy):\n",
    "                                                        model_save_path = os.path.join(results_folder, 'best_model.keras')  # For Keras models\n",
    "                                                        model_save_path_sklearn = os.path.join(results_folder, 'best_model_sklearn.pkl')  # For scikit-learn models\n",
    "\n",
    "                                                        best_accuracy = accuracy\n",
    "                                                        best_params = {\n",
    "                                                                       'sigma':sigma, \n",
    "                                                                       'grid_size':grid_size, \n",
    "                                                                       'gaussian_size':gaussian_size,\n",
    "                                                                       'batch_size':batch_size,\n",
    "                                                                       'dropout':dropout,\n",
    "                                                                       'reg_term':reg_term,\n",
    "                                                                       'lr':lr,\n",
    "                                                                       'patience':patience,\n",
    "                                                                       'min_lr':min_lr,\n",
    "                                                                       'factor':factor,\n",
    "                                                                       'test_size': test_size,\n",
    "                                                                        'model' : option}\n",
    "                                                        print(\"Newest best accuracy: \", best_accuracy)\n",
    "                                                        print(\"Params: \", best_params)\n",
    "                                                        # Save the best model, accuracy and top-3 accuracy\n",
    "                                                        with open(os.path.join(results_folder, \"best_accuracy_MLP_search_accuracy.txt\"), \"w\") as f:\n",
    "                                                            f.write(f\"Best accuracy: {best_accuracy:.4f}\\n\")\n",
    "                                                            f.write(\"Best parameters:\\n\")\n",
    "                                                            f.write(json.dumps(best_params, indent=4))\n",
    "                                                        # Define the path to save the model\n",
    "                                                        '''model_save_path = os.path.join(results_folder, 'best_model.keras')\n",
    "                                                        # Save the model\n",
    "                                                        model.save(model_save_path)'''\n",
    "                                                        #save_model(model, model_save_path if option in [\"mlp\", \"resnet\", \"vgg\", \"mobilenetv2\"] else model_save_path_sklearn, option)\n",
    "\n",
    "                                                            \n",
    "                                                    if(top_3_accuracy > best_top3_accuracy):\n",
    "                                                        best_top3_accuracy = top_3_accuracy\n",
    "                                                        best_params_top3 = {\n",
    "                                                                       'sigma':sigma, \n",
    "                                                                       'grid_size':grid_size, \n",
    "                                                                       'gaussian_size':gaussian_size,\n",
    "                                                                       'batch_size':batch_size,\n",
    "                                                                       'dropout':dropout,\n",
    "                                                                       'reg_term':reg_term,\n",
    "                                                                       'lr':lr,\n",
    "                                                                       'patience':patience,\n",
    "                                                                       'min_lr':min_lr,\n",
    "                                                                       'factor':factor,\n",
    "                                                                       'test_size': test_size,\n",
    "                                                                        'model' : option}\n",
    "                                                        print(\"Newest top3 best accuracy: \", best_top3_accuracy)\n",
    "                                                        print(\"Params top3: \", best_params_top3)\n",
    "                                                        # Save the best model, accuracy and top-3 accuracy\n",
    "                                                        with open(os.path.join(results_folder, \"best_accuracy_MLP_search_top3_accuracy.txt\"), \"w\") as f:\n",
    "                                                            f.write(f\"Top 3 accuracy: {best_top3_accuracy:.4f}\\n\")\n",
    "                                                            f.write(\"Best top3 parameters:\\n\")\n",
    "                                                            f.write(json.dumps(best_params_top3, indent=4))\n",
    "                                                        # Define the path to save the model\n",
    "                                                        '''model_save_path = os.path.join(results_folder, 'top3_model.keras')\n",
    "                                                        # Save the model\n",
    "                                                        model.save(model_save_path)'''\n",
    "                                                        #save_model(model, model_save_path if option in [\"mlp\", \"resnet\", \"vgg\", \"mobilenetv2\"] else model_save_path_sklearn, option)\n",
    "\n",
    "    \n",
    "                                                    if(f1 > best_f1_accuracy):\n",
    "                                                        best_f1_accuracy = f1\n",
    "                                                        best_params_f1 = {\n",
    "                                                                       'sigma':sigma, \n",
    "                                                                       'grid_size':grid_size, \n",
    "                                                                       'gaussian_size':gaussian_size,\n",
    "                                                                       'batch_size':batch_size,\n",
    "                                                                       'dropout':dropout,\n",
    "                                                                       'reg_term':reg_term,\n",
    "                                                                       'lr':lr,\n",
    "                                                                       'patience':patience,\n",
    "                                                                       'min_lr':min_lr,\n",
    "                                                                       'factor':factor,\n",
    "                                                                       'test_size': test_size,\n",
    "                                                                        'model' : option}\n",
    "                                                        print(\"Newest f1 best accuracy: \", best_f1_accuracy)\n",
    "                                                        print(\"Params f1: \", best_params_f1)\n",
    "                                                        # Save the best model, accuracy and top-3 accuracy\n",
    "                                                        with open(os.path.join(results_folder, \"best_accuracy_MLP_search_f1_accuracy.txt\"), \"w\") as f:\n",
    "                                                            f.write(f\"Top f1 accuracy: {best_f1_accuracy:.4f}\\n\")\n",
    "                                                            f.write(\"Best f1 parameters:\\n\")\n",
    "                                                            f.write(json.dumps(best_params_f1, indent=4))\n",
    "                                                        # Define the path to save the model\n",
    "                                                        '''model_save_path = os.path.join(results_folder, 'f1_model.keras')\n",
    "                                                        # Save the model\n",
    "                                                        model.save(model_save_path)'''\n",
    "    \n",
    "                                                    if(per_class_accuracy > best_per_class_accuracy):\n",
    "                                                        best_per_class_accuracy = per_class_accuracy\n",
    "                                                        best_params_per_class = {\n",
    "                                                                       'sigma':sigma, \n",
    "                                                                       'grid_size':grid_size, \n",
    "                                                                       'gaussian_size':gaussian_size,\n",
    "                                                                       'batch_size':batch_size,\n",
    "                                                                       'dropout':dropout,\n",
    "                                                                       'reg_term':reg_term,\n",
    "                                                                       'lr':lr,\n",
    "                                                                       'patience':patience,\n",
    "                                                                       'min_lr':min_lr,\n",
    "                                                                       'factor':factor,\n",
    "                                                                       'test_size': test_size,\n",
    "                                                                        'model' : option}\n",
    "                                                        print(\"Newest per class best accuracy: \", best_per_class_accuracy)\n",
    "                                                        print(\"Params per class: \", best_params_per_class)\n",
    "                                                        # Save the best model, accuracy and top-3 accuracy\n",
    "                                                        with open(os.path.join(results_folder, \"best_accuracy_MLP_search_per_class_accuracy.txt\"), \"w\") as f:\n",
    "                                                            f.write(f\"Top per_class accuracy: {best_per_class_accuracy:.4f}\\n\")\n",
    "                                                            f.write(\"Best per class parameters:\\n\")\n",
    "                                                            f.write(json.dumps(best_params_per_class, indent=4))\n",
    "                                                        # Define the path to save the model\n",
    "                                                        '''model_save_path = os.path.join(results_folder, 'per_class_model.keras')\n",
    "                                                        # Save the model\n",
    "                                                        model.save(model_save_path)'''\n",
    "    \n",
    "                                                    if(balanced_accuracy > best_balanced_accuracy):\n",
    "                                                        best_balanced_accuracy = balanced_accuracy\n",
    "                                                        best_params_balanced = {\n",
    "                                                                       'sigma':sigma, \n",
    "                                                                       'grid_size':grid_size, \n",
    "                                                                       'gaussian_size':gaussian_size,\n",
    "                                                                       'batch_size':batch_size,\n",
    "                                                                       'dropout':dropout,\n",
    "                                                                       'reg_term':reg_term,\n",
    "                                                                       'lr':lr,\n",
    "                                                                       'patience':patience,\n",
    "                                                                       'min_lr':min_lr,\n",
    "                                                                       'factor':factor,\n",
    "                                                                       'test_size': test_size,\n",
    "                                                                        'model' : option}\n",
    "                                                        print(\"Newest balanced best accuracy: \", best_balanced_accuracy)\n",
    "                                                        print(\"Params balanced accuracy: \", best_params_balanced)\n",
    "                                                        # Save the best model, accuracy and top-3 accuracy\n",
    "                                                        with open(os.path.join(results_folder, \"best_accuracy_MLP_search_balanced_accuracy.txt\"), \"w\") as f:\n",
    "                                                            f.write(f\"Top balanced accuracy: {best_balanced_accuracy:.4f}\\n\")\n",
    "                                                            f.write(\"Best balanced accuracy parameters:\\n\")\n",
    "                                                            f.write(json.dumps(best_params_balanced, indent=4))\n",
    "                                                        # Define the path to save the model\n",
    "                                                        '''model_save_path = os.path.join(results_folder, 'balanced_accuracy_model.keras')\n",
    "                                                        # Save the model\n",
    "                                                        model.save(model_save_path)'''\n",
    "                                                    current += 1\n",
    "                                                    print(\"Current: \", current)\n",
    "                                                # Clear large data structures\n",
    "                                                del fixation_maps, all_fixation_maps, X, y\n",
    "                                                gc.collect()\n",
    "                                                \n",
    "                                                # Clear Keras session state\n",
    "                                                tf.keras.backend.clear_session()\n",
    "    \n",
    "                                                # Additionally, delete the model and history to ensure they don't consume memory\n",
    "                                                del model, X_train, X_test, y_train, y_test, y_train_categorical, y_test_categorical, y_pred_prob, y_pred, conf_matrix\n",
    "                                                gc.collect()\n",
    "\n",
    "\n",
    "#print(\"FINAL Best accuracy: \", best_accuracy)\n",
    "#print(\"FINAL Params: \", best_params)\n",
    "\n",
    "# Plotting accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_accuracies.keys(), model_accuracies.values(), color='skyblue')\n",
    "plt.xlabel('Model Option')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for Each Model Option')\n",
    "plt.ylim(0, 1)  # Since accuracy is a fraction between 0 and 1\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Annotate each bar with the accuracy value\n",
    "for i, (model, acc) in enumerate(model_accuracies.items()):\n",
    "    plt.text(i, acc + 0.01, f'{acc:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Save the plot as an image\n",
    "plt.savefig('model_accuracies.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b8637-3f02-4e82-8574-dd97cda889bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
