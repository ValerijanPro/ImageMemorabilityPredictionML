{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e0ac92-9d90-4356-b35d-2ebf6977985b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newest best accuracy:  0.5238095238095238\n",
      "Params:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest top3 best accuracy:  0.682539701461792\n",
      "Params top3:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest f1 best accuracy:  0.4780952380952381\n",
      "Params f1:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest per class best accuracy:  0.5166666666666667\n",
      "Params per class:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.5166666666666667\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  1\n",
      "Newest best accuracy:  0.5396825396825397\n",
      "Params:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest per class best accuracy:  0.5277777777777778\n",
      "Params per class:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.5277777777777778\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  2\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000016C2186D9E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000016C2186D9E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Newest top3 best accuracy:  0.7142857313156128\n",
      "Params top3:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest f1 best accuracy:  0.49293650793650795\n",
      "Params f1:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  3\n",
      "Current:  4\n",
      "Newest top3 best accuracy:  0.761904776096344\n",
      "Params top3:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  5\n",
      "Current:  6\n",
      "Current:  7\n",
      "Newest best accuracy:  0.5714285714285714\n",
      "Params:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest f1 best accuracy:  0.5441269841269841\n",
      "Params f1:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest per class best accuracy:  0.5722222222222223\n",
      "Params per class:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.5722222222222223\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  8\n",
      "Current:  9\n",
      "Newest best accuracy:  0.5873015873015873\n",
      "Params:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  10\n",
      "Current:  11\n",
      "Current:  12\n",
      "Current:  13\n",
      "Newest f1 best accuracy:  0.5455555555555556\n",
      "Params f1:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  14\n",
      "Newest per class best accuracy:  0.5777777777777777\n",
      "Params per class:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.5777777777777777\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  15\n",
      "WARNING:tensorflow:From C:\\Users\\valerijan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Current:  16\n",
      "Current:  17\n",
      "Current:  18\n",
      "Current:  19\n",
      "Current:  20\n",
      "Current:  21\n",
      "Current:  22\n",
      "Current:  23\n",
      "Newest f1 best accuracy:  0.55\n",
      "Params f1:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  24\n",
      "Current:  25\n",
      "Current:  26\n",
      "Current:  27\n",
      "Current:  28\n",
      "Current:  29\n",
      "Current:  30\n",
      "Newest best accuracy:  0.6031746031746031\n",
      "Params:  {'sigma': 1, 'grid_size': (40, 40), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest f1 best accuracy:  0.5850000000000001\n",
      "Params f1:  {'sigma': 1, 'grid_size': (40, 40), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest per class best accuracy:  0.6055555555555555\n",
      "Params per class:  {'sigma': 1, 'grid_size': (40, 40), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.6055555555555555\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (40, 40), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  31\n",
      "Current:  32\n",
      "Current:  33\n",
      "Current:  34\n",
      "Current:  35\n",
      "Current:  36\n",
      "Current:  37\n",
      "Current:  38\n",
      "Current:  39\n",
      "Current:  40\n",
      "Current:  41\n",
      "Current:  42\n",
      "Current:  43\n",
      "Current:  44\n",
      "Current:  45\n",
      "Current:  46\n",
      "Current:  47\n",
      "Current:  48\n",
      "Current:  49\n",
      "Current:  50\n",
      "Current:  51\n",
      "Current:  52\n",
      "Current:  53\n",
      "Current:  54\n",
      "Current:  55\n",
      "Current:  56\n",
      "Current:  57\n",
      "Current:  58\n",
      "Current:  59\n",
      "Current:  60\n",
      "Current:  61\n",
      "Current:  62\n",
      "Current:  63\n",
      "Current:  64\n",
      "Current:  65\n",
      "Current:  66\n",
      "Current:  67\n",
      "Current:  68\n",
      "Current:  69\n",
      "Current:  70\n",
      "Current:  71\n",
      "Current:  72\n",
      "Current:  73\n",
      "Current:  74\n",
      "Current:  75\n",
      "Current:  76\n",
      "Current:  77\n",
      "Current:  78\n",
      "Current:  79\n",
      "Current:  80\n",
      "Current:  81\n",
      "Current:  82\n",
      "Current:  83\n",
      "Current:  84\n",
      "Current:  85\n",
      "Current:  86\n",
      "Current:  87\n",
      "Current:  88\n",
      "Current:  89\n",
      "Current:  90\n",
      "Newest best accuracy:  0.6190476190476191\n",
      "Params:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest per class best accuracy:  0.6222222222222221\n",
      "Params per class:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.6222222222222221\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  91\n",
      "Current:  92\n",
      "Current:  93\n",
      "Current:  94\n",
      "Current:  95\n",
      "Current:  96\n",
      "Current:  97\n",
      "Current:  98\n",
      "Current:  99\n",
      "Current:  100\n",
      "Current:  101\n",
      "Current:  102\n",
      "Current:  103\n",
      "Current:  104\n",
      "Current:  105\n",
      "Current:  106\n",
      "Current:  107\n",
      "Current:  108\n",
      "Current:  109\n",
      "Current:  110\n",
      "Current:  111\n",
      "Current:  112\n",
      "Current:  113\n",
      "Current:  114\n",
      "Current:  115\n",
      "Current:  116\n",
      "Current:  117\n",
      "Current:  118\n",
      "Newest top3 best accuracy:  0.7777777910232544\n",
      "Params top3:  {'sigma': 1, 'grid_size': (40, 40), 'gaussian_size': (1, 1), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  119\n",
      "Current:  120\n",
      "Current:  121\n",
      "Current:  122\n",
      "Current:  123\n",
      "Current:  124\n",
      "Current:  125\n",
      "Current:  126\n",
      "Current:  127\n",
      "Current:  128\n",
      "Current:  129\n",
      "Current:  130\n",
      "Current:  131\n",
      "Current:  132\n",
      "Current:  133\n",
      "Current:  134\n",
      "Current:  135\n",
      "Current:  136\n",
      "Current:  137\n",
      "Current:  138\n",
      "Current:  139\n",
      "Current:  140\n",
      "Current:  141\n",
      "Current:  142\n",
      "Current:  143\n",
      "Current:  144\n",
      "Current:  145\n",
      "Current:  146\n",
      "Current:  147\n",
      "Current:  148\n",
      "Current:  149\n",
      "Current:  150\n",
      "Current:  151\n",
      "Current:  152\n",
      "Current:  153\n",
      "Current:  154\n",
      "Current:  155\n",
      "Current:  156\n",
      "Current:  157\n",
      "Current:  158\n",
      "Current:  159\n",
      "Current:  160\n",
      "Current:  161\n",
      "Current:  162\n",
      "Current:  163\n",
      "Current:  164\n",
      "Current:  165\n",
      "Current:  166\n",
      "Current:  167\n",
      "Current:  168\n",
      "Current:  169\n",
      "Current:  170\n",
      "Current:  171\n",
      "Current:  172\n",
      "Current:  173\n",
      "Current:  174\n",
      "Current:  175\n",
      "Current:  176\n",
      "Current:  177\n",
      "Current:  178\n",
      "Current:  179\n",
      "Current:  180\n",
      "Current:  181\n",
      "Current:  182\n",
      "Current:  183\n",
      "Current:  184\n",
      "Current:  185\n",
      "Current:  186\n",
      "Current:  187\n",
      "Current:  188\n",
      "Current:  189\n",
      "Current:  190\n",
      "Current:  191\n",
      "Current:  192\n",
      "Current:  193\n",
      "Current:  194\n",
      "Current:  195\n",
      "Current:  196\n",
      "Current:  197\n",
      "Current:  198\n",
      "Current:  199\n",
      "Current:  200\n",
      "Current:  201\n",
      "Current:  202\n",
      "Current:  203\n",
      "Current:  204\n",
      "Current:  205\n",
      "Current:  206\n",
      "Current:  207\n",
      "Current:  208\n",
      "Current:  209\n",
      "Current:  210\n",
      "Current:  211\n",
      "Current:  212\n",
      "Current:  213\n",
      "Current:  214\n",
      "Current:  215\n",
      "Current:  216\n",
      "Current:  217\n",
      "Current:  218\n",
      "Current:  219\n",
      "Current:  220\n",
      "Current:  221\n",
      "Current:  222\n",
      "Current:  223\n",
      "Current:  224\n",
      "Current:  225\n",
      "Current:  226\n",
      "Current:  227\n",
      "Current:  228\n",
      "Newest best accuracy:  0.6507936507936508\n",
      "Params:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 48, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest f1 best accuracy:  0.6188888888888889\n",
      "Params f1:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 48, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest per class best accuracy:  0.65\n",
      "Params per class:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 48, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.65\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 48, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  229\n",
      "Current:  230\n",
      "Current:  231\n",
      "Current:  232\n",
      "Current:  233\n",
      "Current:  234\n",
      "Current:  235\n",
      "Current:  236\n",
      "Current:  237\n",
      "Current:  238\n",
      "Current:  239\n",
      "Current:  240\n",
      "Current:  241\n",
      "Current:  242\n",
      "Current:  243\n",
      "Current:  244\n",
      "Current:  245\n",
      "Current:  246\n",
      "Current:  247\n",
      "Current:  248\n",
      "Current:  249\n",
      "Current:  250\n",
      "Current:  251\n",
      "Current:  252\n",
      "Current:  253\n",
      "Current:  254\n",
      "Current:  255\n",
      "Current:  256\n",
      "Current:  257\n",
      "Current:  258\n",
      "Current:  259\n",
      "Current:  260\n",
      "Current:  261\n",
      "Current:  262\n",
      "Current:  263\n",
      "Current:  264\n",
      "Current:  265\n",
      "Current:  266\n",
      "Current:  267\n",
      "Current:  268\n",
      "Current:  269\n",
      "Current:  270\n",
      "Current:  271\n",
      "Current:  272\n",
      "Current:  273\n",
      "Current:  274\n",
      "Current:  275\n",
      "Current:  276\n",
      "Current:  277\n",
      "Current:  278\n",
      "Current:  279\n",
      "Current:  280\n",
      "Current:  281\n",
      "Current:  282\n",
      "Current:  283\n",
      "Current:  284\n",
      "Current:  285\n",
      "Current:  286\n",
      "Current:  287\n",
      "Current:  288\n",
      "Current:  289\n",
      "Current:  290\n",
      "Current:  291\n",
      "Current:  292\n",
      "Current:  293\n",
      "Current:  294\n",
      "Current:  295\n",
      "Current:  296\n",
      "Current:  297\n",
      "Current:  298\n",
      "Current:  299\n",
      "Current:  300\n",
      "Current:  301\n",
      "Current:  302\n",
      "Current:  303\n",
      "Current:  304\n",
      "Current:  305\n",
      "Current:  306\n",
      "Current:  307\n",
      "Current:  308\n",
      "Current:  309\n",
      "Current:  310\n",
      "Current:  311\n",
      "Current:  312\n",
      "Current:  313\n",
      "Current:  314\n",
      "Current:  315\n",
      "Current:  316\n",
      "Current:  317\n",
      "Current:  318\n",
      "Current:  319\n",
      "Current:  320\n",
      "Current:  321\n",
      "Current:  322\n",
      "Current:  323\n",
      "Current:  324\n",
      "Current:  325\n",
      "Current:  326\n",
      "Current:  327\n",
      "Current:  328\n",
      "Current:  329\n",
      "Current:  330\n",
      "Current:  331\n",
      "Current:  332\n",
      "Current:  333\n",
      "Current:  334\n",
      "Current:  335\n",
      "Current:  336\n",
      "Current:  337\n",
      "Current:  338\n",
      "Current:  339\n",
      "Current:  340\n",
      "Current:  341\n",
      "Current:  342\n",
      "Current:  343\n",
      "Current:  344\n",
      "Current:  345\n",
      "Current:  346\n",
      "Current:  347\n",
      "Current:  348\n",
      "Current:  349\n",
      "Current:  350\n",
      "Current:  351\n",
      "Current:  352\n",
      "Current:  353\n",
      "Current:  354\n",
      "Current:  355\n",
      "Current:  356\n",
      "Current:  357\n",
      "Current:  358\n",
      "Current:  359\n",
      "Current:  360\n",
      "Current:  361\n",
      "Current:  362\n",
      "Current:  363\n",
      "Current:  364\n",
      "Current:  365\n",
      "Current:  366\n",
      "Current:  367\n",
      "Current:  368\n",
      "Current:  369\n",
      "Current:  370\n",
      "Current:  371\n",
      "Current:  372\n",
      "Current:  373\n",
      "Current:  374\n",
      "Current:  375\n",
      "Current:  376\n",
      "Current:  377\n",
      "Current:  378\n",
      "Current:  379\n",
      "Current:  380\n",
      "Current:  381\n",
      "Current:  382\n",
      "Current:  383\n",
      "Current:  384\n",
      "Current:  385\n",
      "Current:  386\n",
      "Current:  387\n",
      "Current:  388\n",
      "Current:  389\n",
      "Current:  390\n",
      "Current:  391\n",
      "Current:  392\n",
      "Current:  393\n",
      "Current:  394\n",
      "Current:  395\n",
      "Current:  396\n",
      "Current:  397\n",
      "Current:  398\n",
      "Current:  399\n",
      "Current:  400\n",
      "Current:  401\n",
      "Current:  402\n",
      "Newest f1 best accuracy:  0.6244444444444445\n",
      "Params f1:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (1, 1), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  403\n",
      "Current:  404\n",
      "Current:  405\n",
      "Current:  406\n",
      "Current:  407\n",
      "Current:  408\n",
      "Current:  409\n",
      "Current:  410\n",
      "Current:  411\n",
      "Current:  412\n",
      "Current:  413\n",
      "Current:  414\n",
      "Current:  415\n",
      "Current:  416\n",
      "Current:  417\n",
      "Current:  418\n",
      "Current:  419\n",
      "Current:  420\n",
      "Current:  421\n",
      "Current:  422\n",
      "Current:  423\n",
      "Current:  424\n",
      "Current:  425\n",
      "Current:  426\n",
      "Current:  427\n",
      "Current:  428\n",
      "Current:  429\n",
      "Current:  430\n",
      "Current:  431\n",
      "Current:  432\n",
      "Current:  433\n",
      "Current:  434\n",
      "Current:  435\n",
      "Current:  436\n",
      "Current:  437\n",
      "Current:  438\n",
      "Current:  439\n",
      "Current:  440\n",
      "Current:  441\n",
      "Current:  442\n",
      "Current:  443\n",
      "Current:  444\n",
      "Current:  445\n",
      "Current:  446\n",
      "Current:  447\n",
      "Current:  448\n",
      "Current:  449\n",
      "Current:  450\n",
      "Current:  451\n",
      "Current:  452\n",
      "Current:  453\n",
      "Current:  454\n",
      "Current:  455\n",
      "Current:  456\n",
      "Current:  457\n",
      "Current:  458\n",
      "Current:  459\n",
      "Current:  460\n",
      "Current:  461\n",
      "Current:  462\n",
      "Current:  463\n",
      "Current:  464\n",
      "Current:  465\n",
      "Current:  466\n",
      "Current:  467\n",
      "Current:  468\n",
      "Current:  469\n",
      "Current:  470\n",
      "Current:  471\n",
      "Current:  472\n",
      "Current:  473\n",
      "Current:  474\n",
      "Current:  475\n",
      "Current:  476\n",
      "Current:  477\n",
      "Current:  478\n",
      "Current:  479\n",
      "Current:  480\n",
      "Current:  481\n",
      "Current:  482\n",
      "Current:  483\n",
      "Current:  484\n",
      "Current:  485\n",
      "Current:  486\n",
      "Current:  487\n",
      "Current:  488\n",
      "Current:  489\n",
      "Current:  490\n",
      "Current:  491\n",
      "Current:  492\n",
      "Current:  493\n",
      "Current:  494\n",
      "Current:  495\n",
      "Current:  496\n",
      "Current:  497\n",
      "Current:  498\n",
      "Current:  499\n",
      "Current:  500\n",
      "Current:  501\n",
      "Current:  502\n",
      "Current:  503\n",
      "Current:  504\n",
      "Current:  505\n",
      "Current:  506\n",
      "Current:  507\n",
      "Current:  508\n",
      "Current:  509\n",
      "Current:  510\n",
      "Current:  511\n",
      "Current:  512\n",
      "Current:  513\n",
      "Current:  514\n",
      "Current:  515\n",
      "Current:  516\n",
      "Current:  517\n",
      "Current:  518\n",
      "Current:  519\n",
      "Current:  520\n",
      "Current:  521\n",
      "Current:  522\n",
      "Current:  523\n",
      "Current:  524\n",
      "Current:  525\n",
      "Current:  526\n",
      "Current:  527\n",
      "Current:  528\n",
      "Current:  529\n",
      "Newest top3 best accuracy:  0.8095238208770752\n",
      "Params top3:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (1, 1), 'batch_size': 176, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  530\n",
      "Current:  531\n",
      "Current:  532\n",
      "Current:  533\n",
      "Current:  534\n",
      "Current:  535\n",
      "Current:  536\n",
      "Current:  537\n",
      "Current:  538\n",
      "Current:  539\n",
      "Current:  540\n",
      "Current:  541\n",
      "Current:  542\n",
      "Current:  543\n",
      "Current:  544\n",
      "Current:  545\n",
      "Current:  546\n",
      "Current:  547\n",
      "Current:  548\n",
      "Current:  549\n",
      "Current:  550\n",
      "Current:  551\n",
      "Current:  552\n",
      "Current:  553\n",
      "Current:  554\n",
      "Current:  555\n",
      "Current:  556\n",
      "Current:  557\n",
      "Current:  558\n",
      "Current:  559\n",
      "Current:  560\n",
      "Current:  561\n",
      "Current:  562\n",
      "Current:  563\n",
      "Current:  564\n",
      "Current:  565\n",
      "Current:  566\n",
      "Current:  567\n",
      "Current:  568\n",
      "Current:  569\n",
      "Current:  570\n",
      "Current:  571\n",
      "Current:  572\n",
      "Current:  573\n",
      "Current:  574\n",
      "Current:  575\n",
      "Current:  576\n",
      "Current:  577\n",
      "Current:  578\n",
      "Current:  579\n",
      "Current:  580\n",
      "Current:  581\n",
      "Current:  582\n",
      "Current:  583\n",
      "Current:  584\n",
      "Current:  585\n",
      "Current:  586\n",
      "Current:  587\n",
      "Current:  588\n",
      "Current:  589\n",
      "Current:  590\n",
      "Current:  591\n",
      "Current:  592\n",
      "Current:  593\n",
      "Current:  594\n",
      "Current:  595\n",
      "Current:  596\n",
      "Current:  597\n",
      "Current:  598\n",
      "Current:  599\n",
      "Current:  600\n",
      "Current:  601\n",
      "Current:  602\n",
      "Current:  603\n",
      "Current:  604\n",
      "Current:  605\n",
      "Current:  606\n",
      "Current:  607\n",
      "Current:  608\n",
      "Current:  609\n",
      "Current:  610\n",
      "Current:  611\n",
      "Current:  612\n",
      "Current:  613\n",
      "Current:  614\n",
      "Current:  615\n",
      "Current:  616\n",
      "Current:  617\n",
      "Current:  618\n",
      "Current:  619\n",
      "Current:  620\n",
      "Current:  621\n",
      "Current:  622\n",
      "Current:  623\n",
      "Current:  624\n",
      "Current:  625\n",
      "Current:  626\n",
      "Current:  627\n",
      "Current:  628\n",
      "Current:  629\n",
      "Current:  630\n",
      "Current:  631\n",
      "Current:  632\n",
      "Current:  633\n",
      "Current:  634\n",
      "Current:  635\n",
      "Current:  636\n",
      "Current:  637\n",
      "Current:  638\n",
      "Current:  639\n",
      "Current:  640\n",
      "Current:  641\n",
      "Current:  642\n",
      "Current:  643\n",
      "Current:  644\n",
      "Current:  645\n",
      "Current:  646\n",
      "Current:  647\n",
      "Current:  648\n",
      "Current:  649\n",
      "Current:  650\n",
      "Current:  651\n",
      "Current:  652\n",
      "Current:  653\n",
      "Current:  654\n",
      "Current:  655\n",
      "Current:  656\n",
      "Current:  657\n",
      "Current:  658\n",
      "Current:  659\n",
      "Current:  660\n",
      "Current:  661\n",
      "Current:  662\n",
      "Current:  663\n",
      "Current:  664\n",
      "Current:  665\n",
      "Current:  666\n",
      "Current:  667\n",
      "Current:  668\n",
      "Current:  669\n",
      "Current:  670\n",
      "Current:  671\n",
      "Current:  672\n",
      "Current:  673\n",
      "Current:  674\n",
      "Current:  675\n",
      "Current:  676\n",
      "Current:  677\n",
      "Current:  678\n",
      "Current:  679\n",
      "Current:  680\n",
      "Current:  681\n",
      "Current:  682\n",
      "Current:  683\n",
      "Current:  684\n",
      "Current:  685\n",
      "Current:  686\n",
      "Current:  687\n",
      "Current:  688\n",
      "Current:  689\n",
      "Current:  690\n",
      "Current:  691\n",
      "Current:  692\n",
      "Current:  693\n",
      "Current:  694\n",
      "Current:  695\n",
      "Current:  696\n",
      "Current:  697\n",
      "Current:  698\n",
      "Current:  699\n",
      "Current:  700\n",
      "Current:  701\n",
      "Current:  702\n",
      "Current:  703\n",
      "Current:  704\n",
      "Current:  705\n",
      "Current:  706\n",
      "Current:  707\n",
      "Current:  708\n",
      "Current:  709\n",
      "Current:  710\n",
      "Current:  711\n",
      "Current:  712\n",
      "Current:  713\n",
      "Current:  714\n",
      "Current:  715\n",
      "Current:  716\n",
      "Current:  717\n",
      "Current:  718\n",
      "Current:  719\n",
      "Current:  720\n",
      "Current:  721\n",
      "Current:  722\n",
      "Current:  723\n",
      "Current:  724\n",
      "Current:  725\n",
      "Current:  726\n",
      "Current:  727\n",
      "Current:  728\n",
      "Current:  729\n",
      "Current:  730\n",
      "Current:  731\n",
      "Current:  732\n",
      "Current:  733\n",
      "Current:  734\n",
      "Current:  735\n",
      "Current:  736\n",
      "Current:  737\n",
      "Current:  738\n",
      "Current:  739\n",
      "Current:  740\n",
      "Current:  741\n",
      "Current:  742\n",
      "Current:  743\n",
      "Current:  744\n",
      "Current:  745\n",
      "Current:  746\n",
      "Current:  747\n",
      "Current:  748\n",
      "Current:  749\n",
      "Current:  750\n",
      "Current:  751\n",
      "Current:  752\n",
      "Current:  753\n",
      "Current:  754\n",
      "Current:  755\n",
      "Current:  756\n",
      "Current:  757\n",
      "Current:  758\n",
      "Current:  759\n",
      "Current:  760\n",
      "Current:  761\n",
      "Current:  762\n",
      "Current:  763\n",
      "Current:  764\n",
      "Current:  765\n",
      "Current:  766\n",
      "Current:  767\n",
      "Current:  768\n",
      "Current:  769\n",
      "Current:  770\n",
      "Current:  771\n",
      "Current:  772\n",
      "Current:  773\n",
      "Current:  774\n",
      "Current:  775\n",
      "Current:  776\n",
      "Current:  777\n",
      "Current:  778\n",
      "Current:  779\n",
      "Current:  780\n",
      "Current:  781\n",
      "Current:  782\n",
      "Current:  783\n",
      "Current:  784\n",
      "Current:  785\n",
      "Current:  786\n",
      "Current:  787\n",
      "Current:  788\n",
      "Current:  789\n",
      "Current:  790\n",
      "Current:  791\n",
      "Current:  792\n",
      "Current:  793\n",
      "Current:  794\n",
      "Current:  795\n",
      "Current:  796\n",
      "Current:  797\n",
      "Current:  798\n",
      "Current:  799\n",
      "Current:  800\n",
      "Current:  801\n",
      "Current:  802\n",
      "Current:  803\n",
      "Current:  804\n",
      "Current:  805\n",
      "Current:  806\n",
      "Current:  807\n",
      "Current:  808\n",
      "Current:  809\n",
      "Current:  810\n",
      "Current:  811\n",
      "Current:  812\n",
      "Current:  813\n",
      "Current:  814\n",
      "Current:  815\n",
      "Current:  816\n",
      "Current:  817\n",
      "Current:  818\n",
      "Current:  819\n",
      "Current:  820\n",
      "Current:  821\n",
      "Current:  822\n",
      "Current:  823\n",
      "Current:  824\n",
      "Current:  825\n",
      "Current:  826\n",
      "Current:  827\n",
      "Current:  828\n",
      "Current:  829\n",
      "Current:  830\n",
      "Current:  831\n",
      "Current:  832\n",
      "Current:  833\n",
      "Current:  834\n",
      "Current:  835\n",
      "Current:  836\n",
      "Current:  837\n",
      "Newest best accuracy:  0.6666666666666666\n",
      "Params:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (19, 19), 'batch_size': 48, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest f1 best accuracy:  0.6346031746031747\n",
      "Params f1:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (19, 19), 'batch_size': 48, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest per class best accuracy:  0.6666666666666666\n",
      "Params per class:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (19, 19), 'batch_size': 48, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.6666666666666666\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (19, 19), 'batch_size': 48, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  838\n",
      "Current:  839\n",
      "Current:  840\n",
      "Current:  841\n",
      "Current:  842\n",
      "Current:  843\n",
      "Current:  844\n",
      "Current:  845\n",
      "Current:  846\n",
      "Current:  847\n",
      "Current:  848\n",
      "Current:  849\n",
      "Current:  850\n",
      "Current:  851\n",
      "Current:  852\n",
      "Current:  853\n",
      "Current:  854\n",
      "Current:  855\n",
      "Current:  856\n",
      "Current:  857\n",
      "Current:  858\n",
      "Current:  859\n",
      "Current:  860\n",
      "Current:  861\n",
      "Current:  862\n",
      "Current:  863\n",
      "Current:  864\n",
      "Current:  865\n",
      "Current:  866\n",
      "Current:  867\n",
      "Current:  868\n",
      "Current:  869\n",
      "Current:  870\n",
      "Current:  871\n",
      "Current:  872\n",
      "Current:  873\n",
      "Current:  874\n",
      "Current:  875\n",
      "Current:  876\n",
      "Current:  877\n",
      "Current:  878\n",
      "Current:  879\n",
      "Current:  880\n",
      "Current:  881\n",
      "Current:  882\n",
      "Current:  883\n",
      "Current:  884\n",
      "Current:  885\n",
      "Current:  886\n",
      "Current:  887\n",
      "Current:  888\n",
      "Current:  889\n",
      "Current:  890\n",
      "Current:  891\n",
      "Current:  892\n",
      "Current:  893\n",
      "Current:  894\n",
      "Current:  895\n",
      "Current:  896\n",
      "Current:  897\n",
      "Current:  898\n",
      "Current:  899\n",
      "Current:  900\n",
      "Current:  901\n",
      "Current:  902\n",
      "Current:  903\n",
      "Current:  904\n",
      "Current:  905\n",
      "Current:  906\n",
      "Current:  907\n",
      "Current:  908\n",
      "Current:  909\n",
      "Current:  910\n",
      "Current:  911\n",
      "Current:  912\n",
      "Current:  913\n",
      "Current:  914\n",
      "Current:  915\n",
      "Current:  916\n",
      "Current:  917\n",
      "Current:  918\n",
      "Current:  919\n",
      "Current:  920\n",
      "Current:  921\n",
      "Current:  922\n",
      "Current:  923\n",
      "Current:  924\n",
      "Current:  925\n",
      "Current:  926\n",
      "Current:  927\n",
      "Current:  928\n",
      "Current:  929\n",
      "Current:  930\n",
      "Current:  931\n",
      "Current:  932\n",
      "Current:  933\n",
      "Current:  934\n",
      "Current:  935\n",
      "Current:  936\n",
      "Current:  937\n",
      "Current:  938\n",
      "Current:  939\n",
      "Current:  940\n",
      "Current:  941\n",
      "Current:  942\n",
      "Current:  943\n",
      "Current:  944\n",
      "Current:  945\n",
      "Current:  946\n",
      "Current:  947\n",
      "Current:  948\n",
      "Current:  949\n",
      "Current:  950\n",
      "Current:  951\n",
      "Current:  952\n",
      "Current:  953\n",
      "Current:  954\n",
      "Current:  955\n",
      "Current:  956\n",
      "Current:  957\n",
      "Current:  958\n",
      "Current:  959\n",
      "Current:  960\n",
      "Current:  961\n",
      "Current:  962\n",
      "Current:  963\n",
      "Current:  964\n",
      "Current:  965\n",
      "Current:  966\n",
      "Current:  967\n",
      "Current:  968\n",
      "Current:  969\n",
      "Current:  970\n",
      "Current:  971\n",
      "Current:  972\n",
      "Current:  973\n",
      "Current:  974\n",
      "Current:  975\n",
      "Current:  976\n",
      "Current:  977\n",
      "Current:  978\n",
      "Current:  979\n",
      "Current:  980\n",
      "Current:  981\n",
      "Current:  982\n",
      "Current:  983\n",
      "Current:  984\n",
      "Current:  985\n",
      "Current:  986\n",
      "Current:  987\n",
      "Current:  988\n",
      "Current:  989\n",
      "Current:  990\n",
      "Current:  991\n",
      "Current:  992\n",
      "Current:  993\n",
      "Current:  994\n",
      "Current:  995\n",
      "Current:  996\n",
      "Current:  997\n",
      "Current:  998\n",
      "Current:  999\n",
      "Current:  1000\n",
      "Current:  1001\n",
      "Current:  1002\n",
      "Current:  1003\n",
      "Current:  1004\n",
      "Current:  1005\n",
      "Current:  1006\n",
      "Current:  1007\n",
      "Current:  1008\n",
      "Current:  1009\n",
      "Current:  1010\n",
      "Current:  1011\n",
      "Current:  1012\n",
      "Current:  1013\n",
      "Current:  1014\n",
      "Current:  1015\n",
      "Current:  1016\n",
      "Current:  1017\n",
      "Current:  1018\n",
      "Current:  1019\n",
      "Current:  1020\n",
      "Current:  1021\n",
      "Current:  1022\n",
      "Current:  1023\n",
      "Current:  1024\n",
      "Current:  1025\n",
      "Current:  1026\n",
      "Current:  1027\n",
      "Current:  1028\n",
      "Current:  1029\n",
      "Current:  1030\n",
      "Current:  1031\n",
      "Current:  1032\n",
      "Current:  1033\n",
      "Current:  1034\n",
      "Current:  1035\n",
      "Current:  1036\n",
      "Current:  1037\n",
      "Current:  1038\n",
      "Current:  1039\n",
      "Current:  1040\n",
      "Current:  1041\n",
      "Current:  1042\n",
      "Current:  1043\n",
      "Current:  1044\n",
      "Current:  1045\n",
      "Current:  1046\n",
      "Current:  1047\n",
      "Current:  1048\n",
      "Current:  1049\n",
      "Current:  1050\n",
      "Current:  1051\n",
      "Current:  1052\n",
      "Current:  1053\n",
      "Current:  1054\n",
      "Current:  1055\n",
      "Current:  1056\n",
      "Current:  1057\n",
      "Current:  1058\n",
      "Current:  1059\n",
      "Current:  1060\n",
      "Current:  1061\n",
      "Current:  1062\n",
      "Current:  1063\n",
      "Current:  1064\n",
      "Current:  1065\n",
      "Current:  1066\n",
      "Current:  1067\n",
      "Current:  1068\n",
      "Current:  1069\n",
      "Current:  1070\n",
      "Current:  1071\n",
      "Current:  1072\n",
      "Current:  1073\n",
      "Current:  1074\n",
      "Current:  1075\n",
      "Current:  1076\n",
      "Current:  1077\n",
      "Current:  1078\n",
      "Current:  1079\n",
      "Current:  1080\n",
      "Current:  1081\n",
      "Current:  1082\n",
      "Current:  1083\n",
      "Current:  1084\n",
      "Current:  1085\n",
      "Current:  1086\n",
      "Current:  1087\n",
      "Current:  1088\n",
      "Current:  1089\n",
      "Current:  1090\n",
      "Current:  1091\n",
      "Current:  1092\n",
      "Current:  1093\n",
      "Current:  1094\n",
      "Current:  1095\n",
      "Current:  1096\n",
      "Current:  1097\n",
      "Current:  1098\n",
      "Current:  1099\n",
      "Current:  1100\n",
      "Current:  1101\n",
      "Current:  1102\n",
      "Current:  1103\n",
      "Current:  1104\n",
      "Current:  1105\n",
      "Current:  1106\n",
      "Current:  1107\n",
      "Current:  1108\n",
      "Current:  1109\n",
      "Current:  1110\n",
      "Current:  1111\n",
      "Current:  1112\n",
      "Current:  1113\n",
      "Current:  1114\n",
      "Current:  1115\n",
      "Current:  1116\n",
      "Current:  1117\n",
      "Current:  1118\n",
      "Current:  1119\n",
      "Current:  1120\n",
      "Current:  1121\n",
      "Current:  1122\n",
      "Current:  1123\n",
      "Current:  1124\n",
      "Current:  1125\n",
      "Current:  1126\n",
      "Current:  1127\n",
      "Current:  1128\n",
      "Current:  1129\n",
      "Current:  1130\n",
      "Current:  1131\n",
      "Current:  1132\n",
      "Current:  1133\n",
      "Current:  1134\n",
      "Current:  1135\n",
      "Current:  1136\n",
      "Current:  1137\n",
      "Current:  1138\n",
      "Current:  1139\n",
      "Current:  1140\n",
      "Current:  1141\n",
      "Current:  1142\n",
      "Current:  1143\n",
      "Current:  1144\n",
      "Current:  1145\n",
      "Current:  1146\n",
      "Current:  1147\n",
      "Current:  1148\n",
      "Current:  1149\n",
      "Current:  1150\n",
      "Current:  1151\n",
      "Current:  1152\n",
      "Current:  1153\n",
      "Current:  1154\n",
      "Current:  1155\n",
      "Current:  1156\n",
      "Current:  1157\n",
      "Current:  1158\n",
      "Current:  1159\n",
      "Current:  1160\n",
      "Current:  1161\n",
      "Current:  1162\n",
      "Current:  1163\n",
      "Current:  1164\n",
      "Current:  1165\n",
      "Current:  1166\n",
      "Current:  1167\n",
      "Current:  1168\n",
      "Current:  1169\n",
      "Current:  1170\n",
      "Current:  1171\n",
      "Current:  1172\n",
      "Current:  1173\n",
      "Current:  1174\n",
      "Current:  1175\n",
      "Current:  1176\n",
      "Current:  1177\n",
      "Current:  1178\n",
      "Current:  1179\n",
      "Current:  1180\n",
      "Current:  1181\n",
      "Current:  1182\n",
      "Current:  1183\n",
      "Current:  1184\n",
      "Current:  1185\n",
      "Current:  1186\n",
      "Current:  1187\n",
      "Current:  1188\n",
      "Current:  1189\n",
      "Current:  1190\n",
      "Current:  1191\n",
      "Current:  1192\n",
      "Current:  1193\n",
      "Current:  1194\n",
      "Current:  1195\n",
      "Current:  1196\n",
      "Current:  1197\n",
      "Current:  1198\n",
      "Current:  1199\n",
      "Current:  1200\n",
      "Current:  1201\n",
      "Current:  1202\n",
      "Current:  1203\n",
      "Current:  1204\n",
      "Current:  1205\n",
      "Current:  1206\n",
      "Current:  1207\n",
      "Current:  1208\n",
      "Current:  1209\n",
      "Newest top3 best accuracy:  0.8253968358039856\n",
      "Params top3:  {'sigma': 1, 'grid_size': (20, 20), 'gaussian_size': (35, 35), 'batch_size': 32, 'dropout': 0.5, 'reg_term': 0.001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  1210\n",
      "Current:  1211\n",
      "Current:  1212\n",
      "Current:  1213\n",
      "Current:  1214\n",
      "Current:  1215\n",
      "Current:  1216\n",
      "Current:  1217\n",
      "Current:  1218\n",
      "Current:  1219\n",
      "Current:  1220\n",
      "Current:  1221\n",
      "Current:  1222\n",
      "Current:  1223\n",
      "Current:  1224\n",
      "Current:  1225\n",
      "Current:  1226\n",
      "Current:  1227\n",
      "Current:  1228\n",
      "Current:  1229\n",
      "Current:  1230\n",
      "Current:  1231\n",
      "Current:  1232\n",
      "Current:  1233\n",
      "Current:  1234\n",
      "Current:  1235\n",
      "Current:  1236\n",
      "Current:  1237\n",
      "Current:  1238\n",
      "Current:  1239\n",
      "Current:  1240\n",
      "Current:  1241\n",
      "Current:  1242\n",
      "Current:  1243\n",
      "Current:  1244\n",
      "Current:  1245\n",
      "Current:  1246\n",
      "Current:  1247\n",
      "Current:  1248\n",
      "Current:  1249\n",
      "Current:  1250\n",
      "Current:  1251\n",
      "Current:  1252\n",
      "Current:  1253\n",
      "Current:  1254\n",
      "Current:  1255\n",
      "Current:  1256\n",
      "Current:  1257\n",
      "Current:  1258\n",
      "Current:  1259\n",
      "Current:  1260\n",
      "Current:  1261\n",
      "Current:  1262\n",
      "Current:  1263\n",
      "Current:  1264\n",
      "Current:  1265\n",
      "Current:  1266\n",
      "Current:  1267\n",
      "Current:  1268\n",
      "Current:  1269\n",
      "Current:  1270\n",
      "Current:  1271\n",
      "Current:  1272\n",
      "Current:  1273\n",
      "Current:  1274\n",
      "Current:  1275\n",
      "Current:  1276\n",
      "Current:  1277\n",
      "Current:  1278\n",
      "Current:  1279\n",
      "Current:  1280\n",
      "Current:  1281\n",
      "Current:  1282\n",
      "Current:  1283\n",
      "Current:  1284\n",
      "Current:  1285\n",
      "Current:  1286\n",
      "Current:  1287\n",
      "Current:  1288\n",
      "Current:  1289\n",
      "Current:  1290\n",
      "Current:  1291\n",
      "Current:  1292\n",
      "Current:  1293\n",
      "Current:  1294\n",
      "Current:  1295\n",
      "Current:  1296\n",
      "Current:  1297\n",
      "Current:  1298\n",
      "Current:  1299\n",
      "Current:  1300\n",
      "Current:  1301\n",
      "Current:  1302\n",
      "Current:  1303\n",
      "Current:  1304\n",
      "Current:  1305\n",
      "Current:  1306\n",
      "Current:  1307\n",
      "Current:  1308\n",
      "Current:  1309\n",
      "Current:  1310\n",
      "Current:  1311\n",
      "Current:  1312\n",
      "Current:  1313\n",
      "Current:  1314\n",
      "Current:  1315\n",
      "Current:  1316\n",
      "Current:  1317\n",
      "Current:  1318\n",
      "Current:  1319\n",
      "Current:  1320\n",
      "Current:  1321\n",
      "Current:  1322\n",
      "Current:  1323\n",
      "Current:  1324\n",
      "Current:  1325\n",
      "Current:  1326\n",
      "Current:  1327\n",
      "Current:  1328\n",
      "Current:  1329\n",
      "Current:  1330\n",
      "Current:  1331\n",
      "Current:  1332\n",
      "Current:  1333\n",
      "Current:  1334\n",
      "Current:  1335\n",
      "Current:  1336\n",
      "Current:  1337\n",
      "Current:  1338\n",
      "Current:  1339\n",
      "Current:  1340\n",
      "Current:  1341\n",
      "Current:  1342\n",
      "Current:  1343\n",
      "Current:  1344\n",
      "Current:  1345\n",
      "Current:  1346\n",
      "Current:  1347\n",
      "Current:  1348\n",
      "Current:  1349\n",
      "Current:  1350\n",
      "Current:  1351\n",
      "Current:  1352\n",
      "Current:  1353\n",
      "Current:  1354\n",
      "Current:  1355\n",
      "Current:  1356\n",
      "Current:  1357\n",
      "Current:  1358\n",
      "Current:  1359\n",
      "Current:  1360\n",
      "Current:  1361\n",
      "Current:  1362\n",
      "Current:  1363\n",
      "Current:  1364\n",
      "Current:  1365\n",
      "Current:  1366\n",
      "Current:  1367\n",
      "Current:  1368\n",
      "Current:  1369\n",
      "Current:  1370\n",
      "Current:  1371\n",
      "Current:  1372\n",
      "Current:  1373\n",
      "Current:  1374\n",
      "Current:  1375\n",
      "Current:  1376\n",
      "Current:  1377\n",
      "Current:  1378\n",
      "Current:  1379\n",
      "Current:  1380\n",
      "Current:  1381\n",
      "Current:  1382\n",
      "Current:  1383\n",
      "Current:  1384\n",
      "Current:  1385\n",
      "Current:  1386\n",
      "Current:  1387\n",
      "Current:  1388\n",
      "Current:  1389\n",
      "Current:  1390\n",
      "Current:  1391\n",
      "Current:  1392\n",
      "Current:  1393\n",
      "Current:  1394\n",
      "Current:  1395\n",
      "Current:  1396\n",
      "Current:  1397\n",
      "Current:  1398\n",
      "Current:  1399\n",
      "Current:  1400\n",
      "Current:  1401\n",
      "Current:  1402\n",
      "Current:  1403\n",
      "Current:  1404\n",
      "Current:  1405\n",
      "Current:  1406\n",
      "Current:  1407\n",
      "Current:  1408\n",
      "Current:  1409\n",
      "Current:  1410\n",
      "Current:  1411\n",
      "Current:  1412\n",
      "Current:  1413\n",
      "Current:  1414\n",
      "Current:  1415\n",
      "Current:  1416\n",
      "Current:  1417\n",
      "Current:  1418\n",
      "Current:  1419\n",
      "Current:  1420\n",
      "Current:  1421\n",
      "Current:  1422\n",
      "Current:  1423\n",
      "Current:  1424\n",
      "Current:  1425\n",
      "Current:  1426\n",
      "Current:  1427\n",
      "Current:  1428\n",
      "Current:  1429\n",
      "Current:  1430\n",
      "Current:  1431\n",
      "Current:  1432\n",
      "Current:  1433\n",
      "Current:  1434\n",
      "Current:  1435\n",
      "Current:  1436\n",
      "Current:  1437\n",
      "Current:  1438\n",
      "Current:  1439\n",
      "Current:  1440\n",
      "Current:  1441\n",
      "Current:  1442\n",
      "Current:  1443\n",
      "Current:  1444\n",
      "Current:  1445\n",
      "Current:  1446\n",
      "Current:  1447\n",
      "Current:  1448\n",
      "Current:  1449\n",
      "Current:  1450\n",
      "Current:  1451\n",
      "Current:  1452\n",
      "Current:  1453\n",
      "Current:  1454\n",
      "Current:  1455\n",
      "Current:  1456\n",
      "Current:  1457\n",
      "Current:  1458\n",
      "Current:  1459\n",
      "Current:  1460\n",
      "Current:  1461\n",
      "Current:  1462\n",
      "Current:  1463\n",
      "Current:  1464\n",
      "Current:  1465\n",
      "Current:  1466\n",
      "Current:  1467\n",
      "Current:  1468\n",
      "Current:  1469\n",
      "Current:  1470\n",
      "Current:  1471\n",
      "Current:  1472\n",
      "Current:  1473\n",
      "Current:  1474\n",
      "Current:  1475\n",
      "Current:  1476\n",
      "Current:  1477\n",
      "Current:  1478\n",
      "Current:  1479\n",
      "Current:  1480\n",
      "Current:  1481\n",
      "Current:  1482\n",
      "Current:  1483\n",
      "Current:  1484\n",
      "Current:  1485\n",
      "Current:  1486\n",
      "Current:  1487\n",
      "Current:  1488\n",
      "Current:  1489\n",
      "Current:  1490\n",
      "Current:  1491\n",
      "Current:  1492\n",
      "Current:  1493\n",
      "Current:  1494\n",
      "Current:  1495\n",
      "Current:  1496\n",
      "Current:  1497\n",
      "Current:  1498\n",
      "Current:  1499\n",
      "Current:  1500\n",
      "Current:  1501\n",
      "Current:  1502\n",
      "Current:  1503\n",
      "Current:  1504\n",
      "Current:  1505\n",
      "Current:  1506\n",
      "Current:  1507\n",
      "Current:  1508\n",
      "Current:  1509\n",
      "Current:  1510\n",
      "Current:  1511\n",
      "Current:  1512\n",
      "Current:  1513\n",
      "Current:  1514\n",
      "Current:  1515\n",
      "Current:  1516\n",
      "Current:  1517\n",
      "Current:  1518\n",
      "Current:  1519\n",
      "Current:  1520\n",
      "Current:  1521\n",
      "Current:  1522\n",
      "Current:  1523\n",
      "Current:  1524\n",
      "Current:  1525\n",
      "Current:  1526\n",
      "Current:  1527\n",
      "Current:  1528\n",
      "Current:  1529\n",
      "Current:  1530\n",
      "Current:  1531\n",
      "Current:  1532\n",
      "Current:  1533\n",
      "Current:  1534\n",
      "Current:  1535\n",
      "Current:  1536\n",
      "Current:  1537\n",
      "Current:  1538\n",
      "Current:  1539\n",
      "Current:  1540\n",
      "Current:  1541\n",
      "Current:  1542\n",
      "Current:  1543\n",
      "Current:  1544\n",
      "Current:  1545\n",
      "Current:  1546\n",
      "Current:  1547\n",
      "Current:  1548\n",
      "Current:  1549\n",
      "Current:  1550\n",
      "Current:  1551\n",
      "Current:  1552\n",
      "Current:  1553\n",
      "Current:  1554\n",
      "Current:  1555\n",
      "Current:  1556\n",
      "Current:  1557\n",
      "Current:  1558\n",
      "Current:  1559\n",
      "Current:  1560\n",
      "Current:  1561\n",
      "Current:  1562\n",
      "Current:  1563\n",
      "Current:  1564\n",
      "Current:  1565\n",
      "Current:  1566\n",
      "Current:  1567\n",
      "Current:  1568\n",
      "Current:  1569\n",
      "Current:  1570\n",
      "Current:  1571\n",
      "Current:  1572\n",
      "Current:  1573\n",
      "Current:  1574\n",
      "Current:  1575\n",
      "Current:  1576\n",
      "Current:  1577\n",
      "Current:  1578\n",
      "Current:  1579\n",
      "Current:  1580\n",
      "Current:  1581\n",
      "Current:  1582\n",
      "Current:  1583\n",
      "Current:  1584\n",
      "Current:  1585\n",
      "Current:  1586\n",
      "Current:  1587\n",
      "Current:  1588\n",
      "Current:  1589\n",
      "Current:  1590\n",
      "Current:  1591\n",
      "Current:  1592\n",
      "Current:  1593\n",
      "Current:  1594\n",
      "Current:  1595\n",
      "Current:  1596\n",
      "Current:  1597\n",
      "Current:  1598\n",
      "Current:  1599\n",
      "Current:  1600\n",
      "Current:  1601\n",
      "Current:  1602\n",
      "Current:  1603\n",
      "Current:  1604\n",
      "Current:  1605\n",
      "Current:  1606\n",
      "Current:  1607\n",
      "Current:  1608\n",
      "Current:  1609\n",
      "Current:  1610\n",
      "Current:  1611\n",
      "Current:  1612\n",
      "Current:  1613\n",
      "Current:  1614\n",
      "Current:  1615\n",
      "Current:  1616\n",
      "Current:  1617\n",
      "Current:  1618\n",
      "Current:  1619\n",
      "Current:  1620\n",
      "Current:  1621\n",
      "Current:  1622\n",
      "Current:  1623\n",
      "Current:  1624\n",
      "Current:  1625\n",
      "Current:  1626\n",
      "Current:  1627\n",
      "Current:  1628\n",
      "Current:  1629\n",
      "Current:  1630\n",
      "Current:  1631\n",
      "Current:  1632\n",
      "Current:  1633\n",
      "Current:  1634\n",
      "Current:  1635\n",
      "Current:  1636\n",
      "Current:  1637\n",
      "Current:  1638\n",
      "Current:  1639\n",
      "Current:  1640\n",
      "Current:  1641\n",
      "Current:  1642\n",
      "Current:  1643\n",
      "Current:  1644\n",
      "Current:  1645\n",
      "Current:  1646\n",
      "Current:  1647\n",
      "Current:  1648\n",
      "Current:  1649\n",
      "Current:  1650\n",
      "Current:  1651\n",
      "Current:  1652\n",
      "Current:  1653\n",
      "Current:  1654\n",
      "Current:  1655\n",
      "Current:  1656\n",
      "Current:  1657\n",
      "Current:  1658\n",
      "Current:  1659\n",
      "Current:  1660\n",
      "Current:  1661\n",
      "Current:  1662\n",
      "Current:  1663\n",
      "Current:  1664\n",
      "Current:  1665\n",
      "Current:  1666\n",
      "Current:  1667\n",
      "Current:  1668\n",
      "Current:  1669\n",
      "Current:  1670\n",
      "Current:  1671\n",
      "Current:  1672\n",
      "Current:  1673\n",
      "Current:  1674\n",
      "Current:  1675\n",
      "Current:  1676\n",
      "Current:  1677\n",
      "Current:  1678\n",
      "Current:  1679\n",
      "Current:  1680\n",
      "Current:  1681\n",
      "Current:  1682\n",
      "Current:  1683\n",
      "Current:  1684\n",
      "Current:  1685\n",
      "Current:  1686\n",
      "Current:  1687\n",
      "Current:  1688\n",
      "Current:  1689\n",
      "Current:  1690\n",
      "Current:  1691\n",
      "Current:  1692\n",
      "Current:  1693\n",
      "Current:  1694\n",
      "Current:  1695\n",
      "Current:  1696\n",
      "Current:  1697\n",
      "Current:  1698\n",
      "Current:  1699\n",
      "Current:  1700\n",
      "Current:  1701\n",
      "Current:  1702\n",
      "Current:  1703\n",
      "Current:  1704\n",
      "Current:  1705\n",
      "Current:  1706\n",
      "Current:  1707\n",
      "Current:  1708\n",
      "Current:  1709\n",
      "Current:  1710\n",
      "Current:  1711\n",
      "Current:  1712\n",
      "Current:  1713\n",
      "Current:  1714\n",
      "Current:  1715\n",
      "Current:  1716\n",
      "Current:  1717\n",
      "Current:  1718\n",
      "Current:  1719\n",
      "Current:  1720\n",
      "Current:  1721\n",
      "Current:  1722\n",
      "Current:  1723\n",
      "Current:  1724\n",
      "Current:  1725\n",
      "Current:  1726\n",
      "Current:  1727\n",
      "Current:  1728\n",
      "Current:  1729\n",
      "Current:  1730\n",
      "Current:  1731\n",
      "Current:  1732\n",
      "Current:  1733\n",
      "Current:  1734\n",
      "Current:  1735\n",
      "Current:  1736\n",
      "Current:  1737\n",
      "Current:  1738\n",
      "Current:  1739\n",
      "Current:  1740\n",
      "Current:  1741\n",
      "Current:  1742\n",
      "Current:  1743\n",
      "Current:  1744\n",
      "Current:  1745\n",
      "Current:  1746\n",
      "Current:  1747\n",
      "Current:  1748\n",
      "Current:  1749\n",
      "Current:  1750\n",
      "Current:  1751\n",
      "Current:  1752\n",
      "Current:  1753\n",
      "Current:  1754\n",
      "Current:  1755\n",
      "Current:  1756\n",
      "Current:  1757\n",
      "Current:  1758\n",
      "Current:  1759\n",
      "Current:  1760\n",
      "Current:  1761\n",
      "Current:  1762\n",
      "Current:  1763\n",
      "Current:  1764\n",
      "Current:  1765\n",
      "Current:  1766\n",
      "Current:  1767\n",
      "Current:  1768\n",
      "Current:  1769\n",
      "Current:  1770\n",
      "Current:  1771\n",
      "Current:  1772\n",
      "Current:  1773\n",
      "Current:  1774\n",
      "Current:  1775\n",
      "Current:  1776\n",
      "Current:  1777\n",
      "Current:  1778\n",
      "Current:  1779\n",
      "Current:  1780\n",
      "Current:  1781\n",
      "Current:  1782\n",
      "Current:  1783\n",
      "Current:  1784\n",
      "Current:  1785\n",
      "Current:  1786\n",
      "Current:  1787\n",
      "Current:  1788\n",
      "Current:  1789\n",
      "Current:  1790\n",
      "Current:  1791\n",
      "Current:  1792\n",
      "Current:  1793\n",
      "Current:  1794\n",
      "Current:  1795\n",
      "Current:  1796\n",
      "Current:  1797\n",
      "Current:  1798\n",
      "Current:  1799\n",
      "Current:  1800\n",
      "Current:  1801\n",
      "Current:  1802\n",
      "Current:  1803\n",
      "Current:  1804\n",
      "Current:  1805\n",
      "Current:  1806\n",
      "Current:  1807\n",
      "Current:  1808\n",
      "Current:  1809\n",
      "Current:  1810\n",
      "Current:  1811\n",
      "Current:  1812\n",
      "Current:  1813\n",
      "Current:  1814\n",
      "Current:  1815\n",
      "Current:  1816\n",
      "Current:  1817\n",
      "Current:  1818\n",
      "Current:  1819\n",
      "Current:  1820\n",
      "Current:  1821\n",
      "Current:  1822\n",
      "Current:  1823\n",
      "Current:  1824\n",
      "Current:  1825\n",
      "Current:  1826\n",
      "Current:  1827\n",
      "Current:  1828\n",
      "Current:  1829\n",
      "Current:  1830\n",
      "Current:  1831\n",
      "Current:  1832\n",
      "Current:  1833\n",
      "Current:  1834\n",
      "Current:  1835\n",
      "Current:  1836\n",
      "Current:  1837\n",
      "Current:  1838\n",
      "Current:  1839\n",
      "Current:  1840\n",
      "Current:  1841\n",
      "Current:  1842\n",
      "Current:  1843\n",
      "Current:  1844\n",
      "Current:  1845\n",
      "Current:  1846\n",
      "Current:  1847\n",
      "Current:  1848\n",
      "Current:  1849\n",
      "Current:  1850\n",
      "Current:  1851\n",
      "Current:  1852\n",
      "Current:  1853\n",
      "Current:  1854\n",
      "Current:  1855\n",
      "Current:  1856\n",
      "Current:  1857\n",
      "Current:  1858\n",
      "Current:  1859\n",
      "Current:  1860\n",
      "Current:  1861\n",
      "Current:  1862\n",
      "Current:  1863\n",
      "Current:  1864\n",
      "Current:  1865\n",
      "Current:  1866\n",
      "Current:  1867\n",
      "Current:  1868\n",
      "Current:  1869\n",
      "Current:  1870\n",
      "Current:  1871\n",
      "Current:  1872\n",
      "Current:  1873\n",
      "Current:  1874\n",
      "Current:  1875\n",
      "Current:  1876\n",
      "Current:  1877\n",
      "Current:  1878\n",
      "Current:  1879\n",
      "Current:  1880\n",
      "Current:  1881\n",
      "Current:  1882\n",
      "Current:  1883\n",
      "Current:  1884\n",
      "Current:  1885\n",
      "Current:  1886\n",
      "Current:  1887\n",
      "Current:  1888\n",
      "Current:  1889\n",
      "Current:  1890\n",
      "Current:  1891\n",
      "Current:  1892\n",
      "Current:  1893\n",
      "Current:  1894\n",
      "Current:  1895\n",
      "Current:  1896\n",
      "Current:  1897\n",
      "Current:  1898\n",
      "Current:  1899\n",
      "Current:  1900\n",
      "Current:  1901\n",
      "Current:  1902\n",
      "Current:  1903\n",
      "Current:  1904\n",
      "Current:  1905\n",
      "Current:  1906\n",
      "Current:  1907\n",
      "Current:  1908\n",
      "Current:  1909\n",
      "Current:  1910\n",
      "Current:  1911\n",
      "Current:  1912\n",
      "Current:  1913\n",
      "Current:  1914\n",
      "Current:  1915\n",
      "Current:  1916\n",
      "Current:  1917\n",
      "Current:  1918\n",
      "Current:  1919\n",
      "Current:  1920\n",
      "Current:  1921\n",
      "Current:  1922\n",
      "Current:  1923\n",
      "Current:  1924\n",
      "Current:  1925\n",
      "Current:  1926\n",
      "Current:  1927\n",
      "Current:  1928\n",
      "Current:  1929\n",
      "Current:  1930\n",
      "Current:  1931\n",
      "Current:  1932\n",
      "Current:  1933\n",
      "Current:  1934\n",
      "Current:  1935\n",
      "Current:  1936\n",
      "Current:  1937\n",
      "Current:  1938\n",
      "Current:  1939\n",
      "Current:  1940\n",
      "Current:  1941\n",
      "Current:  1942\n",
      "Current:  1943\n",
      "Current:  1944\n",
      "Current:  1945\n",
      "Current:  1946\n",
      "Current:  1947\n",
      "Current:  1948\n",
      "Current:  1949\n",
      "Current:  1950\n",
      "Current:  1951\n",
      "Current:  1952\n",
      "Current:  1953\n",
      "Current:  1954\n",
      "Current:  1955\n",
      "Current:  1956\n",
      "Current:  1957\n",
      "Current:  1958\n",
      "Current:  1959\n",
      "Current:  1960\n",
      "Current:  1961\n",
      "Current:  1962\n",
      "Current:  1963\n",
      "Current:  1964\n",
      "Current:  1965\n",
      "Current:  1966\n",
      "Current:  1967\n",
      "Current:  1968\n",
      "Current:  1969\n",
      "Current:  1970\n",
      "Current:  1971\n",
      "Current:  1972\n",
      "Current:  1973\n",
      "Current:  1974\n",
      "Current:  1975\n",
      "Current:  1976\n",
      "Current:  1977\n",
      "Current:  1978\n",
      "Current:  1979\n",
      "Current:  1980\n",
      "Current:  1981\n",
      "Current:  1982\n",
      "Current:  1983\n",
      "Current:  1984\n",
      "Current:  1985\n",
      "Current:  1986\n",
      "Current:  1987\n",
      "Current:  1988\n",
      "Current:  1989\n",
      "Current:  1990\n",
      "Current:  1991\n",
      "Current:  1992\n",
      "Current:  1993\n",
      "Current:  1994\n",
      "Current:  1995\n",
      "Current:  1996\n",
      "Current:  1997\n",
      "Current:  1998\n",
      "Current:  1999\n",
      "Current:  2000\n",
      "Current:  2001\n",
      "Current:  2002\n",
      "Current:  2003\n",
      "Current:  2004\n",
      "Current:  2005\n",
      "Current:  2006\n",
      "Current:  2007\n",
      "Current:  2008\n",
      "Current:  2009\n",
      "Current:  2010\n",
      "Current:  2011\n",
      "Current:  2012\n",
      "Current:  2013\n",
      "Current:  2014\n",
      "Current:  2015\n",
      "Current:  2016\n",
      "Current:  2017\n",
      "Current:  2018\n",
      "Current:  2019\n",
      "Current:  2020\n",
      "Current:  2021\n",
      "Current:  2022\n",
      "Current:  2023\n",
      "Current:  2024\n",
      "Current:  2025\n",
      "Current:  2026\n",
      "Current:  2027\n",
      "Current:  2028\n",
      "Current:  2029\n",
      "Current:  2030\n",
      "Current:  2031\n",
      "Current:  2032\n",
      "Current:  2033\n",
      "Current:  2034\n",
      "Current:  2035\n",
      "Current:  2036\n",
      "Current:  2037\n",
      "Current:  2038\n",
      "Current:  2039\n",
      "Current:  2040\n",
      "Current:  2041\n",
      "Current:  2042\n",
      "Current:  2043\n",
      "Current:  2044\n",
      "Current:  2045\n",
      "Current:  2046\n",
      "Current:  2047\n",
      "Current:  2048\n",
      "Current:  2049\n",
      "Current:  2050\n",
      "Current:  2051\n",
      "Current:  2052\n",
      "Current:  2053\n",
      "Current:  2054\n",
      "Current:  2055\n",
      "Current:  2056\n",
      "Current:  2057\n",
      "Current:  2058\n",
      "Current:  2059\n",
      "Current:  2060\n",
      "Current:  2061\n",
      "Current:  2062\n",
      "Current:  2063\n",
      "Current:  2064\n",
      "Current:  2065\n",
      "Current:  2066\n",
      "Current:  2067\n",
      "Current:  2068\n",
      "Current:  2069\n",
      "Current:  2070\n",
      "Current:  2071\n",
      "Current:  2072\n",
      "Current:  2073\n",
      "Current:  2074\n",
      "Current:  2075\n",
      "Current:  2076\n",
      "Current:  2077\n",
      "Current:  2078\n",
      "Current:  2079\n",
      "Current:  2080\n",
      "Current:  2081\n",
      "Current:  2082\n",
      "Current:  2083\n",
      "Current:  2084\n",
      "Current:  2085\n",
      "Current:  2086\n",
      "Current:  2087\n",
      "Current:  2088\n",
      "Current:  2089\n",
      "Current:  2090\n",
      "Current:  2091\n",
      "Current:  2092\n",
      "Current:  2093\n",
      "Current:  2094\n",
      "Current:  2095\n",
      "Current:  2096\n",
      "Current:  2097\n",
      "Current:  2098\n",
      "Current:  2099\n",
      "Current:  2100\n",
      "Current:  2101\n",
      "Current:  2102\n",
      "Current:  2103\n",
      "Current:  2104\n",
      "Current:  2105\n",
      "Current:  2106\n",
      "Current:  2107\n",
      "Current:  2108\n",
      "Current:  2109\n",
      "Current:  2110\n",
      "Current:  2111\n",
      "Current:  2112\n",
      "Current:  2113\n",
      "Current:  2114\n",
      "Current:  2115\n",
      "Current:  2116\n",
      "Current:  2117\n",
      "Current:  2118\n",
      "Current:  2119\n",
      "Current:  2120\n",
      "Current:  2121\n",
      "Current:  2122\n",
      "Current:  2123\n",
      "Current:  2124\n",
      "Current:  2125\n",
      "Current:  2126\n",
      "Current:  2127\n",
      "Current:  2128\n",
      "Current:  2129\n",
      "Current:  2130\n",
      "Current:  2131\n",
      "Current:  2132\n",
      "Current:  2133\n",
      "Current:  2134\n",
      "Current:  2135\n",
      "Current:  2136\n",
      "Current:  2137\n",
      "Current:  2138\n",
      "Current:  2139\n",
      "Current:  2140\n",
      "Current:  2141\n",
      "Current:  2142\n",
      "Current:  2143\n",
      "Current:  2144\n",
      "Current:  2145\n",
      "Current:  2146\n",
      "Current:  2147\n",
      "Current:  2148\n",
      "Current:  2149\n",
      "Current:  2150\n",
      "Current:  2151\n",
      "Current:  2152\n",
      "Current:  2153\n",
      "Current:  2154\n",
      "Current:  2155\n",
      "Current:  2156\n",
      "Current:  2157\n",
      "Current:  2158\n",
      "Current:  2159\n",
      "Current:  2160\n",
      "Current:  2161\n",
      "Current:  2162\n",
      "Current:  2163\n",
      "Current:  2164\n",
      "Current:  2165\n",
      "Current:  2166\n",
      "Current:  2167\n",
      "Current:  2168\n",
      "Current:  2169\n",
      "Current:  2170\n",
      "Current:  2171\n",
      "Current:  2172\n",
      "Current:  2173\n",
      "Current:  2174\n",
      "Current:  2175\n",
      "Current:  2176\n",
      "Current:  2177\n",
      "Current:  2178\n",
      "Current:  2179\n",
      "Current:  2180\n",
      "Current:  2181\n",
      "Current:  2182\n",
      "Current:  2183\n",
      "Current:  2184\n",
      "Current:  2185\n",
      "Current:  2186\n",
      "Current:  2187\n",
      "Current:  2188\n",
      "Current:  2189\n",
      "Current:  2190\n",
      "Current:  2191\n",
      "Current:  2192\n",
      "Current:  2193\n",
      "Current:  2194\n",
      "Current:  2195\n",
      "Current:  2196\n",
      "Current:  2197\n",
      "Current:  2198\n",
      "Current:  2199\n",
      "Current:  2200\n",
      "Current:  2201\n",
      "Current:  2202\n",
      "Newest best accuracy:  0.6825396825396826\n",
      "Params:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest f1 best accuracy:  0.6431746031746033\n",
      "Params f1:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest per class best accuracy:  0.6777777777777777\n",
      "Params per class:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Newest balanced best accuracy:  0.6777777777777777\n",
      "Params balanced accuracy:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n",
      "Current:  2203\n",
      "Current:  2204\n",
      "Current:  2205\n",
      "Current:  2206\n",
      "Current:  2207\n",
      "Current:  2208\n",
      "Current:  2209\n",
      "Current:  2210\n",
      "Current:  2211\n",
      "Current:  2212\n",
      "Current:  2213\n",
      "Current:  2214\n",
      "Current:  2215\n",
      "Current:  2216\n",
      "Current:  2217\n",
      "Current:  2218\n",
      "Current:  2219\n",
      "Current:  2220\n",
      "Current:  2221\n",
      "Current:  2222\n",
      "Current:  2223\n",
      "Current:  2224\n",
      "Current:  2225\n",
      "Current:  2226\n",
      "Current:  2227\n",
      "Current:  2228\n",
      "Current:  2229\n",
      "Current:  2230\n",
      "Current:  2231\n",
      "Current:  2232\n",
      "Current:  2233\n",
      "Current:  2234\n",
      "Current:  2235\n",
      "Current:  2236\n",
      "Current:  2237\n",
      "Current:  2238\n",
      "Current:  2239\n",
      "Current:  2240\n",
      "Current:  2241\n",
      "Current:  2242\n",
      "Current:  2243\n",
      "Current:  2244\n",
      "Current:  2245\n",
      "Current:  2246\n",
      "Current:  2247\n",
      "Current:  2248\n",
      "Current:  2249\n",
      "Current:  2250\n",
      "Current:  2251\n",
      "Current:  2252\n",
      "Current:  2253\n",
      "Current:  2254\n",
      "Current:  2255\n",
      "Current:  2256\n",
      "Current:  2257\n",
      "Current:  2258\n",
      "Current:  2259\n",
      "Current:  2260\n",
      "Current:  2261\n",
      "Current:  2262\n",
      "Current:  2263\n",
      "Current:  2264\n",
      "Current:  2265\n",
      "Current:  2266\n",
      "Current:  2267\n",
      "Current:  2268\n",
      "Current:  2269\n",
      "Current:  2270\n",
      "Current:  2271\n",
      "Current:  2272\n",
      "Current:  2273\n",
      "Current:  2274\n",
      "Current:  2275\n",
      "Current:  2276\n",
      "Current:  2277\n",
      "Current:  2278\n",
      "Current:  2279\n",
      "Current:  2280\n",
      "Current:  2281\n",
      "Current:  2282\n",
      "Current:  2283\n",
      "Current:  2284\n",
      "Current:  2285\n",
      "Current:  2286\n",
      "Current:  2287\n",
      "Current:  2288\n",
      "Current:  2289\n",
      "Current:  2290\n",
      "Current:  2291\n",
      "Current:  2292\n",
      "Current:  2293\n",
      "Current:  2294\n",
      "Current:  2295\n",
      "Current:  2296\n",
      "Current:  2297\n",
      "Current:  2298\n",
      "Current:  2299\n",
      "Current:  2300\n",
      "Current:  2301\n",
      "Current:  2302\n",
      "Current:  2303\n",
      "Current:  2304\n",
      "Current:  2305\n",
      "Current:  2306\n",
      "Current:  2307\n",
      "Current:  2308\n",
      "Current:  2309\n",
      "Current:  2310\n",
      "Current:  2311\n",
      "Current:  2312\n",
      "Current:  2313\n",
      "Current:  2314\n",
      "Current:  2315\n",
      "Current:  2316\n",
      "Current:  2317\n",
      "Current:  2318\n",
      "Current:  2319\n",
      "Current:  2320\n",
      "Current:  2321\n",
      "Current:  2322\n",
      "Current:  2323\n",
      "Current:  2324\n",
      "Current:  2325\n",
      "Current:  2326\n",
      "Current:  2327\n",
      "Current:  2328\n",
      "Current:  2329\n",
      "Current:  2330\n",
      "Current:  2331\n",
      "Current:  2332\n",
      "Current:  2333\n",
      "Current:  2334\n",
      "Current:  2335\n",
      "Current:  2336\n",
      "Current:  2337\n",
      "Current:  2338\n",
      "Current:  2339\n",
      "Current:  2340\n",
      "Current:  2341\n",
      "Current:  2342\n",
      "Current:  2343\n",
      "Current:  2344\n",
      "Current:  2345\n",
      "Current:  2346\n",
      "Current:  2347\n",
      "Current:  2348\n",
      "Current:  2349\n",
      "Current:  2350\n",
      "Current:  2351\n",
      "Current:  2352\n",
      "Current:  2353\n",
      "Current:  2354\n",
      "Current:  2355\n",
      "Current:  2356\n",
      "Current:  2357\n",
      "Current:  2358\n",
      "Current:  2359\n",
      "Current:  2360\n",
      "Current:  2361\n",
      "Current:  2362\n",
      "Current:  2363\n",
      "Current:  2364\n",
      "Current:  2365\n",
      "Current:  2366\n",
      "Current:  2367\n",
      "Current:  2368\n",
      "Current:  2369\n",
      "Current:  2370\n",
      "Current:  2371\n",
      "Current:  2372\n",
      "Current:  2373\n",
      "Current:  2374\n",
      "Current:  2375\n",
      "Current:  2376\n",
      "Current:  2377\n",
      "Current:  2378\n",
      "Current:  2379\n",
      "Current:  2380\n",
      "Current:  2381\n",
      "Current:  2382\n",
      "Current:  2383\n",
      "Current:  2384\n",
      "Current:  2385\n",
      "Current:  2386\n",
      "Current:  2387\n",
      "Current:  2388\n",
      "Current:  2389\n",
      "Current:  2390\n",
      "Current:  2391\n",
      "Current:  2392\n",
      "Current:  2393\n",
      "Current:  2394\n",
      "Current:  2395\n",
      "Current:  2396\n",
      "Current:  2397\n",
      "Current:  2398\n",
      "Current:  2399\n",
      "Current:  2400\n",
      "FINAL Best accuracy:  0.6825396825396826\n",
      "FINAL Params:  {'sigma': 1, 'grid_size': (30, 30), 'gaussian_size': (43, 43), 'batch_size': 128, 'dropout': 0.5, 'reg_term': 0.0001, 'lr': 0.001, 'patience': 30, 'min_lr': 1e-06, 'factor': 0.2, 'test_size': 0.1}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import collections\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import math\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import csv\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, balanced_accuracy_score\n",
    "import gc\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout\n",
    "\n",
    "sigmas = [1]\n",
    "grid_sizes = [(20, 20), (30, 30), (40, 40),(50, 50), (60, 60)]\n",
    "#grid_sizes = [(20, 20)]\n",
    "gaussian_sizes = [(1, 1), (19, 19), (35, 35), (43, 43)]\n",
    "#gaussian_sizes = [(19, 19)]\n",
    "batch_sizes = [32, 48, 128, 176]\n",
    "#batch_sizes = [176]\n",
    "dropouts = [0.5]\n",
    "reg_terms = [0.001, 0.0001]\n",
    "#reg_terms = [0.0001]\n",
    "learning_rates = [1e-3]\n",
    "patiences = [30]\n",
    "min_lrs = [1e-6]\n",
    "factors = [0.2]\n",
    "#test_sizes = [0.1, 0.15, 0.2]\n",
    "test_sizes = [0.1]\n",
    "results_folder = \"best MLP model CEPAJ\"\n",
    "\n",
    "# Create the folder if it does not exist\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_top3_accuracy = 0\n",
    "best_f1_accuracy = 0\n",
    "best_per_class_accuracy = 0\n",
    "best_balanced_accuracy = 0\n",
    "best_params = {}\n",
    "best_params_top3 = {}\n",
    "best_params_f1 = {}\n",
    "best_params_per_class = {}\n",
    "best_params_balanced_accuracy = {}\n",
    "                                            \n",
    "distance = 610\n",
    "h_res = 1920\n",
    "v_res = 1080\n",
    "screen_w = 527\n",
    "screen_h = 296\n",
    "\n",
    "current = 0\n",
    "\n",
    "                                            \n",
    "                                            \n",
    "\n",
    "def compute_ppda(distance, h_res, v_res, screen_w, screen_h):\n",
    "    \"\"\"\n",
    "    Compute the number of pixels per degree of visual angle based on the experimental conditions.\n",
    "    \n",
    "    :param distance: int, the distance between the observer and the screen (in mm)\n",
    "    :param h_res: int, the horizontal resolution of the screen\n",
    "    :param v_res: int, the vertical resolution of the screen\n",
    "    :param screen_w: int, the width of the screen (in mm)\n",
    "    :param screen_h: int, the height of the screen (in mm)\n",
    "    :return horizontal_ppda: float, the number of pixel per degree of visual angle\n",
    "    \"\"\"\n",
    "    pxl_density_x = h_res / screen_w\n",
    "    pxl_density_y = v_res / screen_h\n",
    "    \n",
    "    d = 2 * distance * math.tan(np.deg2rad(0.5))\n",
    "    horizontal_ppda = d * ((pxl_density_x + pxl_density_y) / 2)\n",
    "    \n",
    "    return horizontal_ppda\n",
    "                                            \n",
    "ppda = compute_ppda(distance, h_res, v_res, screen_w, screen_h)\n",
    "\n",
    "for test_size in test_sizes:\n",
    "    for sigma in sigmas:\n",
    "        for factor in factors:\n",
    "            for gaussian_size in gaussian_sizes:\n",
    "                for batch_size in batch_sizes:\n",
    "                    for dropout in dropouts:\n",
    "                        for reg_term in reg_terms:\n",
    "                            for lr in learning_rates:\n",
    "                                for patience in patiences:\n",
    "                                    for min_lr in min_lrs:\n",
    "                                        for grid_size in grid_sizes:\n",
    "\n",
    "                                            def checkObserverRemembered(observer, image_path, base_dir):\n",
    "                                                csv_file_path = os.path.join(base_dir, \"..\" ,\"hit_status.csv\")\n",
    "                                                if not os.path.isfile(csv_file_path):\n",
    "                                                    print(\"Error: CSV file not found.\")\n",
    "                                                    return False\n",
    "                                                df = pd.read_csv(csv_file_path)\n",
    "                                                filtered_rows = df[(df['Setup Folder'] == observer) & (df['Image Path'] == image_path) & (df['Hit'] == 1)]\n",
    "                                                if not filtered_rows.empty:\n",
    "                                                    return True\n",
    "                                                else:\n",
    "                                                    return False\n",
    "\n",
    "                                            def bin_fixations(fixation_map):\n",
    "                                                global grid_size\n",
    "                                                height, width = fixation_map.shape\n",
    "                                                binned_map = np.zeros(grid_size)\n",
    "                                            \n",
    "                                                bin_height = height // grid_size[0]\n",
    "                                                bin_width = width // grid_size[1]\n",
    "                                            \n",
    "                                                for i in range(grid_size[0]):\n",
    "                                                    for j in range(grid_size[1]):\n",
    "                                                        bin_area = fixation_map[i*bin_height:(i+1)*bin_height, j*bin_width:(j+1)*bin_width]\n",
    "                                                        binned_map[i, j] = np.sum(bin_area)\n",
    "                                                        #ili avg?\n",
    "                                            \n",
    "                                                return binned_map\n",
    "                                            \n",
    "                                            def normalize_map(binned_map):\n",
    "                                                return binned_map / np.sum(binned_map)\n",
    "                                                #return binned_map\n",
    "                                            \n",
    "                                            def smooth_map(binned_map):\n",
    "                                                global sigma\n",
    "                                                return gaussian_filter(binned_map, sigma=sigma)\n",
    "                                            \n",
    "                                            def process_fixation_map(fixation_map):\n",
    "                                                binned_map = bin_fixations(fixation_map)\n",
    "                                                normalized_map = normalize_map(binned_map)\n",
    "                                                smoothed_map = smooth_map(normalized_map)\n",
    "                                                return smoothed_map\n",
    "                                            \n",
    "                                            def get_current_fixation_map(image_path, coordinates):\n",
    "                                                image = cv2.imread(image_path)\n",
    "                                                if image is None:\n",
    "                                                    print(f\"Image at {image_path} not found.\")\n",
    "                                                    return\n",
    "                                            \n",
    "                                                coordinates = coordinates[0:120]\n",
    "                                              \n",
    "                                                fixation_map = np.zeros((1080, 1920), dtype=np.float32)\n",
    "                                            \n",
    "                                                # Convert coordinates to pixel coordinates and update the saliency map\n",
    "                                                for x_norm, y_norm in coordinates:\n",
    "                                                    # Scale normalized coordinates to pixel coordinates for the 1920x1080 screen\n",
    "                                                    x = int(((x_norm + 1) / 2 ) * 1920)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "                                                    y = int((y_norm + 0.5) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "                                                    # Update the saliency map if coordinates are within the screen\n",
    "                                                    if 0 <= x < 1920 and 0 <= y < 1080:\n",
    "                                                        fixation_map[y, x] += 1 \n",
    "                                            \n",
    "                                            \n",
    "                                                    '''\n",
    "                                                    # Scale normalized coordinates to pixel coordinates for the 1920x1080 screen\n",
    "                                                    if(x_norm >0 and y_norm >0):\n",
    "                                                        x = int((x_norm + 1 + 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "                                                        y = int((y_norm + 0.5 + 0.05) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "                                                    if(x_norm >0 and y_norm <0):\n",
    "                                                        x = int((x_norm + 1 + 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "                                                        y = int((y_norm + 0.5 - 0.1) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "                                                    if(x_norm <0 and y_norm >0):\n",
    "                                                        x = int((x_norm + 1 - 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "                                                        y = int((y_norm + 0.5 + 0.05) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "                                                    if(x_norm <0 and y_norm <0):\n",
    "                                                        x = int((x_norm + 1 - 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "                                                        y = int((y_norm + 0.5 - 0.1) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "                                                    # Update the saliency map if coordinates are within the screen\n",
    "                                                    if 0 <= x < 1920 and 0 <= y < 1080:\n",
    "                                                        fixation_map[y, x] += 1'''\n",
    "                                                #sigma = ppda / np.sqrt(2)\n",
    "                                                #fixation_map = gaussian_filter(fixation_map, sigma = sigma)\n",
    "                                                fixation_map = cv2.GaussianBlur(fixation_map, gaussian_size, 0)\n",
    "                                                # Crop the saliency map to the 700x700 region\n",
    "                                                fixation_map = fixation_map[190:890, 610:1310]\n",
    "                                                # flip the Y coordinates\n",
    "                                                fixation_map = np.flipud(fixation_map)\n",
    "                                                return fixation_map\n",
    "                                            \n",
    "                                            def normalize_fixation_map(fixation_map):\n",
    "                                                min_val = np.min(fixation_map)\n",
    "                                                max_val = np.max(fixation_map)\n",
    "                                                normalized_fixation_map = (fixation_map - min_val) / (max_val - min_val) * 255\n",
    "                                                return normalized_fixation_map\n",
    "                                            \n",
    "                                            # 90experiments folder\n",
    "                                            base_dir = os.path.abspath(os.path.join(os.getcwd(),\"..\", \"90experiments\"))\n",
    "                                            \n",
    "                                            fixation_maps = {}  # Dictionary to store fixation maps for each imagePath\n",
    "                                            \n",
    "                                            \n",
    "                                            for folder in os.listdir(base_dir):\n",
    "                                                folder_path = os.path.join(base_dir, folder)\n",
    "                                                if not os.path.isdir(folder_path):\n",
    "                                                    continue\n",
    "                                                match = re.search(r'\\d{1,2}$', folder)\n",
    "                                                if match:\n",
    "                                                    observer = int(match.group())\n",
    "                                                #if(observer != 5):\n",
    "                                                #    continue\n",
    "                                                if(observer == 1 or observer == 2 or observer == 49 or observer == 50 or observer == 5):\n",
    "                                                    continue\n",
    "                                            \n",
    "                                                #if(observer not in [70,71,73,74,76,77,79,80,82,83,85,87,88,89,86,90,18,57,6,45,48,60,63,69,3,9,12,21,15,27,30,33,36,42,24,66,51,54,72,75]):\n",
    "                                                #    continue\n",
    "                                                    \n",
    "                                                csv_file_path = os.path.join(folder_path, \"eye_tracker_data.csv\")\n",
    "                                                if not os.path.isfile(csv_file_path):\n",
    "                                                    continue\n",
    "                                                data = pd.read_csv(csv_file_path)\n",
    "                                            \n",
    "                                                filtered_data = data[data['ImagePath'].str.startswith('targetImages')]\n",
    "                                                \n",
    "                                                uniqueImagePaths = []\n",
    "                                                delete_rows = []\n",
    "                                            \n",
    "                                                #get only the eye-tracking data from the first viewing\n",
    "                                                index = 0\n",
    "                                                \n",
    "                                                row = filtered_data.iloc[index]\n",
    "                                                while len(uniqueImagePaths) < 10:\n",
    "                                                    row = filtered_data.iloc[index]\n",
    "                                                    if(row['ImagePath'] not in uniqueImagePaths):\n",
    "                                                        uniqueImagePaths.append(row['ImagePath'])\n",
    "                                                        lastImagePath = row['ImagePath']\n",
    "                                                        index +=1\n",
    "                                                    elif(row['ImagePath'] in uniqueImagePaths):\n",
    "                                                        index += 1    \n",
    "                                                row = filtered_data.iloc[index]\n",
    "                                            \n",
    "                                                while(row['ImagePath'] == lastImagePath):\n",
    "                                                    index +=1\n",
    "                                                    row = filtered_data.iloc[index]\n",
    "                                            \n",
    "                                                filtered_data.reset_index(drop=True, inplace=True)\n",
    "                                                filtered_data = filtered_data.iloc[:index].copy()\n",
    "                                                \n",
    "                                                grouped = filtered_data.groupby('ImagePath')\n",
    "                                            \n",
    "                                                # Generate and save fixation maps for each image in the current folder\n",
    "                                                for image_path, group in grouped:\n",
    "                                                    # Construct full image path by going one directory back from base_dir\n",
    "                                                    full_image_path = os.path.abspath(os.path.join(base_dir, \"..\", image_path))\n",
    "                                                    full_image_path = full_image_path.replace('\\\\', '/')\n",
    "                                            \n",
    "                                                    #check if current observer has remembered this image, if not, continue\n",
    "                                                    if(not checkObserverRemembered(observer, image_path, base_dir)):\n",
    "                                                        continue\n",
    "                                                    \n",
    "                                                    # Extract coordinates\n",
    "                                                    coordinates = group[['PosX', 'PosY']].values\n",
    "                                            \n",
    "                                                    current_fixation_map = get_current_fixation_map (full_image_path, coordinates)\n",
    "                                                    if(np.all(current_fixation_map == 0)):\n",
    "                                                        continue\n",
    "                                            \n",
    "                                                    current_fixation_map_20x20 = process_fixation_map(current_fixation_map)\n",
    "                                                    current_fixation_map_20x20 = normalize_fixation_map(current_fixation_map_20x20)\n",
    "                                                    \n",
    "                                                    #current_fixation_map_20x20 = (current_fixation_map)\n",
    "                                                    #add to dictionary or update it\n",
    "                                                    if image_path not in fixation_maps:\n",
    "                                                        fixation_maps[image_path] = [current_fixation_map_20x20]\n",
    "                                                    else:\n",
    "                                                        fixation_maps[image_path].append(current_fixation_map_20x20)\n",
    "                                                        \n",
    "                                            # Flatten the fixation maps and standardize them\n",
    "                                            all_fixation_maps = []\n",
    "                                            labels = []\n",
    "                                            \n",
    "                                            for image_path, maps in fixation_maps.items():\n",
    "                                                for fixation_map in maps:\n",
    "                                                    all_fixation_maps.append(fixation_map.flatten())\n",
    "                                                    labels.append(image_path)\n",
    "                                            \n",
    "                                            X = np.array(all_fixation_maps)\n",
    "                                            y = np.array(labels)\n",
    "      \n",
    "                                            #training\n",
    "                                            from tensorflow.keras.models import Sequential\n",
    "                                            from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "                                            from tensorflow.keras.optimizers import Adam\n",
    "                                            from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "                                            from sklearn.metrics import classification_report, accuracy_score\n",
    "                                            from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "                                            from tensorflow.keras.regularizers import l2\n",
    "                                            from imblearn.over_sampling import SMOTE\n",
    "                                            import collections\n",
    "                                            import numpy as np\n",
    "                                            import matplotlib.pyplot as plt\n",
    "                                            import json\n",
    "                                            \n",
    "                                            trainings = 15\n",
    "                                            \n",
    "                                            for i in range(trainings):\n",
    "                                                X = X.reshape(-1, grid_size[0], grid_size[1], 1)\n",
    "                                            \n",
    "                                                label_encoder = LabelEncoder()\n",
    "                                                y_encoded = label_encoder.fit_transform(y)\n",
    "                                                label_names = label_encoder.classes_\n",
    "                                                \n",
    "                                                X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=test_size, stratify=y_encoded)\n",
    "                                                \n",
    "                                                y_train_categorical = to_categorical(y_train, num_classes=len(label_encoder.classes_))\n",
    "                                                y_test_categorical = to_categorical(y_test, num_classes=len(label_encoder.classes_))\n",
    "                                                \n",
    "                                                '''\n",
    "                                                # Flatten X_train for SMOTE\n",
    "                                                X_train_flatten = X_train.reshape(X_train.shape[0], -1)\n",
    "                                                \n",
    "                                                # Apply SMOTE to balance the dataset\n",
    "                                                smote = SMOTE()\n",
    "                                                X_train_resampled, y_train_resampled = smote.fit_resample(X_train_flatten, np.argmax(y_train_categorical, axis=1))\n",
    "                                                \n",
    "                                                # Reshape X_train back to original shape\n",
    "                                                X_train_resampled = X_train_resampled.reshape(-1, grid_size[0], grid_size[1], 1)\n",
    "                                                y_train_resampled_categorical = np.eye(len(label_encoder.classes_))[y_train_resampled]\n",
    "                                                \n",
    "                                                # Calculate class weights\n",
    "                                                class_counts = collections.Counter(np.argmax(y_train_resampled_categorical, axis=1))\n",
    "                                                total_samples = sum(class_counts.values())\n",
    "                                                class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "                                                #print(\"Class counts:\", class_counts)\n",
    "                                                #print(\"Class weights:\", class_weights)\n",
    "                                                '''\n",
    "                                                # Define the MLP model\n",
    "                                                \n",
    "                                                model = Sequential([\n",
    "                                                    Input(shape=(grid_size[0], grid_size[1], 1)),\n",
    "                                                    Flatten(),\n",
    "                                                    Dense(512, activation='relu', kernel_regularizer=l2(reg_term)),\n",
    "                                                    BatchNormalization(),\n",
    "                                                    Dropout(dropout),\n",
    "                                                    Dense(256, activation='relu', kernel_regularizer=l2(reg_term)),\n",
    "                                                    BatchNormalization(),\n",
    "                                                    Dropout(dropout),\n",
    "                                                    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "                                                ])\n",
    "                                                '''input_size = (32, 32)\n",
    "                                                X_train = np.array([img_to_array(array_to_img(np.repeat(img, 3, axis=2)).resize(input_size)) for img in X_train])\n",
    "                                                X_test = np.array([img_to_array(array_to_img(np.repeat(img, 3, axis=2)).resize(input_size)) for img in X_test])\n",
    "\n",
    "                                                base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "                                                model = Sequential([\n",
    "                                                    base_model,\n",
    "                                                    GlobalAveragePooling2D(),\n",
    "                                                    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "                                                    Dropout(0.5),\n",
    "                                                    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "                                                ])'''\n",
    "                                                \n",
    "                                                # Compile the model\n",
    "                                                model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3)])\n",
    "                                                \n",
    "                                                # Callbacks for learning rate adjustment and early stopping\n",
    "                                                reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=factor, patience=patience, min_lr=min_lr)\n",
    "                                                early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "                                                model_checkpoint = ModelCheckpoint(os.path.join(results_folder, 'best_model_brute_force.keras'), monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "                                                \n",
    "                                                \n",
    "                                                # Train the model\n",
    "                                                history = model.fit(X_train, y_train_categorical,\n",
    "                                                                    validation_data=(X_test, y_test_categorical),\n",
    "                                                                    epochs=1000,\n",
    "                                                                    callbacks=[reduce_lr, early_stopping, model_checkpoint],\n",
    "                                                                    #class_weight=class_weights,\n",
    "                                                                    batch_size = batch_size,\n",
    "                                                                    verbose = 0\n",
    "                                                                    )\n",
    "                                                \n",
    "                                                # Evaluate the model\n",
    "                                                y_pred_prob = model.predict(X_test, verbose = 0)\n",
    "                                                y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "                                                \n",
    "                                                # Calculate accuracy\n",
    "                                                accuracy = accuracy_score(y_test, y_pred)\n",
    "                                                top_3_accuracy = history.history['val_top_k_categorical_accuracy'][-1]\n",
    "                                                f1 = f1_score(y_test, y_pred, average='macro')  # You can use 'micro' or 'weighted' based on your needs\n",
    "                                                conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "                                                per_class_accuracy = np.mean(conf_matrix.diagonal() / conf_matrix.sum(axis=1))\n",
    "                                                balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "                                                    \n",
    "                                                if(accuracy > best_accuracy):\n",
    "                                                    best_accuracy = accuracy\n",
    "                                                    best_params = {\n",
    "                                                                   'sigma':sigma, \n",
    "                                                                   'grid_size':grid_size, \n",
    "                                                                   'gaussian_size':gaussian_size,\n",
    "                                                                   'batch_size':batch_size,\n",
    "                                                                   'dropout':dropout,\n",
    "                                                                   'reg_term':reg_term,\n",
    "                                                                   'lr':lr,\n",
    "                                                                   'patience':patience,\n",
    "                                                                   'min_lr':min_lr,\n",
    "                                                                   'factor':factor,\n",
    "                                                                   'test_size': test_size}\n",
    "                                                    print(\"Newest best accuracy: \", best_accuracy)\n",
    "                                                    print(\"Params: \", best_params)\n",
    "                                                    # Save the best model, accuracy and top-3 accuracy\n",
    "                                                    with open(os.path.join(results_folder, \"best_accuracy_MLP_search_accuracy.txt\"), \"w\") as f:\n",
    "                                                        f.write(f\"Best accuracy: {best_accuracy:.4f}\\n\")\n",
    "                                                        f.write(\"Best parameters:\\n\")\n",
    "                                                        f.write(json.dumps(best_params, indent=4))\n",
    "                                                    # Define the path to save the model\n",
    "                                                    model_save_path = os.path.join(results_folder, 'best_model.keras')\n",
    "                                                    # Save the model\n",
    "                                                    model.save(model_save_path)\n",
    "                                                        \n",
    "                                                if(top_3_accuracy > best_top3_accuracy):\n",
    "                                                    best_top3_accuracy = top_3_accuracy\n",
    "                                                    best_params_top3 = {\n",
    "                                                                   'sigma':sigma, \n",
    "                                                                   'grid_size':grid_size, \n",
    "                                                                   'gaussian_size':gaussian_size,\n",
    "                                                                   'batch_size':batch_size,\n",
    "                                                                   'dropout':dropout,\n",
    "                                                                   'reg_term':reg_term,\n",
    "                                                                   'lr':lr,\n",
    "                                                                   'patience':patience,\n",
    "                                                                   'min_lr':min_lr,\n",
    "                                                                   'factor':factor,\n",
    "                                                                   'test_size': test_size}\n",
    "                                                    print(\"Newest top3 best accuracy: \", best_top3_accuracy)\n",
    "                                                    print(\"Params top3: \", best_params_top3)\n",
    "                                                    # Save the best model, accuracy and top-3 accuracy\n",
    "                                                    with open(os.path.join(results_folder, \"best_accuracy_MLP_search_top3_accuracy.txt\"), \"w\") as f:\n",
    "                                                        f.write(f\"Top 3 accuracy: {best_top3_accuracy:.4f}\\n\")\n",
    "                                                        f.write(\"Best top3 parameters:\\n\")\n",
    "                                                        f.write(json.dumps(best_params_top3, indent=4))\n",
    "                                                    # Define the path to save the model\n",
    "                                                    model_save_path = os.path.join(results_folder, 'top3_model.keras')\n",
    "                                                    # Save the model\n",
    "                                                    model.save(model_save_path)\n",
    "\n",
    "                                                if(f1 > best_f1_accuracy):\n",
    "                                                    best_f1_accuracy = f1\n",
    "                                                    best_params_f1 = {\n",
    "                                                                   'sigma':sigma, \n",
    "                                                                   'grid_size':grid_size, \n",
    "                                                                   'gaussian_size':gaussian_size,\n",
    "                                                                   'batch_size':batch_size,\n",
    "                                                                   'dropout':dropout,\n",
    "                                                                   'reg_term':reg_term,\n",
    "                                                                   'lr':lr,\n",
    "                                                                   'patience':patience,\n",
    "                                                                   'min_lr':min_lr,\n",
    "                                                                   'factor':factor,\n",
    "                                                                   'test_size': test_size}\n",
    "                                                    print(\"Newest f1 best accuracy: \", best_f1_accuracy)\n",
    "                                                    print(\"Params f1: \", best_params_f1)\n",
    "                                                    # Save the best model, accuracy and top-3 accuracy\n",
    "                                                    with open(os.path.join(results_folder, \"best_accuracy_MLP_search_f1_accuracy.txt\"), \"w\") as f:\n",
    "                                                        f.write(f\"Top f1 accuracy: {best_f1_accuracy:.4f}\\n\")\n",
    "                                                        f.write(\"Best f1 parameters:\\n\")\n",
    "                                                        f.write(json.dumps(best_params_f1, indent=4))\n",
    "                                                    # Define the path to save the model\n",
    "                                                    model_save_path = os.path.join(results_folder, 'f1_model.keras')\n",
    "                                                    # Save the model\n",
    "                                                    model.save(model_save_path)\n",
    "\n",
    "                                                if(per_class_accuracy > best_per_class_accuracy):\n",
    "                                                    best_per_class_accuracy = per_class_accuracy\n",
    "                                                    best_params_per_class = {\n",
    "                                                                   'sigma':sigma, \n",
    "                                                                   'grid_size':grid_size, \n",
    "                                                                   'gaussian_size':gaussian_size,\n",
    "                                                                   'batch_size':batch_size,\n",
    "                                                                   'dropout':dropout,\n",
    "                                                                   'reg_term':reg_term,\n",
    "                                                                   'lr':lr,\n",
    "                                                                   'patience':patience,\n",
    "                                                                   'min_lr':min_lr,\n",
    "                                                                   'factor':factor,\n",
    "                                                                   'test_size': test_size}\n",
    "                                                    print(\"Newest per class best accuracy: \", best_per_class_accuracy)\n",
    "                                                    print(\"Params per class: \", best_params_per_class)\n",
    "                                                    # Save the best model, accuracy and top-3 accuracy\n",
    "                                                    with open(os.path.join(results_folder, \"best_accuracy_MLP_search_per_class_accuracy.txt\"), \"w\") as f:\n",
    "                                                        f.write(f\"Top per_class accuracy: {best_per_class_accuracy:.4f}\\n\")\n",
    "                                                        f.write(\"Best per class parameters:\\n\")\n",
    "                                                        f.write(json.dumps(best_params_per_class, indent=4))\n",
    "                                                    # Define the path to save the model\n",
    "                                                    model_save_path = os.path.join(results_folder, 'per_class_model.keras')\n",
    "                                                    # Save the model\n",
    "                                                    model.save(model_save_path)\n",
    "\n",
    "                                                if(balanced_accuracy > best_balanced_accuracy):\n",
    "                                                    best_balanced_accuracy = balanced_accuracy\n",
    "                                                    best_params_balanced = {\n",
    "                                                                   'sigma':sigma, \n",
    "                                                                   'grid_size':grid_size, \n",
    "                                                                   'gaussian_size':gaussian_size,\n",
    "                                                                   'batch_size':batch_size,\n",
    "                                                                   'dropout':dropout,\n",
    "                                                                   'reg_term':reg_term,\n",
    "                                                                   'lr':lr,\n",
    "                                                                   'patience':patience,\n",
    "                                                                   'min_lr':min_lr,\n",
    "                                                                   'factor':factor,\n",
    "                                                                   'test_size': test_size}\n",
    "                                                    print(\"Newest balanced best accuracy: \", best_balanced_accuracy)\n",
    "                                                    print(\"Params balanced accuracy: \", best_params_balanced)\n",
    "                                                    # Save the best model, accuracy and top-3 accuracy\n",
    "                                                    with open(os.path.join(results_folder, \"best_accuracy_MLP_search_balanced_accuracy.txt\"), \"w\") as f:\n",
    "                                                        f.write(f\"Top balanced accuracy: {best_balanced_accuracy:.4f}\\n\")\n",
    "                                                        f.write(\"Best balanced accuracy parameters:\\n\")\n",
    "                                                        f.write(json.dumps(best_params_balanced, indent=4))\n",
    "                                                    # Define the path to save the model\n",
    "                                                    model_save_path = os.path.join(results_folder, 'balanced_accuracy_model.keras')\n",
    "                                                    # Save the model\n",
    "                                                    model.save(model_save_path)\n",
    "                                                current += 1\n",
    "                                                print(\"Current: \", current)\n",
    "                                            # Clear large data structures\n",
    "                                            del fixation_maps, all_fixation_maps, X, y\n",
    "                                            gc.collect()\n",
    "                                            \n",
    "                                            # Clear Keras session state\n",
    "                                            tf.keras.backend.clear_session()\n",
    "\n",
    "                                            # Additionally, delete the model and history to ensure they don't consume memory\n",
    "                                            del model, history, X_train, X_test, y_train, y_test, y_train_categorical, y_test_categorical, y_pred_prob, y_pred, conf_matrix\n",
    "                                            gc.collect()\n",
    "\n",
    "\n",
    "print(\"FINAL Best accuracy: \", best_accuracy)\n",
    "print(\"FINAL Params: \", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b8637-3f02-4e82-8574-dd97cda889bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
