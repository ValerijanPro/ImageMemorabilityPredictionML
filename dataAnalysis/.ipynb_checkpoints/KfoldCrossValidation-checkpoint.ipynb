{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba0c476-af43-44d4-8d38-3cac4bcf5f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626\n"
     ]
    }
   ],
   "source": [
    "#create dictionary: ImagePath, array of FIXMAPS\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import math\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import csv\n",
    "\n",
    "grid_size = (20,20)\n",
    "sigma = 1\n",
    "def compute_ppda(distance, h_res, v_res, screen_w, screen_h):\n",
    "    \"\"\"\n",
    "    Compute the number of pixels per degree of visual angle based on the experimental conditions.\n",
    "\n",
    "    :param distance: int, the distance between the observer and the screen (in mm)\n",
    "    :param h_res: int, the horizontal resolution of the screen\n",
    "    :param v_res: int, the vertical resolution of the screen\n",
    "    :param screen_w: int, the width of the screen (in mm)\n",
    "    :param screen_h: int, the height of the screen (in mm)\n",
    "    :return horizontal_ppda: float, the number of pixel per degree of visual angle\n",
    "    \"\"\"\n",
    "    pxl_density_x = h_res / screen_w\n",
    "    pxl_density_y = v_res / screen_h\n",
    "\n",
    "    d = 2 * distance * math.tan(np.deg2rad(0.5))\n",
    "    horizontal_ppda = d * ((pxl_density_x + pxl_density_y) / 2)\n",
    "\n",
    "    return horizontal_ppda\n",
    "\n",
    "def checkObserverRemembered(observer, image_path, base_dir):\n",
    "    csv_file_path = os.path.join(base_dir, \"..\" ,\"hit_status.csv\")\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "        print(\"Error: CSV file not found.\")\n",
    "        return False\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    filtered_rows = df[(df['Setup Folder'] == observer) & (df['Image Path'] == image_path) & (df['Hit'] == 1)]\n",
    "    if not filtered_rows.empty:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "distance = 610\n",
    "h_res = 1920\n",
    "v_res = 1080\n",
    "screen_w = 527\n",
    "screen_h = 296\n",
    "\n",
    "ppda = compute_ppda(distance, h_res, v_res, screen_w, screen_h)\n",
    "#print(\"ppda\", ppda)\n",
    "\n",
    "def bin_fixations(fixation_map):\n",
    "    global grid_size\n",
    "    height, width = fixation_map.shape\n",
    "    binned_map = np.zeros(grid_size)\n",
    "\n",
    "    bin_height = height // grid_size[0]\n",
    "    bin_width = width // grid_size[1]\n",
    "\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            bin_area = fixation_map[i*bin_height:(i+1)*bin_height, j*bin_width:(j+1)*bin_width]\n",
    "            binned_map[i, j] = np.sum(bin_area)\n",
    "            #ili avg?\n",
    "\n",
    "    return binned_map\n",
    "\n",
    "def normalize_map(binned_map):\n",
    "    return binned_map / np.sum(binned_map)\n",
    "    #return binned_map\n",
    "\n",
    "def smooth_map(binned_map):\n",
    "    global sigma\n",
    "    return gaussian_filter(binned_map, sigma=sigma)\n",
    "\n",
    "def process_fixation_map(fixation_map):\n",
    "    binned_map = bin_fixations(fixation_map)\n",
    "    normalized_map = normalize_map(binned_map)\n",
    "    smoothed_map = smooth_map(normalized_map)\n",
    "    return smoothed_map\n",
    "\n",
    "def get_current_fixation_map(image_path, coordinates):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Image at {image_path} not found.\")\n",
    "        return\n",
    "\n",
    "    coordinates = coordinates[0:120]\n",
    "  \n",
    "    fixation_map = np.zeros((1080, 1920), dtype=np.float32)\n",
    "\n",
    "    # Convert coordinates to pixel coordinates and update the saliency map\n",
    "    for x_norm, y_norm in coordinates:\n",
    "        # Scale normalized coordinates to pixel coordinates for the 1920x1080 screen\n",
    "        if(x_norm >0 and y_norm >0):\n",
    "            x = int((x_norm + 1 + 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "            y = int((y_norm + 0.5 + 0.05) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "        if(x_norm >0 and y_norm <0):\n",
    "            x = int((x_norm + 1 + 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "            y = int((y_norm + 0.5 - 0.1) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "        if(x_norm <0 and y_norm >0):\n",
    "            x = int((x_norm + 1 - 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "            y = int((y_norm + 0.5 + 0.05) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "        if(x_norm <0 and y_norm <0):\n",
    "            x = int((x_norm + 1 - 0.1) * 960)  # Scaling from (-1, 1) to (0, 1920) range\n",
    "            y = int((y_norm + 0.5 - 0.1) * 1080) # Scaling from (-0.5, 0.5) to (0, 1080) range\n",
    "        # Update the saliency map if coordinates are within the screen\n",
    "        if 0 <= x < 1920 and 0 <= y < 1080:\n",
    "            fixation_map[y, x] += 1\n",
    "    #sigma = ppda / np.sqrt(2)\n",
    "    #fixation_map = gaussian_filter(fixation_map, sigma = sigma)\n",
    "    fixation_map = cv2.GaussianBlur(fixation_map, (11,11), 0)\n",
    "    # Crop the saliency map to the 700x700 region\n",
    "    fixation_map = fixation_map[190:890, 610:1310]\n",
    "    # flip the Y coordinates\n",
    "    fixation_map = np.flipud(fixation_map)\n",
    "    return fixation_map\n",
    "\n",
    "def normalize_fixation_map(fixation_map):\n",
    "    min_val = np.min(fixation_map)\n",
    "    max_val = np.max(fixation_map)\n",
    "    normalized_fixation_map = (fixation_map - min_val) / (max_val - min_val) * 255\n",
    "    return normalized_fixation_map\n",
    "\n",
    "# 90experiments folder\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(),\"..\", \"90experiments\"))\n",
    "\n",
    "fixation_maps = {}  # Dictionary to store fixation maps for each imagePath\n",
    "\n",
    "\n",
    "for folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    match = re.search(r'\\d{1,2}$', folder)\n",
    "    if match:\n",
    "        observer = int(match.group())\n",
    "    #if(observer != 5):\n",
    "    #    continue\n",
    "    if(observer == 1 or observer == 2 or observer == 49 or observer == 50 or observer == 5):\n",
    "        continue\n",
    "\n",
    "    #if(observer not in [70,71,73,74,76,77,79,80,82,83,85,87,88,89,86,90,18,57,6,45,48,60,63,69,3,9,12,21,15,27,30,33,36,42,24,66,51,54,72,75]):\n",
    "    #    continue\n",
    "        \n",
    "    csv_file_path = os.path.join(folder_path, \"eye_tracker_data.csv\")\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "        continue\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    filtered_data = data[data['ImagePath'].str.startswith('targetImages')]\n",
    "    \n",
    "    uniqueImagePaths = []\n",
    "    delete_rows = []\n",
    "\n",
    "    #get only the eye-tracking data from the first viewing\n",
    "    index = 0\n",
    "    \n",
    "    row = filtered_data.iloc[index]\n",
    "    while len(uniqueImagePaths) < 10:\n",
    "        row = filtered_data.iloc[index]\n",
    "        if(row['ImagePath'] not in uniqueImagePaths):\n",
    "            uniqueImagePaths.append(row['ImagePath'])\n",
    "            lastImagePath = row['ImagePath']\n",
    "            index +=1\n",
    "        elif(row['ImagePath'] in uniqueImagePaths):\n",
    "            index += 1    \n",
    "    row = filtered_data.iloc[index]\n",
    "\n",
    "    while(row['ImagePath'] == lastImagePath):\n",
    "        index +=1\n",
    "        row = filtered_data.iloc[index]\n",
    "\n",
    "    filtered_data.reset_index(drop=True, inplace=True)\n",
    "    filtered_data = filtered_data.iloc[:index].copy()\n",
    "    \n",
    "    grouped = filtered_data.groupby('ImagePath')\n",
    "\n",
    "    # Generate and save fixation maps for each image in the current folder\n",
    "    for image_path, group in grouped:\n",
    "        # Construct full image path by going one directory back from base_dir\n",
    "        full_image_path = os.path.abspath(os.path.join(base_dir, \"..\", image_path))\n",
    "        full_image_path = full_image_path.replace('\\\\', '/')\n",
    "\n",
    "        #check if current observer has remembered this image, if not, continue\n",
    "        if(not checkObserverRemembered(observer, image_path, base_dir)):\n",
    "            continue\n",
    "        \n",
    "        # Extract coordinates\n",
    "        coordinates = group[['PosX', 'PosY']].values\n",
    "\n",
    "        current_fixation_map = get_current_fixation_map (full_image_path, coordinates)\n",
    "        if(np.all(current_fixation_map == 0)):\n",
    "            continue\n",
    "\n",
    "        current_fixation_map_20x20 = process_fixation_map(current_fixation_map)\n",
    "        current_fixation_map_20x20 = normalize_fixation_map(current_fixation_map_20x20)\n",
    "        \n",
    "        #current_fixation_map_20x20 = (current_fixation_map)\n",
    "        #add to dictionary or update it\n",
    "        if image_path not in fixation_maps:\n",
    "            fixation_maps[image_path] = [current_fixation_map_20x20]\n",
    "        else:\n",
    "            fixation_maps[image_path].append(current_fixation_map_20x20)\n",
    "\n",
    "# Flatten the fixation maps and standardize them\n",
    "all_fixation_maps = []\n",
    "labels = []\n",
    "\n",
    "for image_path, maps in fixation_maps.items():\n",
    "    for fixation_map in maps:\n",
    "        all_fixation_maps.append(fixation_map.flatten())\n",
    "        labels.append(image_path)\n",
    "\n",
    "X = np.array(all_fixation_maps)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(len(all_fixation_maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1119208b-28bd-450f-b699-56a067e3a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 0.9145313501358032; compile_metrics of 100.0%\n",
      "Score for fold 2: loss of 0.8997101187705994; compile_metrics of 100.0%\n",
      "Score for fold 3: loss of 5.18798828125; compile_metrics of 0.0%\n",
      "Score for fold 4: loss of 5.27087926864624; compile_metrics of 0.0%\n",
      "Score for fold 5: loss of 0.8512699007987976; compile_metrics of 100.0%\n",
      "Score for fold 6: loss of 1.3072110414505005; compile_metrics of 100.0%\n",
      "Score for fold 7: loss of 2.512240171432495; compile_metrics of 0.0%\n",
      "Score for fold 8: loss of 1.5243921279907227; compile_metrics of 100.0%\n",
      "Score for fold 9: loss of 0.8665581345558167; compile_metrics of 100.0%\n",
      "Score for fold 10: loss of 2.663489818572998; compile_metrics of 0.0%\n",
      "Score for fold 11: loss of 1.3502180576324463; compile_metrics of 100.0%\n",
      "Score for fold 12: loss of 3.0292389392852783; compile_metrics of 0.0%\n",
      "Score for fold 13: loss of 1.3630461692810059; compile_metrics of 100.0%\n",
      "Score for fold 14: loss of 5.265138149261475; compile_metrics of 0.0%\n",
      "Score for fold 15: loss of 2.1779792308807373; compile_metrics of 0.0%\n",
      "Score for fold 16: loss of 4.600186347961426; compile_metrics of 0.0%\n",
      "Score for fold 17: loss of 0.9702556729316711; compile_metrics of 100.0%\n",
      "Score for fold 18: loss of 7.65270471572876; compile_metrics of 0.0%\n",
      "Score for fold 19: loss of 5.420086860656738; compile_metrics of 0.0%\n",
      "Score for fold 20: loss of 2.763669490814209; compile_metrics of 0.0%\n",
      "Score for fold 21: loss of 1.019710898399353; compile_metrics of 100.0%\n",
      "Score for fold 22: loss of 1.4367300271987915; compile_metrics of 100.0%\n",
      "Score for fold 23: loss of 6.769132614135742; compile_metrics of 0.0%\n",
      "Score for fold 24: loss of 0.9914399981498718; compile_metrics of 100.0%\n",
      "Score for fold 25: loss of 0.8522976636886597; compile_metrics of 100.0%\n",
      "Score for fold 26: loss of 0.9962201118469238; compile_metrics of 100.0%\n",
      "Score for fold 27: loss of 1.1184160709381104; compile_metrics of 100.0%\n",
      "Score for fold 28: loss of 0.8942587375640869; compile_metrics of 100.0%\n",
      "Score for fold 29: loss of 0.8536155223846436; compile_metrics of 100.0%\n",
      "Score for fold 30: loss of 1.0487735271453857; compile_metrics of 100.0%\n",
      "Score for fold 31: loss of 2.5986111164093018; compile_metrics of 0.0%\n",
      "Score for fold 32: loss of 0.8556042909622192; compile_metrics of 100.0%\n",
      "Score for fold 33: loss of 10.650633811950684; compile_metrics of 0.0%\n",
      "Score for fold 34: loss of 3.422433853149414; compile_metrics of 0.0%\n",
      "Score for fold 35: loss of 1.016284704208374; compile_metrics of 100.0%\n",
      "Score for fold 36: loss of 0.8496214747428894; compile_metrics of 100.0%\n",
      "Score for fold 37: loss of 0.865243673324585; compile_metrics of 100.0%\n",
      "Score for fold 38: loss of 0.8860505819320679; compile_metrics of 100.0%\n",
      "Score for fold 39: loss of 1.188276767730713; compile_metrics of 100.0%\n",
      "Score for fold 40: loss of 0.8571082353591919; compile_metrics of 100.0%\n",
      "Score for fold 41: loss of 2.778978109359741; compile_metrics of 0.0%\n",
      "Score for fold 42: loss of 0.8476248383522034; compile_metrics of 100.0%\n",
      "Score for fold 43: loss of 0.8637148141860962; compile_metrics of 100.0%\n",
      "Score for fold 44: loss of 0.9180042147636414; compile_metrics of 100.0%\n",
      "Score for fold 45: loss of 1.1472538709640503; compile_metrics of 100.0%\n",
      "Score for fold 46: loss of 4.383771896362305; compile_metrics of 0.0%\n",
      "Score for fold 47: loss of 1.0646260976791382; compile_metrics of 100.0%\n",
      "Score for fold 48: loss of 1.1228376626968384; compile_metrics of 100.0%\n",
      "Score for fold 49: loss of 5.926088809967041; compile_metrics of 0.0%\n",
      "Score for fold 50: loss of 1.43735933303833; compile_metrics of 100.0%\n",
      "Score for fold 51: loss of 1.018784523010254; compile_metrics of 100.0%\n",
      "Score for fold 52: loss of 0.8999790549278259; compile_metrics of 100.0%\n",
      "Score for fold 53: loss of 1.0636838674545288; compile_metrics of 100.0%\n",
      "Score for fold 54: loss of 1.3621835708618164; compile_metrics of 100.0%\n",
      "Score for fold 55: loss of 3.968229293823242; compile_metrics of 0.0%\n",
      "Score for fold 56: loss of 2.4329519271850586; compile_metrics of 0.0%\n",
      "Score for fold 57: loss of 1.108647108078003; compile_metrics of 100.0%\n",
      "Score for fold 58: loss of 2.145414352416992; compile_metrics of 0.0%\n",
      "Score for fold 59: loss of 1.1898925304412842; compile_metrics of 100.0%\n",
      "Score for fold 60: loss of 0.9813845753669739; compile_metrics of 100.0%\n",
      "Score for fold 61: loss of 1.4515748023986816; compile_metrics of 100.0%\n",
      "Score for fold 62: loss of 1.0760406255722046; compile_metrics of 100.0%\n",
      "Score for fold 63: loss of 0.9202590584754944; compile_metrics of 100.0%\n",
      "Score for fold 64: loss of 3.6321418285369873; compile_metrics of 0.0%\n",
      "Score for fold 65: loss of 1.098732829093933; compile_metrics of 100.0%\n",
      "Score for fold 66: loss of 0.8965722322463989; compile_metrics of 100.0%\n",
      "Score for fold 67: loss of 0.9632938504219055; compile_metrics of 100.0%\n",
      "Score for fold 68: loss of 1.8310900926589966; compile_metrics of 100.0%\n",
      "Score for fold 69: loss of 7.672354698181152; compile_metrics of 0.0%\n",
      "Score for fold 70: loss of 1.0073388814926147; compile_metrics of 100.0%\n",
      "Score for fold 71: loss of 0.9069358110427856; compile_metrics of 100.0%\n",
      "Score for fold 72: loss of 0.950287938117981; compile_metrics of 100.0%\n",
      "Score for fold 73: loss of 1.002694845199585; compile_metrics of 100.0%\n",
      "Score for fold 74: loss of 2.7553863525390625; compile_metrics of 0.0%\n",
      "Score for fold 75: loss of 1.0388094186782837; compile_metrics of 100.0%\n",
      "Score for fold 76: loss of 0.9377893209457397; compile_metrics of 100.0%\n",
      "Score for fold 77: loss of 0.9358014464378357; compile_metrics of 100.0%\n",
      "Score for fold 78: loss of 1.1500808000564575; compile_metrics of 100.0%\n",
      "Score for fold 79: loss of 0.9729105830192566; compile_metrics of 100.0%\n",
      "Score for fold 80: loss of 0.8851261138916016; compile_metrics of 100.0%\n",
      "Score for fold 81: loss of 3.198911428451538; compile_metrics of 0.0%\n",
      "Score for fold 82: loss of 6.230937480926514; compile_metrics of 0.0%\n",
      "Score for fold 83: loss of 1.092820644378662; compile_metrics of 100.0%\n",
      "Score for fold 84: loss of 1.0357728004455566; compile_metrics of 100.0%\n",
      "Score for fold 85: loss of 2.2176527976989746; compile_metrics of 0.0%\n",
      "Score for fold 86: loss of 5.443781852722168; compile_metrics of 0.0%\n",
      "Score for fold 87: loss of 0.9510019421577454; compile_metrics of 100.0%\n",
      "Score for fold 88: loss of 0.9825817942619324; compile_metrics of 100.0%\n",
      "Score for fold 89: loss of 1.9529452323913574; compile_metrics of 100.0%\n",
      "Score for fold 90: loss of 2.2843568325042725; compile_metrics of 0.0%\n",
      "Score for fold 91: loss of 3.3209431171417236; compile_metrics of 0.0%\n",
      "Score for fold 92: loss of 4.080124855041504; compile_metrics of 0.0%\n",
      "Score for fold 93: loss of 2.754002332687378; compile_metrics of 0.0%\n",
      "Score for fold 94: loss of 2.717539072036743; compile_metrics of 0.0%\n",
      "Score for fold 95: loss of 2.905186176300049; compile_metrics of 0.0%\n",
      "Score for fold 96: loss of 0.8674067854881287; compile_metrics of 100.0%\n",
      "Score for fold 97: loss of 0.8453837633132935; compile_metrics of 100.0%\n",
      "Score for fold 98: loss of 3.619534969329834; compile_metrics of 0.0%\n",
      "Score for fold 99: loss of 0.9100253582000732; compile_metrics of 100.0%\n",
      "Score for fold 100: loss of 0.8853994607925415; compile_metrics of 100.0%\n",
      "Score for fold 101: loss of 0.8757191300392151; compile_metrics of 100.0%\n",
      "Score for fold 102: loss of 0.8487359285354614; compile_metrics of 100.0%\n",
      "Score for fold 103: loss of 0.8502488136291504; compile_metrics of 100.0%\n",
      "Score for fold 104: loss of 0.8770296573638916; compile_metrics of 100.0%\n",
      "Score for fold 105: loss of 0.8583393692970276; compile_metrics of 100.0%\n",
      "Score for fold 106: loss of 0.9544982314109802; compile_metrics of 100.0%\n",
      "Score for fold 107: loss of 2.0895586013793945; compile_metrics of 100.0%\n",
      "Score for fold 108: loss of 0.868798553943634; compile_metrics of 100.0%\n",
      "Score for fold 109: loss of 1.029897928237915; compile_metrics of 100.0%\n",
      "Score for fold 110: loss of 0.8422600030899048; compile_metrics of 100.0%\n",
      "Score for fold 111: loss of 0.8510485291481018; compile_metrics of 100.0%\n",
      "Score for fold 112: loss of 1.0252915620803833; compile_metrics of 100.0%\n",
      "Score for fold 113: loss of 0.8971385359764099; compile_metrics of 100.0%\n",
      "Score for fold 114: loss of 0.8570410013198853; compile_metrics of 100.0%\n",
      "Score for fold 115: loss of 2.5421040058135986; compile_metrics of 0.0%\n",
      "Score for fold 116: loss of 0.8439052104949951; compile_metrics of 100.0%\n",
      "Score for fold 117: loss of 0.9441524744033813; compile_metrics of 100.0%\n",
      "Score for fold 118: loss of 0.8566249012947083; compile_metrics of 100.0%\n",
      "Score for fold 119: loss of 4.417215347290039; compile_metrics of 0.0%\n",
      "Score for fold 120: loss of 0.8772983551025391; compile_metrics of 100.0%\n",
      "Score for fold 121: loss of 6.678630828857422; compile_metrics of 0.0%\n",
      "Score for fold 122: loss of 0.8506032228469849; compile_metrics of 100.0%\n",
      "Score for fold 123: loss of 0.844548761844635; compile_metrics of 100.0%\n",
      "Score for fold 124: loss of 4.96046257019043; compile_metrics of 0.0%\n",
      "Score for fold 125: loss of 3.323199987411499; compile_metrics of 0.0%\n",
      "Score for fold 126: loss of 0.8494036197662354; compile_metrics of 100.0%\n",
      "Score for fold 127: loss of 0.8655701875686646; compile_metrics of 100.0%\n",
      "Score for fold 128: loss of 0.8427124619483948; compile_metrics of 100.0%\n",
      "Score for fold 129: loss of 0.8583852648735046; compile_metrics of 100.0%\n",
      "Score for fold 130: loss of 0.8481633067131042; compile_metrics of 100.0%\n",
      "Score for fold 131: loss of 0.858261227607727; compile_metrics of 100.0%\n",
      "Score for fold 132: loss of 0.8486145734786987; compile_metrics of 100.0%\n",
      "Score for fold 133: loss of 0.843243420124054; compile_metrics of 100.0%\n",
      "Score for fold 134: loss of 0.875340461730957; compile_metrics of 100.0%\n",
      "Score for fold 135: loss of 1.042438268661499; compile_metrics of 100.0%\n",
      "Score for fold 136: loss of 0.946558952331543; compile_metrics of 100.0%\n",
      "Score for fold 137: loss of 1.1735543012619019; compile_metrics of 100.0%\n",
      "Score for fold 138: loss of 4.585023880004883; compile_metrics of 0.0%\n",
      "Score for fold 139: loss of 2.8207719326019287; compile_metrics of 0.0%\n",
      "Score for fold 140: loss of 1.4197571277618408; compile_metrics of 100.0%\n",
      "Score for fold 141: loss of 0.9894739985466003; compile_metrics of 100.0%\n",
      "Score for fold 142: loss of 2.641016721725464; compile_metrics of 0.0%\n",
      "Score for fold 143: loss of 4.459473609924316; compile_metrics of 0.0%\n",
      "Score for fold 144: loss of 0.903637170791626; compile_metrics of 100.0%\n",
      "Score for fold 145: loss of 1.2223024368286133; compile_metrics of 100.0%\n",
      "Score for fold 146: loss of 0.9341103434562683; compile_metrics of 100.0%\n",
      "Score for fold 147: loss of 0.9614943861961365; compile_metrics of 100.0%\n",
      "Score for fold 148: loss of 1.0447571277618408; compile_metrics of 100.0%\n",
      "Score for fold 149: loss of 4.580891132354736; compile_metrics of 0.0%\n",
      "Score for fold 150: loss of 1.623137354850769; compile_metrics of 100.0%\n",
      "Score for fold 151: loss of 3.0571906566619873; compile_metrics of 0.0%\n",
      "Score for fold 152: loss of 5.05222225189209; compile_metrics of 0.0%\n",
      "Score for fold 153: loss of 0.9414388537406921; compile_metrics of 100.0%\n",
      "Score for fold 154: loss of 0.9513896703720093; compile_metrics of 100.0%\n",
      "Score for fold 155: loss of 0.9970871210098267; compile_metrics of 100.0%\n",
      "Score for fold 156: loss of 2.469743013381958; compile_metrics of 0.0%\n",
      "Score for fold 157: loss of 1.5135303735733032; compile_metrics of 100.0%\n",
      "Score for fold 158: loss of 0.892777144908905; compile_metrics of 100.0%\n",
      "Score for fold 159: loss of 6.663459777832031; compile_metrics of 0.0%\n",
      "Score for fold 160: loss of 3.201796770095825; compile_metrics of 0.0%\n",
      "Score for fold 161: loss of 1.2834351062774658; compile_metrics of 100.0%\n",
      "Score for fold 162: loss of 1.4165918827056885; compile_metrics of 100.0%\n",
      "Score for fold 163: loss of 3.608611583709717; compile_metrics of 0.0%\n",
      "Score for fold 164: loss of 1.013505458831787; compile_metrics of 100.0%\n",
      "Score for fold 165: loss of 2.4509665966033936; compile_metrics of 0.0%\n",
      "Score for fold 166: loss of 0.9840571284294128; compile_metrics of 100.0%\n",
      "Score for fold 167: loss of 3.515469551086426; compile_metrics of 0.0%\n",
      "Score for fold 168: loss of 4.139593601226807; compile_metrics of 0.0%\n",
      "Score for fold 169: loss of 2.1948046684265137; compile_metrics of 0.0%\n",
      "Score for fold 170: loss of 1.9040757417678833; compile_metrics of 100.0%\n",
      "Score for fold 171: loss of 2.4330427646636963; compile_metrics of 0.0%\n",
      "Score for fold 172: loss of 9.265478134155273; compile_metrics of 0.0%\n",
      "Score for fold 173: loss of 1.3637393712997437; compile_metrics of 100.0%\n",
      "Score for fold 174: loss of 0.8832404017448425; compile_metrics of 100.0%\n",
      "Score for fold 175: loss of 0.8674540519714355; compile_metrics of 100.0%\n",
      "Score for fold 176: loss of 1.0323398113250732; compile_metrics of 100.0%\n",
      "Score for fold 177: loss of 4.5135979652404785; compile_metrics of 0.0%\n",
      "Score for fold 178: loss of 0.9060455560684204; compile_metrics of 100.0%\n",
      "Score for fold 179: loss of 4.041301250457764; compile_metrics of 0.0%\n",
      "Score for fold 180: loss of 0.8793611526489258; compile_metrics of 100.0%\n",
      "Score for fold 181: loss of 1.007790446281433; compile_metrics of 100.0%\n",
      "Score for fold 182: loss of 4.3650736808776855; compile_metrics of 0.0%\n",
      "Score for fold 183: loss of 0.9729588627815247; compile_metrics of 100.0%\n",
      "Score for fold 184: loss of 1.2608150243759155; compile_metrics of 100.0%\n",
      "Score for fold 185: loss of 1.1290364265441895; compile_metrics of 100.0%\n",
      "Score for fold 186: loss of 0.9676791429519653; compile_metrics of 100.0%\n",
      "Score for fold 187: loss of 3.1401355266571045; compile_metrics of 0.0%\n",
      "Score for fold 188: loss of 0.9027136564254761; compile_metrics of 100.0%\n",
      "Score for fold 189: loss of 1.0563855171203613; compile_metrics of 100.0%\n",
      "Score for fold 190: loss of 2.1845340728759766; compile_metrics of 0.0%\n",
      "Score for fold 191: loss of 4.012030601501465; compile_metrics of 0.0%\n",
      "Score for fold 192: loss of 1.1500426530838013; compile_metrics of 100.0%\n",
      "Score for fold 193: loss of 0.858735978603363; compile_metrics of 100.0%\n",
      "Score for fold 194: loss of 3.5254149436950684; compile_metrics of 0.0%\n",
      "Score for fold 195: loss of 0.8655185699462891; compile_metrics of 100.0%\n",
      "Score for fold 196: loss of 4.445860385894775; compile_metrics of 0.0%\n",
      "Score for fold 197: loss of 0.8958170413970947; compile_metrics of 100.0%\n",
      "Score for fold 198: loss of 4.186395645141602; compile_metrics of 0.0%\n",
      "Score for fold 199: loss of 1.6231493949890137; compile_metrics of 100.0%\n",
      "Score for fold 200: loss of 5.213723182678223; compile_metrics of 0.0%\n",
      "Score for fold 201: loss of 4.592926502227783; compile_metrics of 0.0%\n",
      "Score for fold 202: loss of 3.8238515853881836; compile_metrics of 0.0%\n",
      "Score for fold 203: loss of 1.5331189632415771; compile_metrics of 100.0%\n",
      "Score for fold 204: loss of 1.9469194412231445; compile_metrics of 100.0%\n",
      "Score for fold 205: loss of 1.6485645771026611; compile_metrics of 100.0%\n",
      "Score for fold 206: loss of 0.8736959099769592; compile_metrics of 100.0%\n",
      "Score for fold 207: loss of 1.9564417600631714; compile_metrics of 100.0%\n",
      "Score for fold 208: loss of 0.8894238471984863; compile_metrics of 100.0%\n",
      "Score for fold 209: loss of 2.249124526977539; compile_metrics of 100.0%\n",
      "Score for fold 210: loss of 1.157512903213501; compile_metrics of 100.0%\n",
      "Score for fold 211: loss of 4.04813289642334; compile_metrics of 0.0%\n",
      "Score for fold 212: loss of 1.4706069231033325; compile_metrics of 100.0%\n",
      "Score for fold 213: loss of 1.7207614183425903; compile_metrics of 100.0%\n",
      "Score for fold 214: loss of 3.872807025909424; compile_metrics of 0.0%\n",
      "Score for fold 215: loss of 1.6601011753082275; compile_metrics of 100.0%\n",
      "Score for fold 216: loss of 1.2671796083450317; compile_metrics of 100.0%\n",
      "Score for fold 217: loss of 9.151823043823242; compile_metrics of 0.0%\n",
      "Score for fold 218: loss of 3.41133713722229; compile_metrics of 0.0%\n",
      "Score for fold 219: loss of 1.5242996215820312; compile_metrics of 100.0%\n",
      "Score for fold 220: loss of 4.526025772094727; compile_metrics of 0.0%\n",
      "Score for fold 221: loss of 4.809184551239014; compile_metrics of 0.0%\n",
      "Score for fold 222: loss of 0.9819000959396362; compile_metrics of 100.0%\n",
      "Score for fold 223: loss of 4.509710311889648; compile_metrics of 0.0%\n",
      "Score for fold 224: loss of 1.0820006132125854; compile_metrics of 100.0%\n",
      "Score for fold 225: loss of 2.493654489517212; compile_metrics of 0.0%\n",
      "Score for fold 226: loss of 1.1396369934082031; compile_metrics of 100.0%\n",
      "Score for fold 227: loss of 1.1565322875976562; compile_metrics of 100.0%\n",
      "Score for fold 228: loss of 1.1396799087524414; compile_metrics of 100.0%\n",
      "Score for fold 229: loss of 1.3473470211029053; compile_metrics of 100.0%\n",
      "Score for fold 230: loss of 1.2807239294052124; compile_metrics of 100.0%\n",
      "Score for fold 231: loss of 0.8975663781166077; compile_metrics of 100.0%\n",
      "Score for fold 232: loss of 3.504795551300049; compile_metrics of 0.0%\n",
      "Score for fold 233: loss of 5.2884745597839355; compile_metrics of 0.0%\n",
      "Score for fold 234: loss of 1.4628809690475464; compile_metrics of 100.0%\n",
      "Score for fold 235: loss of 1.563801646232605; compile_metrics of 100.0%\n",
      "Score for fold 236: loss of 3.3461806774139404; compile_metrics of 0.0%\n",
      "Score for fold 237: loss of 0.8631141185760498; compile_metrics of 100.0%\n",
      "Score for fold 238: loss of 3.130690574645996; compile_metrics of 0.0%\n",
      "Score for fold 239: loss of 1.5758824348449707; compile_metrics of 100.0%\n",
      "Score for fold 240: loss of 7.754516124725342; compile_metrics of 0.0%\n",
      "Score for fold 241: loss of 0.943807065486908; compile_metrics of 100.0%\n",
      "Score for fold 242: loss of 1.6853501796722412; compile_metrics of 100.0%\n",
      "Score for fold 243: loss of 1.612069010734558; compile_metrics of 100.0%\n",
      "Score for fold 244: loss of 0.9322299957275391; compile_metrics of 100.0%\n",
      "Score for fold 245: loss of 5.855983257293701; compile_metrics of 0.0%\n",
      "Score for fold 246: loss of 0.8973303437232971; compile_metrics of 100.0%\n",
      "Score for fold 247: loss of 0.9579850435256958; compile_metrics of 100.0%\n",
      "Score for fold 248: loss of 4.164032459259033; compile_metrics of 0.0%\n",
      "Score for fold 249: loss of 2.961075782775879; compile_metrics of 0.0%\n",
      "Score for fold 250: loss of 1.0691900253295898; compile_metrics of 100.0%\n",
      "Score for fold 251: loss of 6.608582019805908; compile_metrics of 0.0%\n",
      "Score for fold 252: loss of 1.0435467958450317; compile_metrics of 100.0%\n",
      "Score for fold 253: loss of 0.9752589464187622; compile_metrics of 100.0%\n",
      "Score for fold 254: loss of 9.656205177307129; compile_metrics of 0.0%\n",
      "Score for fold 255: loss of 2.7057058811187744; compile_metrics of 0.0%\n",
      "Score for fold 256: loss of 6.151993751525879; compile_metrics of 0.0%\n",
      "Score for fold 257: loss of 9.549145698547363; compile_metrics of 0.0%\n",
      "Score for fold 258: loss of 1.6350444555282593; compile_metrics of 100.0%\n",
      "Score for fold 259: loss of 1.140688180923462; compile_metrics of 100.0%\n",
      "Score for fold 260: loss of 1.272387981414795; compile_metrics of 100.0%\n",
      "Score for fold 261: loss of 2.2854979038238525; compile_metrics of 100.0%\n",
      "Score for fold 262: loss of 0.8673141598701477; compile_metrics of 100.0%\n",
      "Score for fold 263: loss of 2.5854382514953613; compile_metrics of 0.0%\n",
      "Score for fold 264: loss of 0.8433277606964111; compile_metrics of 100.0%\n",
      "Score for fold 265: loss of 0.8572008013725281; compile_metrics of 100.0%\n",
      "Score for fold 266: loss of 2.721238613128662; compile_metrics of 0.0%\n",
      "Score for fold 267: loss of 0.8481435775756836; compile_metrics of 100.0%\n",
      "Score for fold 268: loss of 1.1650007963180542; compile_metrics of 100.0%\n",
      "Score for fold 269: loss of 0.8490662574768066; compile_metrics of 100.0%\n",
      "Score for fold 270: loss of 2.633624315261841; compile_metrics of 0.0%\n",
      "Score for fold 271: loss of 0.8498359322547913; compile_metrics of 100.0%\n",
      "Score for fold 272: loss of 5.160952091217041; compile_metrics of 0.0%\n",
      "Score for fold 273: loss of 0.8826664686203003; compile_metrics of 100.0%\n",
      "Score for fold 274: loss of 0.8504940867424011; compile_metrics of 100.0%\n",
      "Score for fold 275: loss of 1.1376429796218872; compile_metrics of 100.0%\n",
      "Score for fold 276: loss of 0.8587802648544312; compile_metrics of 100.0%\n",
      "Score for fold 277: loss of 0.8746435046195984; compile_metrics of 100.0%\n",
      "Score for fold 278: loss of 0.8494434952735901; compile_metrics of 100.0%\n",
      "Score for fold 279: loss of 0.8498833179473877; compile_metrics of 100.0%\n",
      "Score for fold 280: loss of 0.8521544933319092; compile_metrics of 100.0%\n",
      "Score for fold 281: loss of 1.0890833139419556; compile_metrics of 100.0%\n",
      "Score for fold 282: loss of 0.880096971988678; compile_metrics of 100.0%\n",
      "Score for fold 283: loss of 1.3260064125061035; compile_metrics of 100.0%\n",
      "Score for fold 284: loss of 0.8473911285400391; compile_metrics of 100.0%\n",
      "Score for fold 285: loss of 0.8939700722694397; compile_metrics of 100.0%\n",
      "Score for fold 286: loss of 4.434677600860596; compile_metrics of 0.0%\n",
      "Score for fold 287: loss of 0.8508908152580261; compile_metrics of 100.0%\n",
      "Score for fold 288: loss of 0.8983324766159058; compile_metrics of 100.0%\n",
      "Score for fold 289: loss of 0.9843370318412781; compile_metrics of 100.0%\n",
      "Score for fold 290: loss of 0.8517184853553772; compile_metrics of 100.0%\n",
      "Score for fold 291: loss of 3.3782148361206055; compile_metrics of 0.0%\n",
      "Score for fold 292: loss of 1.2917580604553223; compile_metrics of 100.0%\n",
      "Score for fold 293: loss of 1.144906759262085; compile_metrics of 100.0%\n",
      "Score for fold 294: loss of 5.7757158279418945; compile_metrics of 0.0%\n",
      "Score for fold 295: loss of 0.8478230834007263; compile_metrics of 100.0%\n",
      "Score for fold 296: loss of 1.5614266395568848; compile_metrics of 100.0%\n",
      "Score for fold 297: loss of 1.2176244258880615; compile_metrics of 100.0%\n",
      "Score for fold 298: loss of 0.9958539605140686; compile_metrics of 100.0%\n",
      "Score for fold 299: loss of 3.170675039291382; compile_metrics of 0.0%\n",
      "Score for fold 300: loss of 1.173718810081482; compile_metrics of 100.0%\n",
      "Score for fold 301: loss of 1.2617276906967163; compile_metrics of 100.0%\n",
      "Score for fold 302: loss of 7.932820796966553; compile_metrics of 0.0%\n",
      "Score for fold 303: loss of 1.0100228786468506; compile_metrics of 100.0%\n",
      "Score for fold 304: loss of 1.0789541006088257; compile_metrics of 100.0%\n",
      "Score for fold 305: loss of 1.0157586336135864; compile_metrics of 100.0%\n",
      "Score for fold 306: loss of 4.519046306610107; compile_metrics of 0.0%\n",
      "Score for fold 307: loss of 4.454129219055176; compile_metrics of 0.0%\n",
      "Score for fold 308: loss of 5.8411784172058105; compile_metrics of 0.0%\n",
      "Score for fold 309: loss of 2.989814281463623; compile_metrics of 0.0%\n",
      "Score for fold 310: loss of 1.769945740699768; compile_metrics of 100.0%\n",
      "Score for fold 311: loss of 1.3872629404067993; compile_metrics of 100.0%\n",
      "Score for fold 312: loss of 1.3379818201065063; compile_metrics of 100.0%\n",
      "Score for fold 313: loss of 6.436212062835693; compile_metrics of 0.0%\n",
      "Score for fold 314: loss of 2.8640477657318115; compile_metrics of 0.0%\n",
      "Score for fold 315: loss of 1.1514062881469727; compile_metrics of 100.0%\n",
      "Score for fold 316: loss of 1.3635591268539429; compile_metrics of 100.0%\n",
      "Score for fold 317: loss of 1.0330979824066162; compile_metrics of 100.0%\n",
      "Score for fold 318: loss of 2.9557766914367676; compile_metrics of 0.0%\n",
      "Score for fold 319: loss of 5.021607875823975; compile_metrics of 0.0%\n",
      "Score for fold 320: loss of 3.1465868949890137; compile_metrics of 0.0%\n",
      "Score for fold 321: loss of 4.859615325927734; compile_metrics of 0.0%\n",
      "Score for fold 322: loss of 6.504964351654053; compile_metrics of 0.0%\n",
      "Score for fold 323: loss of 3.5826098918914795; compile_metrics of 0.0%\n",
      "Score for fold 324: loss of 3.3974528312683105; compile_metrics of 0.0%\n",
      "Score for fold 325: loss of 1.2962771654129028; compile_metrics of 100.0%\n",
      "Score for fold 326: loss of 8.11217975616455; compile_metrics of 0.0%\n",
      "Score for fold 327: loss of 1.2693532705307007; compile_metrics of 100.0%\n",
      "Score for fold 328: loss of 3.3092093467712402; compile_metrics of 0.0%\n",
      "Score for fold 329: loss of 1.3978208303451538; compile_metrics of 100.0%\n",
      "Score for fold 330: loss of 1.219752550125122; compile_metrics of 100.0%\n",
      "Score for fold 331: loss of 1.2913975715637207; compile_metrics of 100.0%\n",
      "Score for fold 332: loss of 3.7751498222351074; compile_metrics of 0.0%\n",
      "Score for fold 333: loss of 1.1600545644760132; compile_metrics of 100.0%\n",
      "Score for fold 334: loss of 1.3920832872390747; compile_metrics of 100.0%\n",
      "Score for fold 335: loss of 4.015491485595703; compile_metrics of 0.0%\n",
      "Score for fold 336: loss of 3.241015911102295; compile_metrics of 0.0%\n",
      "Score for fold 337: loss of 9.854778289794922; compile_metrics of 0.0%\n",
      "Score for fold 338: loss of 7.1056060791015625; compile_metrics of 0.0%\n",
      "Score for fold 339: loss of 9.575594902038574; compile_metrics of 0.0%\n",
      "Score for fold 340: loss of 1.5303739309310913; compile_metrics of 100.0%\n",
      "Score for fold 341: loss of 0.9527623057365417; compile_metrics of 100.0%\n",
      "Score for fold 342: loss of 1.1338540315628052; compile_metrics of 100.0%\n",
      "Score for fold 343: loss of 1.1969587802886963; compile_metrics of 100.0%\n",
      "Score for fold 344: loss of 0.8984581232070923; compile_metrics of 100.0%\n",
      "Score for fold 345: loss of 0.9781526923179626; compile_metrics of 100.0%\n",
      "Score for fold 346: loss of 1.453646183013916; compile_metrics of 100.0%\n",
      "Score for fold 347: loss of 0.9219076037406921; compile_metrics of 100.0%\n",
      "Score for fold 348: loss of 1.0221309661865234; compile_metrics of 100.0%\n",
      "Score for fold 349: loss of 2.193153142929077; compile_metrics of 100.0%\n",
      "Score for fold 350: loss of 0.8715349435806274; compile_metrics of 100.0%\n",
      "Score for fold 351: loss of 0.8845039010047913; compile_metrics of 100.0%\n",
      "Score for fold 352: loss of 0.8740363717079163; compile_metrics of 100.0%\n",
      "Score for fold 353: loss of 4.014443874359131; compile_metrics of 0.0%\n",
      "Score for fold 354: loss of 0.9377525448799133; compile_metrics of 100.0%\n",
      "Score for fold 355: loss of 0.874453067779541; compile_metrics of 100.0%\n",
      "Score for fold 356: loss of 0.875803530216217; compile_metrics of 100.0%\n",
      "Score for fold 357: loss of 0.8745123147964478; compile_metrics of 100.0%\n",
      "Score for fold 358: loss of 1.1358495950698853; compile_metrics of 100.0%\n",
      "Score for fold 359: loss of 0.905798077583313; compile_metrics of 100.0%\n",
      "Score for fold 360: loss of 1.196414828300476; compile_metrics of 100.0%\n",
      "Score for fold 361: loss of 0.8608114719390869; compile_metrics of 100.0%\n",
      "Score for fold 362: loss of 0.9142574667930603; compile_metrics of 100.0%\n",
      "Score for fold 363: loss of 0.8920106291770935; compile_metrics of 100.0%\n",
      "Score for fold 364: loss of 0.907257080078125; compile_metrics of 100.0%\n",
      "Score for fold 365: loss of 0.9912732839584351; compile_metrics of 100.0%\n",
      "Score for fold 366: loss of 1.0097646713256836; compile_metrics of 100.0%\n",
      "Score for fold 367: loss of 0.8452950119972229; compile_metrics of 100.0%\n",
      "Score for fold 368: loss of 0.8597180247306824; compile_metrics of 100.0%\n",
      "Score for fold 369: loss of 0.8594998717308044; compile_metrics of 100.0%\n",
      "Score for fold 370: loss of 0.9031014442443848; compile_metrics of 100.0%\n",
      "Score for fold 371: loss of 1.0246700048446655; compile_metrics of 100.0%\n",
      "Score for fold 372: loss of 0.8723406195640564; compile_metrics of 100.0%\n",
      "Score for fold 373: loss of 3.7357330322265625; compile_metrics of 0.0%\n",
      "Score for fold 374: loss of 0.8517526388168335; compile_metrics of 100.0%\n",
      "Score for fold 375: loss of 6.657891750335693; compile_metrics of 0.0%\n",
      "Score for fold 376: loss of 3.565439462661743; compile_metrics of 0.0%\n",
      "Score for fold 377: loss of 4.787084579467773; compile_metrics of 0.0%\n",
      "Score for fold 378: loss of 0.8588809370994568; compile_metrics of 100.0%\n",
      "Score for fold 379: loss of 0.9811761379241943; compile_metrics of 100.0%\n",
      "Score for fold 380: loss of 3.507692337036133; compile_metrics of 0.0%\n",
      "Score for fold 381: loss of 2.4508912563323975; compile_metrics of 0.0%\n",
      "Score for fold 382: loss of 2.6034562587738037; compile_metrics of 0.0%\n",
      "Score for fold 383: loss of 3.7655715942382812; compile_metrics of 0.0%\n",
      "Score for fold 384: loss of 1.674576997756958; compile_metrics of 100.0%\n",
      "Score for fold 385: loss of 6.435464859008789; compile_metrics of 0.0%\n",
      "Score for fold 386: loss of 1.3051568269729614; compile_metrics of 100.0%\n",
      "Score for fold 387: loss of 0.8471894264221191; compile_metrics of 100.0%\n",
      "Score for fold 388: loss of 0.9241212010383606; compile_metrics of 100.0%\n",
      "Score for fold 389: loss of 4.589362621307373; compile_metrics of 0.0%\n",
      "Score for fold 390: loss of 1.1900503635406494; compile_metrics of 100.0%\n",
      "Score for fold 391: loss of 1.0583935976028442; compile_metrics of 100.0%\n",
      "Score for fold 392: loss of 2.8429954051971436; compile_metrics of 0.0%\n",
      "Score for fold 393: loss of 8.738680839538574; compile_metrics of 0.0%\n",
      "Score for fold 394: loss of 2.9355812072753906; compile_metrics of 0.0%\n",
      "Score for fold 395: loss of 1.0060216188430786; compile_metrics of 100.0%\n",
      "Score for fold 396: loss of 2.6921873092651367; compile_metrics of 0.0%\n",
      "Score for fold 397: loss of 1.0157511234283447; compile_metrics of 100.0%\n",
      "Score for fold 398: loss of 0.9667556881904602; compile_metrics of 100.0%\n",
      "Score for fold 399: loss of 0.8523995280265808; compile_metrics of 100.0%\n",
      "Score for fold 400: loss of 0.8825660943984985; compile_metrics of 100.0%\n",
      "Score for fold 401: loss of 0.9969438314437866; compile_metrics of 100.0%\n",
      "Score for fold 402: loss of 1.9642162322998047; compile_metrics of 100.0%\n",
      "Score for fold 403: loss of 2.391787528991699; compile_metrics of 0.0%\n",
      "Score for fold 404: loss of 3.927360773086548; compile_metrics of 0.0%\n",
      "Score for fold 405: loss of 5.0722246170043945; compile_metrics of 0.0%\n",
      "Score for fold 406: loss of 3.5384254455566406; compile_metrics of 0.0%\n",
      "Score for fold 407: loss of 3.484746217727661; compile_metrics of 0.0%\n",
      "Score for fold 408: loss of 3.830873489379883; compile_metrics of 0.0%\n",
      "Score for fold 409: loss of 6.34848165512085; compile_metrics of 0.0%\n",
      "Score for fold 410: loss of 6.238144397735596; compile_metrics of 0.0%\n",
      "Score for fold 411: loss of 1.2610890865325928; compile_metrics of 100.0%\n",
      "Score for fold 412: loss of 4.736393451690674; compile_metrics of 0.0%\n",
      "Score for fold 413: loss of 1.7625651359558105; compile_metrics of 100.0%\n",
      "Score for fold 414: loss of 5.012626647949219; compile_metrics of 0.0%\n",
      "Score for fold 415: loss of 2.1779661178588867; compile_metrics of 100.0%\n",
      "Score for fold 416: loss of 5.591870307922363; compile_metrics of 0.0%\n",
      "Score for fold 417: loss of 2.9185383319854736; compile_metrics of 0.0%\n",
      "Score for fold 418: loss of 5.890451431274414; compile_metrics of 0.0%\n",
      "Score for fold 419: loss of 3.551107406616211; compile_metrics of 0.0%\n",
      "Score for fold 420: loss of 1.1133108139038086; compile_metrics of 100.0%\n",
      "Score for fold 421: loss of 4.27368688583374; compile_metrics of 0.0%\n",
      "Score for fold 422: loss of 0.931184709072113; compile_metrics of 100.0%\n",
      "Score for fold 423: loss of 2.38320255279541; compile_metrics of 0.0%\n",
      "Score for fold 424: loss of 5.267359733581543; compile_metrics of 0.0%\n",
      "Score for fold 425: loss of 0.9657809138298035; compile_metrics of 100.0%\n",
      "Score for fold 426: loss of 5.369721412658691; compile_metrics of 0.0%\n",
      "Score for fold 427: loss of 1.0198718309402466; compile_metrics of 100.0%\n",
      "Score for fold 428: loss of 1.2317463159561157; compile_metrics of 100.0%\n",
      "Score for fold 429: loss of 0.8989838361740112; compile_metrics of 100.0%\n",
      "Score for fold 430: loss of 0.8677964806556702; compile_metrics of 100.0%\n",
      "Score for fold 431: loss of 3.930673599243164; compile_metrics of 0.0%\n",
      "Score for fold 432: loss of 9.267595291137695; compile_metrics of 0.0%\n",
      "Score for fold 433: loss of 5.60447359085083; compile_metrics of 0.0%\n",
      "Score for fold 434: loss of 0.9728865623474121; compile_metrics of 100.0%\n",
      "Score for fold 435: loss of 0.9395511150360107; compile_metrics of 100.0%\n",
      "Score for fold 436: loss of 7.066920280456543; compile_metrics of 0.0%\n",
      "Score for fold 437: loss of 0.9028868079185486; compile_metrics of 100.0%\n",
      "Score for fold 438: loss of 0.941817581653595; compile_metrics of 100.0%\n",
      "Score for fold 439: loss of 1.1316767930984497; compile_metrics of 100.0%\n",
      "Score for fold 440: loss of 1.1677120923995972; compile_metrics of 100.0%\n",
      "Score for fold 441: loss of 1.1524488925933838; compile_metrics of 100.0%\n",
      "Score for fold 442: loss of 0.8736048936843872; compile_metrics of 100.0%\n",
      "Score for fold 443: loss of 7.754158020019531; compile_metrics of 0.0%\n",
      "Score for fold 444: loss of 1.1109637022018433; compile_metrics of 100.0%\n",
      "Score for fold 445: loss of 0.9722211956977844; compile_metrics of 100.0%\n",
      "Score for fold 446: loss of 6.2608642578125; compile_metrics of 0.0%\n",
      "Score for fold 447: loss of 3.9154727458953857; compile_metrics of 0.0%\n",
      "Score for fold 448: loss of 5.73952579498291; compile_metrics of 0.0%\n",
      "Score for fold 449: loss of 0.8522388935089111; compile_metrics of 100.0%\n",
      "Score for fold 450: loss of 1.1021989583969116; compile_metrics of 100.0%\n",
      "Score for fold 451: loss of 5.916237831115723; compile_metrics of 0.0%\n",
      "Score for fold 452: loss of 0.8631327748298645; compile_metrics of 100.0%\n",
      "Score for fold 453: loss of 4.682063579559326; compile_metrics of 0.0%\n",
      "Score for fold 454: loss of 1.2076034545898438; compile_metrics of 100.0%\n",
      "Score for fold 455: loss of 5.630715370178223; compile_metrics of 0.0%\n",
      "Score for fold 456: loss of 0.862305760383606; compile_metrics of 100.0%\n",
      "Score for fold 457: loss of 2.664862871170044; compile_metrics of 0.0%\n",
      "Score for fold 458: loss of 1.06349778175354; compile_metrics of 100.0%\n",
      "Score for fold 459: loss of 0.9886334538459778; compile_metrics of 100.0%\n",
      "Score for fold 460: loss of 2.3784704208374023; compile_metrics of 0.0%\n",
      "Score for fold 461: loss of 0.8519549369812012; compile_metrics of 100.0%\n",
      "Score for fold 462: loss of 0.9534243941307068; compile_metrics of 100.0%\n",
      "Score for fold 463: loss of 0.9181550145149231; compile_metrics of 100.0%\n",
      "Score for fold 464: loss of 1.68165123462677; compile_metrics of 100.0%\n",
      "Score for fold 465: loss of 0.9628011584281921; compile_metrics of 100.0%\n",
      "Score for fold 466: loss of 1.0179420709609985; compile_metrics of 100.0%\n",
      "Score for fold 467: loss of 1.8748878240585327; compile_metrics of 100.0%\n",
      "Score for fold 468: loss of 5.245460033416748; compile_metrics of 0.0%\n",
      "Score for fold 469: loss of 3.905306100845337; compile_metrics of 0.0%\n",
      "Score for fold 470: loss of 1.128383755683899; compile_metrics of 100.0%\n",
      "Score for fold 471: loss of 1.9305542707443237; compile_metrics of 100.0%\n",
      "Score for fold 472: loss of 3.250541925430298; compile_metrics of 0.0%\n",
      "Score for fold 473: loss of 1.7031760215759277; compile_metrics of 100.0%\n",
      "Score for fold 474: loss of 2.6337392330169678; compile_metrics of 0.0%\n",
      "Score for fold 475: loss of 0.9416050910949707; compile_metrics of 100.0%\n",
      "Score for fold 476: loss of 5.787972927093506; compile_metrics of 0.0%\n",
      "Score for fold 477: loss of 0.895718514919281; compile_metrics of 100.0%\n",
      "Score for fold 478: loss of 0.9654291868209839; compile_metrics of 100.0%\n",
      "Score for fold 479: loss of 1.1258184909820557; compile_metrics of 100.0%\n",
      "Score for fold 480: loss of 3.0998830795288086; compile_metrics of 0.0%\n",
      "Score for fold 481: loss of 0.8942877054214478; compile_metrics of 100.0%\n",
      "Score for fold 482: loss of 1.1342461109161377; compile_metrics of 100.0%\n",
      "Score for fold 483: loss of 0.8484440445899963; compile_metrics of 100.0%\n",
      "Score for fold 484: loss of 0.9966463446617126; compile_metrics of 100.0%\n",
      "Score for fold 485: loss of 0.934831440448761; compile_metrics of 100.0%\n",
      "Score for fold 486: loss of 0.8481729030609131; compile_metrics of 100.0%\n",
      "Score for fold 487: loss of 1.399198293685913; compile_metrics of 100.0%\n",
      "Score for fold 488: loss of 5.648273468017578; compile_metrics of 0.0%\n",
      "Score for fold 489: loss of 0.8738768100738525; compile_metrics of 100.0%\n",
      "Score for fold 490: loss of 0.8583182096481323; compile_metrics of 100.0%\n",
      "Score for fold 491: loss of 0.8855317831039429; compile_metrics of 100.0%\n",
      "Score for fold 492: loss of 1.0291144847869873; compile_metrics of 100.0%\n",
      "Score for fold 493: loss of 1.934589147567749; compile_metrics of 0.0%\n",
      "Score for fold 494: loss of 0.8921712636947632; compile_metrics of 100.0%\n",
      "Score for fold 495: loss of 0.8602950572967529; compile_metrics of 100.0%\n",
      "Score for fold 496: loss of 1.2888462543487549; compile_metrics of 100.0%\n",
      "Score for fold 497: loss of 1.2910066843032837; compile_metrics of 100.0%\n",
      "Score for fold 498: loss of 0.8522692322731018; compile_metrics of 100.0%\n",
      "Score for fold 499: loss of 3.328951835632324; compile_metrics of 0.0%\n",
      "Score for fold 500: loss of 1.020725131034851; compile_metrics of 100.0%\n",
      "Score for fold 501: loss of 1.1115247011184692; compile_metrics of 100.0%\n",
      "Score for fold 502: loss of 2.921600341796875; compile_metrics of 0.0%\n",
      "Score for fold 503: loss of 1.046860933303833; compile_metrics of 100.0%\n",
      "Score for fold 504: loss of 3.3162894248962402; compile_metrics of 0.0%\n",
      "Score for fold 505: loss of 3.6917219161987305; compile_metrics of 0.0%\n",
      "Score for fold 506: loss of 5.6500020027160645; compile_metrics of 0.0%\n",
      "Score for fold 507: loss of 0.9393284916877747; compile_metrics of 100.0%\n",
      "Score for fold 508: loss of 4.090847015380859; compile_metrics of 0.0%\n",
      "Score for fold 509: loss of 4.45186710357666; compile_metrics of 0.0%\n",
      "Score for fold 510: loss of 6.551788330078125; compile_metrics of 0.0%\n",
      "Score for fold 511: loss of 2.8068902492523193; compile_metrics of 0.0%\n",
      "Score for fold 512: loss of 3.134007215499878; compile_metrics of 0.0%\n",
      "Score for fold 513: loss of 0.9536581635475159; compile_metrics of 100.0%\n",
      "Score for fold 514: loss of 1.8619239330291748; compile_metrics of 100.0%\n",
      "Score for fold 515: loss of 3.315091848373413; compile_metrics of 0.0%\n",
      "Score for fold 516: loss of 1.056910514831543; compile_metrics of 100.0%\n",
      "Score for fold 517: loss of 1.0535497665405273; compile_metrics of 100.0%\n",
      "Score for fold 518: loss of 3.3491766452789307; compile_metrics of 0.0%\n",
      "Score for fold 519: loss of 1.0750502347946167; compile_metrics of 100.0%\n",
      "Score for fold 520: loss of 0.8711564540863037; compile_metrics of 100.0%\n",
      "Score for fold 521: loss of 0.9117313027381897; compile_metrics of 100.0%\n",
      "Score for fold 522: loss of 0.8851953148841858; compile_metrics of 100.0%\n",
      "Score for fold 523: loss of 0.9197556972503662; compile_metrics of 100.0%\n",
      "Score for fold 524: loss of 1.038914680480957; compile_metrics of 100.0%\n",
      "Score for fold 525: loss of 0.8827887773513794; compile_metrics of 100.0%\n",
      "Score for fold 526: loss of 3.8366634845733643; compile_metrics of 0.0%\n",
      "Score for fold 527: loss of 0.9066535234451294; compile_metrics of 100.0%\n",
      "Score for fold 528: loss of 1.0293089151382446; compile_metrics of 100.0%\n",
      "Score for fold 529: loss of 2.4019601345062256; compile_metrics of 0.0%\n",
      "Score for fold 530: loss of 1.3652039766311646; compile_metrics of 100.0%\n",
      "Score for fold 531: loss of 4.022776126861572; compile_metrics of 0.0%\n",
      "Score for fold 532: loss of 1.00943922996521; compile_metrics of 100.0%\n",
      "Score for fold 533: loss of 1.4633793830871582; compile_metrics of 100.0%\n",
      "Score for fold 534: loss of 3.6590521335601807; compile_metrics of 0.0%\n",
      "Score for fold 535: loss of 0.9532613754272461; compile_metrics of 100.0%\n",
      "Score for fold 536: loss of 1.1464579105377197; compile_metrics of 100.0%\n",
      "Score for fold 537: loss of 3.8279504776000977; compile_metrics of 0.0%\n",
      "Score for fold 538: loss of 5.602502346038818; compile_metrics of 0.0%\n",
      "Score for fold 539: loss of 1.229465126991272; compile_metrics of 100.0%\n",
      "Score for fold 540: loss of 7.015792369842529; compile_metrics of 0.0%\n",
      "Score for fold 541: loss of 0.9629337787628174; compile_metrics of 100.0%\n",
      "Score for fold 542: loss of 0.9987808465957642; compile_metrics of 100.0%\n",
      "Score for fold 543: loss of 1.1227718591690063; compile_metrics of 100.0%\n",
      "Score for fold 544: loss of 1.1582074165344238; compile_metrics of 100.0%\n",
      "Score for fold 545: loss of 4.625802040100098; compile_metrics of 0.0%\n",
      "Score for fold 546: loss of 0.9863466024398804; compile_metrics of 100.0%\n",
      "Average accuracy over all folds: 64.46886446886447\n",
      "Average loss over all folds: 2.2746849730337932\n",
      "Problematic samples (indices with loss > 3.0): [2, 3, 11, 13, 15, 17, 18, 22, 32, 33, 45, 48, 54, 63, 68, 80, 81, 85, 90, 91, 97, 118, 120, 123, 124, 137, 142, 148, 150, 151, 158, 159, 162, 166, 167, 171, 176, 178, 181, 186, 190, 193, 195, 197, 199, 200, 201, 210, 213, 216, 217, 219, 220, 222, 231, 232, 235, 237, 239, 244, 247, 250, 253, 255, 256, 271, 285, 290, 293, 298, 301, 305, 306, 307, 312, 318, 319, 320, 321, 322, 323, 325, 327, 331, 334, 335, 336, 337, 338, 352, 372, 374, 375, 376, 379, 382, 384, 388, 392, 403, 404, 405, 406, 407, 408, 409, 411, 413, 415, 417, 418, 420, 423, 425, 430, 431, 432, 435, 442, 445, 446, 447, 450, 452, 454, 467, 468, 471, 475, 479, 487, 498, 503, 504, 505, 507, 508, 509, 511, 514, 517, 525, 530, 533, 536, 537, 539, 544]\n",
      "Number of samples after removing problematic samples: 398\n"
     ]
    }
   ],
   "source": [
    "#LEave one out cross validation\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import collections\n",
    "\n",
    "# Assuming X and y are already defined and X contains grayscale images\n",
    "X = X.reshape(-1, 20, 20, 1)  # Reshape to include channel dimension\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "y_categorical = to_categorical(y_encoded, num_classes=len(label_encoder.classes_))\n",
    "\n",
    "# Define leave-one-out cross-validation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Initialize variables to store results\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "fold_no = 1\n",
    "\n",
    "problematic_samples = []\n",
    "high_loss_threshold = 3.0  # Define a threshold for high loss\n",
    "\n",
    "for train, test in loo.split(X, y_encoded):\n",
    "    model = Sequential([\n",
    "        Input(shape=(grid_size[0], grid_size[1], 1)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(np.unique(y)), activation='softmax', kernel_regularizer=l2(0.001))\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_counts = collections.Counter(y_encoded[train])\n",
    "    total_samples = sum(class_counts.values())\n",
    "    class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "\n",
    "    # Callbacks\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=30, min_lr=1e-6)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X[train], to_categorical(y_encoded[train], num_classes=len(label_encoder.classes_)),\n",
    "                        class_weight=class_weights,\n",
    "                        epochs=300,\n",
    "                        validation_data=(X[test], to_categorical(y_encoded[test], num_classes=len(label_encoder.classes_))),\n",
    "                        callbacks=[reduce_lr, early_stopping], verbose = 0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    scores = model.evaluate(X[test], to_categorical(y_encoded[test], num_classes=len(label_encoder.classes_)), verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    # Store problematic samples\n",
    "    if scores[0] > high_loss_threshold:\n",
    "        problematic_samples.append(test[0])\n",
    "\n",
    "    fold_no += 1\n",
    "\n",
    "# Print the average accuracy and loss over all folds\n",
    "print('Average accuracy over all folds:', np.mean(acc_per_fold))\n",
    "print('Average loss over all folds:', np.mean(loss_per_fold))\n",
    "\n",
    "# Print and remove problematic samples\n",
    "print(f'Problematic samples (indices with loss > {high_loss_threshold}):', problematic_samples)\n",
    "\n",
    "# Remove problematic samples from the dataset\n",
    "X_filtered = np.delete(X, problematic_samples, axis=0)\n",
    "y_filtered = np.delete(y_encoded, problematic_samples, axis=0)\n",
    "\n",
    "print('Number of samples after removing problematic samples:', X_filtered.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef6cb6-66a6-4a1c-9a55-ffda7f89b7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
